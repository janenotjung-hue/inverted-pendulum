{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72377727\n",
      "Iteration 2, loss = 0.60907403\n",
      "Iteration 3, loss = 0.49420315\n",
      "Iteration 4, loss = 0.39302435\n",
      "Iteration 5, loss = 0.31985753\n",
      "Iteration 6, loss = 0.26600449\n",
      "Iteration 7, loss = 0.22678157\n",
      "Iteration 8, loss = 0.19939808\n",
      "Iteration 9, loss = 0.17746569\n",
      "Iteration 10, loss = 0.15824231\n",
      "Iteration 11, loss = 0.14334053\n",
      "Iteration 12, loss = 0.12962831\n",
      "Iteration 13, loss = 0.11730712\n",
      "Iteration 14, loss = 0.10605656\n",
      "Iteration 15, loss = 0.09562688\n",
      "Iteration 16, loss = 0.08715838\n",
      "Iteration 17, loss = 0.08036928\n",
      "Iteration 18, loss = 0.07531786\n",
      "Iteration 19, loss = 0.07142495\n",
      "Iteration 20, loss = 0.06859838\n",
      "Iteration 21, loss = 0.06596506\n",
      "Iteration 22, loss = 0.06363562\n",
      "Iteration 23, loss = 0.06180422\n",
      "Iteration 24, loss = 0.06014878\n",
      "Iteration 25, loss = 0.05858677\n",
      "Iteration 26, loss = 0.05702718\n",
      "Iteration 27, loss = 0.05590068\n",
      "Iteration 28, loss = 0.05464272\n",
      "Iteration 29, loss = 0.05334058\n",
      "Iteration 30, loss = 0.05220214\n",
      "Iteration 31, loss = 0.05145083\n",
      "Iteration 32, loss = 0.05036168\n",
      "Iteration 33, loss = 0.04902710\n",
      "Iteration 34, loss = 0.04813414\n",
      "Iteration 35, loss = 0.04724045\n",
      "Iteration 36, loss = 0.04651646\n",
      "Iteration 37, loss = 0.04569302\n",
      "Iteration 38, loss = 0.04515327\n",
      "Iteration 39, loss = 0.04438221\n",
      "Iteration 40, loss = 0.04378458\n",
      "Iteration 41, loss = 0.04320418\n",
      "Iteration 42, loss = 0.04241910\n",
      "Iteration 43, loss = 0.04207632\n",
      "Iteration 44, loss = 0.04146685\n",
      "Iteration 45, loss = 0.04091707\n",
      "Iteration 46, loss = 0.04054142\n",
      "Iteration 47, loss = 0.03995992\n",
      "Iteration 48, loss = 0.03939563\n",
      "Iteration 49, loss = 0.03900211\n",
      "Iteration 50, loss = 0.03853075\n",
      "Iteration 51, loss = 0.03815922\n",
      "Iteration 52, loss = 0.03767121\n",
      "Iteration 53, loss = 0.03709232\n",
      "Iteration 54, loss = 0.03679546\n",
      "Iteration 55, loss = 0.03620112\n",
      "Iteration 56, loss = 0.03595189\n",
      "Iteration 57, loss = 0.03552842\n",
      "Iteration 58, loss = 0.03522520\n",
      "Iteration 59, loss = 0.03490653\n",
      "Iteration 60, loss = 0.03448226\n",
      "Iteration 61, loss = 0.03409901\n",
      "Iteration 62, loss = 0.03374028\n",
      "Iteration 63, loss = 0.03338948\n",
      "Iteration 64, loss = 0.03301388\n",
      "Iteration 65, loss = 0.03284415\n",
      "Iteration 66, loss = 0.03259840\n",
      "Iteration 67, loss = 0.03213036\n",
      "Iteration 68, loss = 0.03191875\n",
      "Iteration 69, loss = 0.03153746\n",
      "Iteration 70, loss = 0.03142386\n",
      "Iteration 71, loss = 0.03112191\n",
      "Iteration 72, loss = 0.03079203\n",
      "Iteration 73, loss = 0.03057072\n",
      "Iteration 74, loss = 0.03026187\n",
      "Iteration 75, loss = 0.02997395\n",
      "Iteration 76, loss = 0.02965289\n",
      "Iteration 77, loss = 0.02951298\n",
      "Iteration 78, loss = 0.02928504\n",
      "Iteration 79, loss = 0.02896632\n",
      "Iteration 80, loss = 0.02905682\n",
      "Iteration 81, loss = 0.02839828\n",
      "Iteration 82, loss = 0.02807437\n",
      "Iteration 83, loss = 0.02779701\n",
      "Iteration 84, loss = 0.02744231\n",
      "Iteration 85, loss = 0.02717227\n",
      "Iteration 86, loss = 0.02688205\n",
      "Iteration 87, loss = 0.02722663\n",
      "Iteration 88, loss = 0.02643690\n",
      "Iteration 89, loss = 0.02623027\n",
      "Iteration 90, loss = 0.02591410\n",
      "Iteration 91, loss = 0.02584877\n",
      "Iteration 92, loss = 0.02549152\n",
      "Iteration 93, loss = 0.02551670\n",
      "Iteration 94, loss = 0.02504334\n",
      "Iteration 95, loss = 0.02486586\n",
      "Iteration 96, loss = 0.02480904\n",
      "Iteration 97, loss = 0.02425344\n",
      "Iteration 98, loss = 0.02404460\n",
      "Iteration 99, loss = 0.02397918\n",
      "Iteration 100, loss = 0.02383858\n",
      "Iteration 101, loss = 0.02357947\n",
      "Iteration 102, loss = 0.02322907\n",
      "Iteration 103, loss = 0.02306089\n",
      "Iteration 104, loss = 0.02285432\n",
      "Iteration 105, loss = 0.02264759\n",
      "Iteration 106, loss = 0.02279855\n",
      "Iteration 107, loss = 0.02230618\n",
      "Iteration 108, loss = 0.02190289\n",
      "Iteration 109, loss = 0.02175299\n",
      "Iteration 110, loss = 0.02151964\n",
      "Iteration 111, loss = 0.02167714\n",
      "Iteration 112, loss = 0.02126261\n",
      "Iteration 113, loss = 0.02152911\n",
      "Iteration 114, loss = 0.02089070\n",
      "Iteration 115, loss = 0.02094475\n",
      "Iteration 116, loss = 0.02068701\n",
      "Iteration 117, loss = 0.02037144\n",
      "Iteration 118, loss = 0.02013728\n",
      "Iteration 119, loss = 0.01994925\n",
      "Iteration 120, loss = 0.01979060\n",
      "Iteration 121, loss = 0.01980786\n",
      "Iteration 122, loss = 0.01957515\n",
      "Iteration 123, loss = 0.01929720\n",
      "Iteration 124, loss = 0.01922660\n",
      "Iteration 125, loss = 0.01919442\n",
      "Iteration 126, loss = 0.01901147\n",
      "Iteration 127, loss = 0.01887871\n",
      "Iteration 128, loss = 0.01886511\n",
      "Iteration 129, loss = 0.01868368\n",
      "Iteration 130, loss = 0.01844107\n",
      "Iteration 131, loss = 0.01859858\n",
      "Iteration 132, loss = 0.01814229\n",
      "Iteration 133, loss = 0.01803310\n",
      "Iteration 134, loss = 0.01802198\n",
      "Iteration 135, loss = 0.01801381\n",
      "Iteration 136, loss = 0.01768472\n",
      "Iteration 137, loss = 0.01753721\n",
      "Iteration 138, loss = 0.01748848\n",
      "Iteration 139, loss = 0.01731551\n",
      "Iteration 140, loss = 0.01741596\n",
      "Iteration 141, loss = 0.01699139\n",
      "Iteration 142, loss = 0.01695486\n",
      "Iteration 143, loss = 0.01688079\n",
      "Iteration 144, loss = 0.01687649\n",
      "Iteration 145, loss = 0.01691978\n",
      "Iteration 146, loss = 0.01657795\n",
      "Iteration 147, loss = 0.01639104\n",
      "Iteration 148, loss = 0.01649515\n",
      "Iteration 149, loss = 0.01626999\n",
      "Iteration 150, loss = 0.01608557\n",
      "Iteration 151, loss = 0.01593443\n",
      "Iteration 152, loss = 0.01601731\n",
      "Iteration 153, loss = 0.01604453\n",
      "Iteration 154, loss = 0.01562671\n",
      "Iteration 155, loss = 0.01569305\n",
      "Iteration 156, loss = 0.01541391\n",
      "Iteration 157, loss = 0.01523488\n",
      "Iteration 158, loss = 0.01524387\n",
      "Iteration 159, loss = 0.01515979\n",
      "Iteration 160, loss = 0.01505221\n",
      "Iteration 161, loss = 0.01511335\n",
      "Iteration 162, loss = 0.01514228\n",
      "Iteration 163, loss = 0.01489599\n",
      "Iteration 164, loss = 0.01459436\n",
      "Iteration 165, loss = 0.01445265\n",
      "Iteration 166, loss = 0.01451843\n",
      "Iteration 167, loss = 0.01439579\n",
      "Iteration 168, loss = 0.01426157\n",
      "Iteration 169, loss = 0.01457685\n",
      "Iteration 170, loss = 0.01409082\n",
      "Iteration 171, loss = 0.01404886\n",
      "Iteration 172, loss = 0.01388753\n",
      "Iteration 173, loss = 0.01382713\n",
      "Iteration 174, loss = 0.01367945\n",
      "Iteration 175, loss = 0.01387523\n",
      "Iteration 176, loss = 0.01346940\n",
      "Iteration 177, loss = 0.01374043\n",
      "Iteration 178, loss = 0.01382097\n",
      "Iteration 179, loss = 0.01335015\n",
      "Iteration 180, loss = 0.01319094\n",
      "Iteration 181, loss = 0.01313626\n",
      "Iteration 182, loss = 0.01309109\n",
      "Iteration 183, loss = 0.01307909\n",
      "Iteration 184, loss = 0.01346567\n",
      "Iteration 185, loss = 0.01284938\n",
      "Iteration 186, loss = 0.01267523\n",
      "Iteration 187, loss = 0.01291670\n",
      "Iteration 188, loss = 0.01267710\n",
      "Iteration 189, loss = 0.01262420\n",
      "Iteration 190, loss = 0.01257882\n",
      "Iteration 191, loss = 0.01232370\n",
      "Iteration 192, loss = 0.01225725\n",
      "Iteration 193, loss = 0.01231272\n",
      "Iteration 194, loss = 0.01232298\n",
      "Iteration 195, loss = 0.01231243\n",
      "Iteration 196, loss = 0.01232287\n",
      "Iteration 197, loss = 0.01214901\n",
      "Iteration 198, loss = 0.01222743\n",
      "Iteration 199, loss = 0.01191248\n",
      "Iteration 200, loss = 0.01194101\n",
      "Iteration 201, loss = 0.01171398\n",
      "Iteration 202, loss = 0.01174863\n",
      "Iteration 203, loss = 0.01202217\n",
      "Iteration 204, loss = 0.01159054\n",
      "Iteration 205, loss = 0.01167185\n",
      "Iteration 206, loss = 0.01165373\n",
      "Iteration 207, loss = 0.01161924\n",
      "Iteration 208, loss = 0.01141185\n",
      "Iteration 209, loss = 0.01133479\n",
      "Iteration 210, loss = 0.01126877\n",
      "Iteration 211, loss = 0.01138769\n",
      "Iteration 212, loss = 0.01125926\n",
      "Iteration 213, loss = 0.01102413\n",
      "Iteration 214, loss = 0.01113308\n",
      "Iteration 215, loss = 0.01094613\n",
      "Iteration 216, loss = 0.01106083\n",
      "Iteration 217, loss = 0.01102373\n",
      "Iteration 218, loss = 0.01099663\n",
      "Iteration 219, loss = 0.01081492\n",
      "Iteration 220, loss = 0.01093008\n",
      "Iteration 221, loss = 0.01078096\n",
      "Iteration 222, loss = 0.01069527\n",
      "Iteration 223, loss = 0.01065213\n",
      "Iteration 224, loss = 0.01098361\n",
      "Iteration 225, loss = 0.01083008\n",
      "Iteration 226, loss = 0.01075848\n",
      "Iteration 227, loss = 0.01056083\n",
      "Iteration 228, loss = 0.01093194\n",
      "Iteration 229, loss = 0.01071975\n",
      "Iteration 230, loss = 0.01041145\n",
      "Iteration 231, loss = 0.01101324\n",
      "Iteration 232, loss = 0.01088111\n",
      "Iteration 233, loss = 0.01114776\n",
      "Iteration 234, loss = 0.01068791\n",
      "Iteration 235, loss = 0.01014878\n",
      "Iteration 236, loss = 0.01027828\n",
      "Iteration 237, loss = 0.01016947\n",
      "Iteration 238, loss = 0.01019362\n",
      "Iteration 239, loss = 0.01031320\n",
      "Iteration 240, loss = 0.01018574\n",
      "Iteration 241, loss = 0.01021312\n",
      "Iteration 242, loss = 0.00996983\n",
      "Iteration 243, loss = 0.00985230\n",
      "Iteration 244, loss = 0.01020465\n",
      "Iteration 245, loss = 0.00983377\n",
      "Iteration 246, loss = 0.00990872\n",
      "Iteration 247, loss = 0.01035634\n",
      "Iteration 248, loss = 0.01026878\n",
      "Iteration 249, loss = 0.00990715\n",
      "Iteration 250, loss = 0.01004142\n",
      "Iteration 251, loss = 0.00972762\n",
      "Iteration 252, loss = 0.01015103\n",
      "Iteration 253, loss = 0.00997505\n",
      "Iteration 254, loss = 0.00987317\n",
      "Iteration 255, loss = 0.00985643\n",
      "Iteration 256, loss = 0.00979246\n",
      "Iteration 257, loss = 0.00990313\n",
      "Iteration 258, loss = 0.00985078\n",
      "Iteration 259, loss = 0.00962919\n",
      "Iteration 260, loss = 0.00972924\n",
      "Iteration 261, loss = 0.00962020\n",
      "Iteration 262, loss = 0.00950457\n",
      "Iteration 263, loss = 0.00959381\n",
      "Iteration 264, loss = 0.00950049\n",
      "Iteration 265, loss = 0.00943965\n",
      "Iteration 266, loss = 0.00964008\n",
      "Iteration 267, loss = 0.00944948\n",
      "Iteration 268, loss = 0.00934224\n",
      "Iteration 269, loss = 0.00955402\n",
      "Iteration 270, loss = 0.00964095\n",
      "Iteration 271, loss = 0.00948559\n",
      "Iteration 272, loss = 0.00959436\n",
      "Iteration 273, loss = 0.00956731\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71978639\n",
      "Iteration 2, loss = 0.69131588\n",
      "Iteration 3, loss = 0.67919037\n",
      "Iteration 4, loss = 0.67191917\n",
      "Iteration 5, loss = 0.66460381\n",
      "Iteration 6, loss = 0.65286486\n",
      "Iteration 7, loss = 0.64050352\n",
      "Iteration 8, loss = 0.62719247\n",
      "Iteration 9, loss = 0.61267798\n",
      "Iteration 10, loss = 0.59328298\n",
      "Iteration 11, loss = 0.57837632\n",
      "Iteration 12, loss = 0.56360599\n",
      "Iteration 13, loss = 0.54793799\n",
      "Iteration 14, loss = 0.53443584\n",
      "Iteration 15, loss = 0.52124864\n",
      "Iteration 16, loss = 0.51148191\n",
      "Iteration 17, loss = 0.49974834\n",
      "Iteration 18, loss = 0.48880204\n",
      "Iteration 19, loss = 0.47829569\n",
      "Iteration 20, loss = 0.46843883\n",
      "Iteration 21, loss = 0.45918595\n",
      "Iteration 22, loss = 0.45000702\n",
      "Iteration 23, loss = 0.44064715\n",
      "Iteration 24, loss = 0.43180678\n",
      "Iteration 25, loss = 0.42350099\n",
      "Iteration 26, loss = 0.41589075\n",
      "Iteration 27, loss = 0.40831677\n",
      "Iteration 28, loss = 0.40081562\n",
      "Iteration 29, loss = 0.39447593\n",
      "Iteration 30, loss = 0.38696527\n",
      "Iteration 31, loss = 0.38160354\n",
      "Iteration 32, loss = 0.37475822\n",
      "Iteration 33, loss = 0.36839509\n",
      "Iteration 34, loss = 0.36478473\n",
      "Iteration 35, loss = 0.35859896\n",
      "Iteration 36, loss = 0.35493059\n",
      "Iteration 37, loss = 0.34792961\n",
      "Iteration 38, loss = 0.34467615\n",
      "Iteration 39, loss = 0.33805075\n",
      "Iteration 40, loss = 0.33311302\n",
      "Iteration 41, loss = 0.33181166\n",
      "Iteration 42, loss = 0.32486076\n",
      "Iteration 43, loss = 0.32081325\n",
      "Iteration 44, loss = 0.31624781\n",
      "Iteration 45, loss = 0.31413225\n",
      "Iteration 46, loss = 0.30936316\n",
      "Iteration 47, loss = 0.30476378\n",
      "Iteration 48, loss = 0.30436143\n",
      "Iteration 49, loss = 0.29748390\n",
      "Iteration 50, loss = 0.29617166\n",
      "Iteration 51, loss = 0.29075544\n",
      "Iteration 52, loss = 0.28627257\n",
      "Iteration 53, loss = 0.28335103\n",
      "Iteration 54, loss = 0.28179098\n",
      "Iteration 55, loss = 0.27680598\n",
      "Iteration 56, loss = 0.27387194\n",
      "Iteration 57, loss = 0.27074235\n",
      "Iteration 58, loss = 0.27026596\n",
      "Iteration 59, loss = 0.26503575\n",
      "Iteration 60, loss = 0.26234089\n",
      "Iteration 61, loss = 0.25857844\n",
      "Iteration 62, loss = 0.25620691\n",
      "Iteration 63, loss = 0.25190244\n",
      "Iteration 64, loss = 0.24921112\n",
      "Iteration 65, loss = 0.24665348\n",
      "Iteration 66, loss = 0.24612772\n",
      "Iteration 67, loss = 0.24080385\n",
      "Iteration 68, loss = 0.23767045\n",
      "Iteration 69, loss = 0.23707189\n",
      "Iteration 70, loss = 0.23620517\n",
      "Iteration 71, loss = 0.23077357\n",
      "Iteration 72, loss = 0.22749918\n",
      "Iteration 73, loss = 0.22517189\n",
      "Iteration 74, loss = 0.22251749\n",
      "Iteration 75, loss = 0.22080836\n",
      "Iteration 76, loss = 0.21987943\n",
      "Iteration 77, loss = 0.21648117\n",
      "Iteration 78, loss = 0.21463177\n",
      "Iteration 79, loss = 0.21139136\n",
      "Iteration 80, loss = 0.20884823\n",
      "Iteration 81, loss = 0.20611255\n",
      "Iteration 82, loss = 0.20389334\n",
      "Iteration 83, loss = 0.20502267\n",
      "Iteration 84, loss = 0.19977981\n",
      "Iteration 85, loss = 0.19880115\n",
      "Iteration 86, loss = 0.19499971\n",
      "Iteration 87, loss = 0.19371188\n",
      "Iteration 88, loss = 0.19047636\n",
      "Iteration 89, loss = 0.19014912\n",
      "Iteration 90, loss = 0.18663889\n",
      "Iteration 91, loss = 0.18695866\n",
      "Iteration 92, loss = 0.18547508\n",
      "Iteration 93, loss = 0.18153060\n",
      "Iteration 94, loss = 0.17832844\n",
      "Iteration 95, loss = 0.17753693\n",
      "Iteration 96, loss = 0.17505189\n",
      "Iteration 97, loss = 0.17386169\n",
      "Iteration 98, loss = 0.17271336\n",
      "Iteration 99, loss = 0.16968378\n",
      "Iteration 100, loss = 0.16766545\n",
      "Iteration 101, loss = 0.16772729\n",
      "Iteration 102, loss = 0.16579610\n",
      "Iteration 103, loss = 0.16278024\n",
      "Iteration 104, loss = 0.16280839\n",
      "Iteration 105, loss = 0.16004656\n",
      "Iteration 106, loss = 0.15928706\n",
      "Iteration 107, loss = 0.15806721\n",
      "Iteration 108, loss = 0.15484199\n",
      "Iteration 109, loss = 0.15244231\n",
      "Iteration 110, loss = 0.15188892\n",
      "Iteration 111, loss = 0.15265363\n",
      "Iteration 112, loss = 0.14968748\n",
      "Iteration 113, loss = 0.14866067\n",
      "Iteration 114, loss = 0.14848764\n",
      "Iteration 115, loss = 0.14561929\n",
      "Iteration 116, loss = 0.14315396\n",
      "Iteration 117, loss = 0.14503005\n",
      "Iteration 118, loss = 0.14099328\n",
      "Iteration 119, loss = 0.14148019\n",
      "Iteration 120, loss = 0.13983960\n",
      "Iteration 121, loss = 0.13662611\n",
      "Iteration 122, loss = 0.13628560\n",
      "Iteration 123, loss = 0.13586734\n",
      "Iteration 124, loss = 0.13505466\n",
      "Iteration 125, loss = 0.13388841\n",
      "Iteration 126, loss = 0.13671032\n",
      "Iteration 127, loss = 0.13179901\n",
      "Iteration 128, loss = 0.13087256\n",
      "Iteration 129, loss = 0.13242598\n",
      "Iteration 130, loss = 0.12878728\n",
      "Iteration 131, loss = 0.12947868\n",
      "Iteration 132, loss = 0.13280324\n",
      "Iteration 133, loss = 0.12722550\n",
      "Iteration 134, loss = 0.12336843\n",
      "Iteration 135, loss = 0.12332216\n",
      "Iteration 136, loss = 0.12200520\n",
      "Iteration 137, loss = 0.12141565\n",
      "Iteration 138, loss = 0.11975568\n",
      "Iteration 139, loss = 0.11934064\n",
      "Iteration 140, loss = 0.11761378\n",
      "Iteration 141, loss = 0.11832546\n",
      "Iteration 142, loss = 0.11794390\n",
      "Iteration 143, loss = 0.11889180\n",
      "Iteration 144, loss = 0.11547003\n",
      "Iteration 145, loss = 0.11455381\n",
      "Iteration 146, loss = 0.12174431\n",
      "Iteration 147, loss = 0.11528431\n",
      "Iteration 148, loss = 0.11278762\n",
      "Iteration 149, loss = 0.11442852\n",
      "Iteration 150, loss = 0.11218684\n",
      "Iteration 151, loss = 0.11131958\n",
      "Iteration 152, loss = 0.10880892\n",
      "Iteration 153, loss = 0.11022469\n",
      "Iteration 154, loss = 0.11034231\n",
      "Iteration 155, loss = 0.10697429\n",
      "Iteration 156, loss = 0.10598394\n",
      "Iteration 157, loss = 0.10715648\n",
      "Iteration 158, loss = 0.10457719\n",
      "Iteration 159, loss = 0.10377719\n",
      "Iteration 160, loss = 0.10323883\n",
      "Iteration 161, loss = 0.10304738\n",
      "Iteration 162, loss = 0.10123593\n",
      "Iteration 163, loss = 0.10114477\n",
      "Iteration 164, loss = 0.10031629\n",
      "Iteration 165, loss = 0.10009051\n",
      "Iteration 166, loss = 0.10113080\n",
      "Iteration 167, loss = 0.10079792\n",
      "Iteration 168, loss = 0.09914745\n",
      "Iteration 169, loss = 0.09796719\n",
      "Iteration 170, loss = 0.09681428\n",
      "Iteration 171, loss = 0.09624522\n",
      "Iteration 172, loss = 0.09489709\n",
      "Iteration 173, loss = 0.09601396\n",
      "Iteration 174, loss = 0.09678851\n",
      "Iteration 175, loss = 0.09531124\n",
      "Iteration 176, loss = 0.09277848\n",
      "Iteration 177, loss = 0.09328287\n",
      "Iteration 178, loss = 0.09139611\n",
      "Iteration 179, loss = 0.09161213\n",
      "Iteration 180, loss = 0.09588010\n",
      "Iteration 181, loss = 0.09015485\n",
      "Iteration 182, loss = 0.09318274\n",
      "Iteration 183, loss = 0.08972518\n",
      "Iteration 184, loss = 0.08793579\n",
      "Iteration 185, loss = 0.09033660\n",
      "Iteration 186, loss = 0.08885242\n",
      "Iteration 187, loss = 0.08832187\n",
      "Iteration 188, loss = 0.09176482\n",
      "Iteration 189, loss = 0.08826445\n",
      "Iteration 190, loss = 0.08733107\n",
      "Iteration 191, loss = 0.08711784\n",
      "Iteration 192, loss = 0.08622489\n",
      "Iteration 193, loss = 0.08449386\n",
      "Iteration 194, loss = 0.08365798\n",
      "Iteration 195, loss = 0.08387789\n",
      "Iteration 196, loss = 0.08440945\n",
      "Iteration 197, loss = 0.08310549\n",
      "Iteration 198, loss = 0.08468832\n",
      "Iteration 199, loss = 0.08158522\n",
      "Iteration 200, loss = 0.08582545\n",
      "Iteration 201, loss = 0.08111538\n",
      "Iteration 202, loss = 0.07964462\n",
      "Iteration 203, loss = 0.08218367\n",
      "Iteration 204, loss = 0.08017364\n",
      "Iteration 205, loss = 0.07863827\n",
      "Iteration 206, loss = 0.08246369\n",
      "Iteration 207, loss = 0.07690243\n",
      "Iteration 208, loss = 0.07693734\n",
      "Iteration 209, loss = 0.07719282\n",
      "Iteration 210, loss = 0.07661960\n",
      "Iteration 211, loss = 0.07536109\n",
      "Iteration 212, loss = 0.07595911\n",
      "Iteration 213, loss = 0.07903631\n",
      "Iteration 214, loss = 0.07573253\n",
      "Iteration 215, loss = 0.07564834\n",
      "Iteration 216, loss = 0.08025254\n",
      "Iteration 217, loss = 0.07602622\n",
      "Iteration 218, loss = 0.07253336\n",
      "Iteration 219, loss = 0.07195905\n",
      "Iteration 220, loss = 0.07192623\n",
      "Iteration 221, loss = 0.07188371\n",
      "Iteration 222, loss = 0.07117750\n",
      "Iteration 223, loss = 0.07132967\n",
      "Iteration 224, loss = 0.07357745\n",
      "Iteration 225, loss = 0.07033048\n",
      "Iteration 226, loss = 0.07322889\n",
      "Iteration 227, loss = 0.06938271\n",
      "Iteration 228, loss = 0.06876552\n",
      "Iteration 229, loss = 0.06789320\n",
      "Iteration 230, loss = 0.06877728\n",
      "Iteration 231, loss = 0.07033035\n",
      "Iteration 232, loss = 0.06843467\n",
      "Iteration 233, loss = 0.06758433\n",
      "Iteration 234, loss = 0.06828979\n",
      "Iteration 235, loss = 0.07000293\n",
      "Iteration 236, loss = 0.06773791\n",
      "Iteration 237, loss = 0.06634028\n",
      "Iteration 238, loss = 0.06666144\n",
      "Iteration 239, loss = 0.06706373\n",
      "Iteration 240, loss = 0.06481176\n",
      "Iteration 241, loss = 0.06630457\n",
      "Iteration 242, loss = 0.06900083\n",
      "Iteration 243, loss = 0.06517060\n",
      "Iteration 244, loss = 0.06381461\n",
      "Iteration 245, loss = 0.06342886\n",
      "Iteration 246, loss = 0.06336573\n",
      "Iteration 247, loss = 0.06915207\n",
      "Iteration 248, loss = 0.06204075\n",
      "Iteration 249, loss = 0.06339370\n",
      "Iteration 250, loss = 0.06267804\n",
      "Iteration 251, loss = 0.06436091\n",
      "Iteration 252, loss = 0.06216888\n",
      "Iteration 253, loss = 0.06085540\n",
      "Iteration 254, loss = 0.06213419\n",
      "Iteration 255, loss = 0.06116730\n",
      "Iteration 256, loss = 0.06021249\n",
      "Iteration 257, loss = 0.06165538\n",
      "Iteration 258, loss = 0.06027151\n",
      "Iteration 259, loss = 0.06040451\n",
      "Iteration 260, loss = 0.06018465\n",
      "Iteration 261, loss = 0.05892791\n",
      "Iteration 262, loss = 0.05824605\n",
      "Iteration 263, loss = 0.06014125\n",
      "Iteration 264, loss = 0.05765534\n",
      "Iteration 265, loss = 0.05723676\n",
      "Iteration 266, loss = 0.05862731\n",
      "Iteration 267, loss = 0.05774077\n",
      "Iteration 268, loss = 0.05870778\n",
      "Iteration 269, loss = 0.06006498\n",
      "Iteration 270, loss = 0.05725942\n",
      "Iteration 271, loss = 0.05962126\n",
      "Iteration 272, loss = 0.05747636\n",
      "Iteration 273, loss = 0.05772409\n",
      "Iteration 274, loss = 0.05523008\n",
      "Iteration 275, loss = 0.05757105\n",
      "Iteration 276, loss = 0.05651050\n",
      "Iteration 277, loss = 0.05483391\n",
      "Iteration 278, loss = 0.05396011\n",
      "Iteration 279, loss = 0.05379846\n",
      "Iteration 280, loss = 0.05727488\n",
      "Iteration 281, loss = 0.05760172\n",
      "Iteration 282, loss = 0.05439935\n",
      "Iteration 283, loss = 0.05582451\n",
      "Iteration 284, loss = 0.05611605\n",
      "Iteration 285, loss = 0.05268908\n",
      "Iteration 286, loss = 0.05234854\n",
      "Iteration 287, loss = 0.05238016\n",
      "Iteration 288, loss = 0.05168484\n",
      "Iteration 289, loss = 0.05163939\n",
      "Iteration 290, loss = 0.05101584\n",
      "Iteration 291, loss = 0.05206811\n",
      "Iteration 292, loss = 0.05207929\n",
      "Iteration 293, loss = 0.05458900\n",
      "Iteration 294, loss = 0.05212647\n",
      "Iteration 295, loss = 0.05644182\n",
      "Iteration 296, loss = 0.05234529\n",
      "Iteration 297, loss = 0.05147247\n",
      "Iteration 298, loss = 0.05033063\n",
      "Iteration 299, loss = 0.05148268\n",
      "Iteration 300, loss = 0.05065125\n",
      "Iteration 301, loss = 0.04920936\n",
      "Iteration 302, loss = 0.04977550\n",
      "Iteration 303, loss = 0.04830630\n",
      "Iteration 304, loss = 0.04905947\n",
      "Iteration 305, loss = 0.04842371\n",
      "Iteration 306, loss = 0.04860519\n",
      "Iteration 307, loss = 0.04847185\n",
      "Iteration 308, loss = 0.05470979\n",
      "Iteration 309, loss = 0.05059393\n",
      "Iteration 310, loss = 0.04980267\n",
      "Iteration 311, loss = 0.04731654\n",
      "Iteration 312, loss = 0.04754753\n",
      "Iteration 313, loss = 0.04671491\n",
      "Iteration 314, loss = 0.04687715\n",
      "Iteration 315, loss = 0.05101001\n",
      "Iteration 316, loss = 0.04642704\n",
      "Iteration 317, loss = 0.04681446\n",
      "Iteration 318, loss = 0.04644881\n",
      "Iteration 319, loss = 0.04794157\n",
      "Iteration 320, loss = 0.04503865\n",
      "Iteration 321, loss = 0.04803923\n",
      "Iteration 322, loss = 0.04852774\n",
      "Iteration 323, loss = 0.04634568\n",
      "Iteration 324, loss = 0.04719472\n",
      "Iteration 325, loss = 0.04515541\n",
      "Iteration 326, loss = 0.04397066\n",
      "Iteration 327, loss = 0.04764472\n",
      "Iteration 328, loss = 0.04523426\n",
      "Iteration 329, loss = 0.04734854\n",
      "Iteration 330, loss = 0.04346019\n",
      "Iteration 331, loss = 0.04449885\n",
      "Iteration 332, loss = 0.04284072\n",
      "Iteration 333, loss = 0.04417811\n",
      "Iteration 334, loss = 0.04378691\n",
      "Iteration 335, loss = 0.04226795\n",
      "Iteration 336, loss = 0.04304820\n",
      "Iteration 337, loss = 0.04254776\n",
      "Iteration 338, loss = 0.04466624\n",
      "Iteration 339, loss = 0.04392903\n",
      "Iteration 340, loss = 0.04377717\n",
      "Iteration 341, loss = 0.04197525\n",
      "Iteration 342, loss = 0.04208536\n",
      "Iteration 343, loss = 0.04241945\n",
      "Iteration 344, loss = 0.04353913\n",
      "Iteration 345, loss = 0.04238211\n",
      "Iteration 346, loss = 0.04424324\n",
      "Iteration 347, loss = 0.04334812\n",
      "Iteration 348, loss = 0.04456855\n",
      "Iteration 349, loss = 0.04123001\n",
      "Iteration 350, loss = 0.03992269\n",
      "Iteration 351, loss = 0.04244723\n",
      "Iteration 352, loss = 0.03995492\n",
      "Iteration 353, loss = 0.04004218\n",
      "Iteration 354, loss = 0.03957546\n",
      "Iteration 355, loss = 0.04077198\n",
      "Iteration 356, loss = 0.04067851\n",
      "Iteration 357, loss = 0.04079653\n",
      "Iteration 358, loss = 0.03892019\n",
      "Iteration 359, loss = 0.03908532\n",
      "Iteration 360, loss = 0.04183574\n",
      "Iteration 361, loss = 0.03841060\n",
      "Iteration 362, loss = 0.03829784\n",
      "Iteration 363, loss = 0.04048683\n",
      "Iteration 364, loss = 0.03964345\n",
      "Iteration 365, loss = 0.03863938\n",
      "Iteration 366, loss = 0.03834692\n",
      "Iteration 367, loss = 0.03947123\n",
      "Iteration 368, loss = 0.03836603\n",
      "Iteration 369, loss = 0.03943943\n",
      "Iteration 370, loss = 0.03764332\n",
      "Iteration 371, loss = 0.04657478\n",
      "Iteration 372, loss = 0.04054001\n",
      "Iteration 373, loss = 0.03884730\n",
      "Iteration 374, loss = 0.04010212\n",
      "Iteration 375, loss = 0.03745244\n",
      "Iteration 376, loss = 0.03684152\n",
      "Iteration 377, loss = 0.03605723\n",
      "Iteration 378, loss = 0.03979211\n",
      "Iteration 379, loss = 0.03880271\n",
      "Iteration 380, loss = 0.03737162\n",
      "Iteration 381, loss = 0.03795492\n",
      "Iteration 382, loss = 0.04008160\n",
      "Iteration 383, loss = 0.04008228\n",
      "Iteration 384, loss = 0.03630333\n",
      "Iteration 385, loss = 0.03534089\n",
      "Iteration 386, loss = 0.03584226\n",
      "Iteration 387, loss = 0.04194767\n",
      "Iteration 388, loss = 0.03595854\n",
      "Iteration 389, loss = 0.03535831\n",
      "Iteration 390, loss = 0.03672082\n",
      "Iteration 391, loss = 0.03500335\n",
      "Iteration 392, loss = 0.03473764\n",
      "Iteration 393, loss = 0.03502206\n",
      "Iteration 394, loss = 0.04118716\n",
      "Iteration 395, loss = 0.03841034\n",
      "Iteration 396, loss = 0.03867776\n",
      "Iteration 397, loss = 0.03576421\n",
      "Iteration 398, loss = 0.03518292\n",
      "Iteration 399, loss = 0.03417626\n",
      "Iteration 400, loss = 0.03468497\n",
      "Iteration 401, loss = 0.03400697\n",
      "Iteration 402, loss = 0.03528007\n",
      "Iteration 403, loss = 0.03673798\n",
      "Iteration 404, loss = 0.03472121\n",
      "Iteration 405, loss = 0.03580982\n",
      "Iteration 406, loss = 0.03485065\n",
      "Iteration 407, loss = 0.03321374\n",
      "Iteration 408, loss = 0.03303320\n",
      "Iteration 409, loss = 0.03333715\n",
      "Iteration 410, loss = 0.03247712\n",
      "Iteration 411, loss = 0.03235704\n",
      "Iteration 412, loss = 0.03308981\n",
      "Iteration 413, loss = 0.03248058\n",
      "Iteration 414, loss = 0.03234965\n",
      "Iteration 415, loss = 0.03262748\n",
      "Iteration 416, loss = 0.03355780\n",
      "Iteration 417, loss = 0.03330912\n",
      "Iteration 418, loss = 0.03668811\n",
      "Iteration 419, loss = 0.03806436\n",
      "Iteration 420, loss = 0.03130914\n",
      "Iteration 421, loss = 0.03340861\n",
      "Iteration 422, loss = 0.03369839\n",
      "Iteration 423, loss = 0.03487083\n",
      "Iteration 424, loss = 0.03400803\n",
      "Iteration 425, loss = 0.03445209\n",
      "Iteration 426, loss = 0.03425759\n",
      "Iteration 427, loss = 0.03275479\n",
      "Iteration 428, loss = 0.03245845\n",
      "Iteration 429, loss = 0.04028738\n",
      "Iteration 430, loss = 0.03315308\n",
      "Iteration 431, loss = 0.03361607\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68862966\n",
      "Iteration 2, loss = 0.67497964\n",
      "Iteration 3, loss = 0.66677465\n",
      "Iteration 4, loss = 0.66060545\n",
      "Iteration 5, loss = 0.65231930\n",
      "Iteration 6, loss = 0.64126849\n",
      "Iteration 7, loss = 0.62627869\n",
      "Iteration 8, loss = 0.60925582\n",
      "Iteration 9, loss = 0.59038550\n",
      "Iteration 10, loss = 0.56937268\n",
      "Iteration 11, loss = 0.54663088\n",
      "Iteration 12, loss = 0.52368861\n",
      "Iteration 13, loss = 0.50030877\n",
      "Iteration 14, loss = 0.47882240\n",
      "Iteration 15, loss = 0.45855353\n",
      "Iteration 16, loss = 0.43934796\n",
      "Iteration 17, loss = 0.41777362\n",
      "Iteration 18, loss = 0.40008459\n",
      "Iteration 19, loss = 0.38488546\n",
      "Iteration 20, loss = 0.37161944\n",
      "Iteration 21, loss = 0.36109347\n",
      "Iteration 22, loss = 0.34905967\n",
      "Iteration 23, loss = 0.33868399\n",
      "Iteration 24, loss = 0.32993037\n",
      "Iteration 25, loss = 0.32043994\n",
      "Iteration 26, loss = 0.31196933\n",
      "Iteration 27, loss = 0.30333366\n",
      "Iteration 28, loss = 0.29640582\n",
      "Iteration 29, loss = 0.28869073\n",
      "Iteration 30, loss = 0.28111267\n",
      "Iteration 31, loss = 0.27473172\n",
      "Iteration 32, loss = 0.26774337\n",
      "Iteration 33, loss = 0.26096986\n",
      "Iteration 34, loss = 0.25685127\n",
      "Iteration 35, loss = 0.25123844\n",
      "Iteration 36, loss = 0.24417109\n",
      "Iteration 37, loss = 0.23852366\n",
      "Iteration 38, loss = 0.23287799\n",
      "Iteration 39, loss = 0.22805393\n",
      "Iteration 40, loss = 0.22337616\n",
      "Iteration 41, loss = 0.22082598\n",
      "Iteration 42, loss = 0.21629284\n",
      "Iteration 43, loss = 0.21047244\n",
      "Iteration 44, loss = 0.20464052\n",
      "Iteration 45, loss = 0.20095474\n",
      "Iteration 46, loss = 0.19715779\n",
      "Iteration 47, loss = 0.19282238\n",
      "Iteration 48, loss = 0.19111698\n",
      "Iteration 49, loss = 0.18558020\n",
      "Iteration 50, loss = 0.18223814\n",
      "Iteration 51, loss = 0.17828049\n",
      "Iteration 52, loss = 0.17526619\n",
      "Iteration 53, loss = 0.17319216\n",
      "Iteration 54, loss = 0.16894944\n",
      "Iteration 55, loss = 0.16539634\n",
      "Iteration 56, loss = 0.16273585\n",
      "Iteration 57, loss = 0.16092167\n",
      "Iteration 58, loss = 0.15698904\n",
      "Iteration 59, loss = 0.15455739\n",
      "Iteration 60, loss = 0.15416093\n",
      "Iteration 61, loss = 0.15053520\n",
      "Iteration 62, loss = 0.14716201\n",
      "Iteration 63, loss = 0.14589424\n",
      "Iteration 64, loss = 0.14381749\n",
      "Iteration 65, loss = 0.14075980\n",
      "Iteration 66, loss = 0.13945808\n",
      "Iteration 67, loss = 0.13735717\n",
      "Iteration 68, loss = 0.13435598\n",
      "Iteration 69, loss = 0.13616519\n",
      "Iteration 70, loss = 0.13032114\n",
      "Iteration 71, loss = 0.12930329\n",
      "Iteration 72, loss = 0.12811145\n",
      "Iteration 73, loss = 0.12524902\n",
      "Iteration 74, loss = 0.12751863\n",
      "Iteration 75, loss = 0.12274179\n",
      "Iteration 76, loss = 0.12209947\n",
      "Iteration 77, loss = 0.11840098\n",
      "Iteration 78, loss = 0.11736216\n",
      "Iteration 79, loss = 0.11583093\n",
      "Iteration 80, loss = 0.11449479\n",
      "Iteration 81, loss = 0.11351926\n",
      "Iteration 82, loss = 0.11134753\n",
      "Iteration 83, loss = 0.11326781\n",
      "Iteration 84, loss = 0.11006137\n",
      "Iteration 85, loss = 0.10792638\n",
      "Iteration 86, loss = 0.10602131\n",
      "Iteration 87, loss = 0.10426352\n",
      "Iteration 88, loss = 0.10283335\n",
      "Iteration 89, loss = 0.10197749\n",
      "Iteration 90, loss = 0.10202101\n",
      "Iteration 91, loss = 0.09935904\n",
      "Iteration 92, loss = 0.09827227\n",
      "Iteration 93, loss = 0.09849680\n",
      "Iteration 94, loss = 0.09650413\n",
      "Iteration 95, loss = 0.09471349\n",
      "Iteration 96, loss = 0.09369701\n",
      "Iteration 97, loss = 0.09253331\n",
      "Iteration 98, loss = 0.09241041\n",
      "Iteration 99, loss = 0.09162636\n",
      "Iteration 100, loss = 0.09040007\n",
      "Iteration 101, loss = 0.08993249\n",
      "Iteration 102, loss = 0.08776914\n",
      "Iteration 103, loss = 0.08624135\n",
      "Iteration 104, loss = 0.08559435\n",
      "Iteration 105, loss = 0.08480588\n",
      "Iteration 106, loss = 0.08330607\n",
      "Iteration 107, loss = 0.08293894\n",
      "Iteration 108, loss = 0.08188770\n",
      "Iteration 109, loss = 0.08065418\n",
      "Iteration 110, loss = 0.08057942\n",
      "Iteration 111, loss = 0.07976359\n",
      "Iteration 112, loss = 0.07848945\n",
      "Iteration 113, loss = 0.07808525\n",
      "Iteration 114, loss = 0.07782551\n",
      "Iteration 115, loss = 0.07615910\n",
      "Iteration 116, loss = 0.07563940\n",
      "Iteration 117, loss = 0.07596569\n",
      "Iteration 118, loss = 0.07574727\n",
      "Iteration 119, loss = 0.07287818\n",
      "Iteration 120, loss = 0.07385818\n",
      "Iteration 121, loss = 0.07352386\n",
      "Iteration 122, loss = 0.06983326\n",
      "Iteration 123, loss = 0.06919728\n",
      "Iteration 124, loss = 0.06902509\n",
      "Iteration 125, loss = 0.06754056\n",
      "Iteration 126, loss = 0.06703653\n",
      "Iteration 127, loss = 0.06703088\n",
      "Iteration 128, loss = 0.06544650\n",
      "Iteration 129, loss = 0.06519632\n",
      "Iteration 130, loss = 0.06439863\n",
      "Iteration 131, loss = 0.06388267\n",
      "Iteration 132, loss = 0.06339009\n",
      "Iteration 133, loss = 0.06216363\n",
      "Iteration 134, loss = 0.06185981\n",
      "Iteration 135, loss = 0.06068260\n",
      "Iteration 136, loss = 0.06111749\n",
      "Iteration 137, loss = 0.06107024\n",
      "Iteration 138, loss = 0.05937643\n",
      "Iteration 139, loss = 0.05990848\n",
      "Iteration 140, loss = 0.05918201\n",
      "Iteration 141, loss = 0.05775381\n",
      "Iteration 142, loss = 0.05756302\n",
      "Iteration 143, loss = 0.05791741\n",
      "Iteration 144, loss = 0.05733784\n",
      "Iteration 145, loss = 0.05978229\n",
      "Iteration 146, loss = 0.05654839\n",
      "Iteration 147, loss = 0.05664626\n",
      "Iteration 148, loss = 0.05452131\n",
      "Iteration 149, loss = 0.05417469\n",
      "Iteration 150, loss = 0.05335466\n",
      "Iteration 151, loss = 0.05467435\n",
      "Iteration 152, loss = 0.05408266\n",
      "Iteration 153, loss = 0.05270901\n",
      "Iteration 154, loss = 0.05129800\n",
      "Iteration 155, loss = 0.05097143\n",
      "Iteration 156, loss = 0.05295995\n",
      "Iteration 157, loss = 0.05333131\n",
      "Iteration 158, loss = 0.04978666\n",
      "Iteration 159, loss = 0.04914059\n",
      "Iteration 160, loss = 0.05517432\n",
      "Iteration 161, loss = 0.05446173\n",
      "Iteration 162, loss = 0.04886122\n",
      "Iteration 163, loss = 0.04855669\n",
      "Iteration 164, loss = 0.04764980\n",
      "Iteration 165, loss = 0.04716081\n",
      "Iteration 166, loss = 0.04850077\n",
      "Iteration 167, loss = 0.04791020\n",
      "Iteration 168, loss = 0.04604836\n",
      "Iteration 169, loss = 0.04656009\n",
      "Iteration 170, loss = 0.04479952\n",
      "Iteration 171, loss = 0.04503051\n",
      "Iteration 172, loss = 0.04547674\n",
      "Iteration 173, loss = 0.04410035\n",
      "Iteration 174, loss = 0.04431564\n",
      "Iteration 175, loss = 0.04467857\n",
      "Iteration 176, loss = 0.04362458\n",
      "Iteration 177, loss = 0.04385763\n",
      "Iteration 178, loss = 0.04609074\n",
      "Iteration 179, loss = 0.04425138\n",
      "Iteration 180, loss = 0.04176905\n",
      "Iteration 181, loss = 0.04227875\n",
      "Iteration 182, loss = 0.04216262\n",
      "Iteration 183, loss = 0.04294649\n",
      "Iteration 184, loss = 0.04091432\n",
      "Iteration 185, loss = 0.04122148\n",
      "Iteration 186, loss = 0.04115085\n",
      "Iteration 187, loss = 0.04102621\n",
      "Iteration 188, loss = 0.04221781\n",
      "Iteration 189, loss = 0.04126024\n",
      "Iteration 190, loss = 0.04225335\n",
      "Iteration 191, loss = 0.03999698\n",
      "Iteration 192, loss = 0.04152566\n",
      "Iteration 193, loss = 0.03937496\n",
      "Iteration 194, loss = 0.03826313\n",
      "Iteration 195, loss = 0.03787261\n",
      "Iteration 196, loss = 0.03813596\n",
      "Iteration 197, loss = 0.03835856\n",
      "Iteration 198, loss = 0.03674084\n",
      "Iteration 199, loss = 0.03726068\n",
      "Iteration 200, loss = 0.03692263\n",
      "Iteration 201, loss = 0.03927685\n",
      "Iteration 202, loss = 0.03600668\n",
      "Iteration 203, loss = 0.03755278\n",
      "Iteration 204, loss = 0.03804845\n",
      "Iteration 205, loss = 0.03615979\n",
      "Iteration 206, loss = 0.03652113\n",
      "Iteration 207, loss = 0.03527182\n",
      "Iteration 208, loss = 0.03511292\n",
      "Iteration 209, loss = 0.03539161\n",
      "Iteration 210, loss = 0.03571788\n",
      "Iteration 211, loss = 0.03511784\n",
      "Iteration 212, loss = 0.03374898\n",
      "Iteration 213, loss = 0.03347137\n",
      "Iteration 214, loss = 0.03889454\n",
      "Iteration 215, loss = 0.03481398\n",
      "Iteration 216, loss = 0.03357124\n",
      "Iteration 217, loss = 0.03248339\n",
      "Iteration 218, loss = 0.03261410\n",
      "Iteration 219, loss = 0.03274949\n",
      "Iteration 220, loss = 0.03244451\n",
      "Iteration 221, loss = 0.03260624\n",
      "Iteration 222, loss = 0.03174550\n",
      "Iteration 223, loss = 0.03120688\n",
      "Iteration 224, loss = 0.03212761\n",
      "Iteration 225, loss = 0.03167688\n",
      "Iteration 226, loss = 0.03327767\n",
      "Iteration 227, loss = 0.03529322\n",
      "Iteration 228, loss = 0.03185553\n",
      "Iteration 229, loss = 0.03147836\n",
      "Iteration 230, loss = 0.03062254\n",
      "Iteration 231, loss = 0.03035138\n",
      "Iteration 232, loss = 0.03112491\n",
      "Iteration 233, loss = 0.03006694\n",
      "Iteration 234, loss = 0.02930227\n",
      "Iteration 235, loss = 0.03015429\n",
      "Iteration 236, loss = 0.03074453\n",
      "Iteration 237, loss = 0.03118881\n",
      "Iteration 238, loss = 0.02869169\n",
      "Iteration 239, loss = 0.02974055\n",
      "Iteration 240, loss = 0.03028107\n",
      "Iteration 241, loss = 0.03050557\n",
      "Iteration 242, loss = 0.02923151\n",
      "Iteration 243, loss = 0.02929988\n",
      "Iteration 244, loss = 0.02838694\n",
      "Iteration 245, loss = 0.02777569\n",
      "Iteration 246, loss = 0.02863250\n",
      "Iteration 247, loss = 0.02738821\n",
      "Iteration 248, loss = 0.02725721\n",
      "Iteration 249, loss = 0.02677506\n",
      "Iteration 250, loss = 0.02709404\n",
      "Iteration 251, loss = 0.02957428\n",
      "Iteration 252, loss = 0.02804126\n",
      "Iteration 253, loss = 0.02681261\n",
      "Iteration 254, loss = 0.03028836\n",
      "Iteration 255, loss = 0.02694681\n",
      "Iteration 256, loss = 0.02677066\n",
      "Iteration 257, loss = 0.02605776\n",
      "Iteration 258, loss = 0.02640606\n",
      "Iteration 259, loss = 0.02705751\n",
      "Iteration 260, loss = 0.02590901\n",
      "Iteration 261, loss = 0.02711022\n",
      "Iteration 262, loss = 0.02461579\n",
      "Iteration 263, loss = 0.02420191\n",
      "Iteration 264, loss = 0.02544736\n",
      "Iteration 265, loss = 0.02427644\n",
      "Iteration 266, loss = 0.02450008\n",
      "Iteration 267, loss = 0.02399565\n",
      "Iteration 268, loss = 0.02443299\n",
      "Iteration 269, loss = 0.02647743\n",
      "Iteration 270, loss = 0.02422793\n",
      "Iteration 271, loss = 0.02720515\n",
      "Iteration 272, loss = 0.02397554\n",
      "Iteration 273, loss = 0.02360657\n",
      "Iteration 274, loss = 0.02302031\n",
      "Iteration 275, loss = 0.02265688\n",
      "Iteration 276, loss = 0.02298121\n",
      "Iteration 277, loss = 0.02312557\n",
      "Iteration 278, loss = 0.02359556\n",
      "Iteration 279, loss = 0.02579131\n",
      "Iteration 280, loss = 0.02456894\n",
      "Iteration 281, loss = 0.02492658\n",
      "Iteration 282, loss = 0.02294765\n",
      "Iteration 283, loss = 0.02224070\n",
      "Iteration 284, loss = 0.02199826\n",
      "Iteration 285, loss = 0.02194819\n",
      "Iteration 286, loss = 0.02426771\n",
      "Iteration 287, loss = 0.02181585\n",
      "Iteration 288, loss = 0.02220550\n",
      "Iteration 289, loss = 0.02317533\n",
      "Iteration 290, loss = 0.02176494\n",
      "Iteration 291, loss = 0.02121785\n",
      "Iteration 292, loss = 0.02316516\n",
      "Iteration 293, loss = 0.02253877\n",
      "Iteration 294, loss = 0.02299249\n",
      "Iteration 295, loss = 0.02245036\n",
      "Iteration 296, loss = 0.02113070\n",
      "Iteration 297, loss = 0.02076215\n",
      "Iteration 298, loss = 0.02281540\n",
      "Iteration 299, loss = 0.02425112\n",
      "Iteration 300, loss = 0.02509906\n",
      "Iteration 301, loss = 0.02272553\n",
      "Iteration 302, loss = 0.02141986\n",
      "Iteration 303, loss = 0.02050843\n",
      "Iteration 304, loss = 0.02124826\n",
      "Iteration 305, loss = 0.02084365\n",
      "Iteration 306, loss = 0.02217789\n",
      "Iteration 307, loss = 0.02102639\n",
      "Iteration 308, loss = 0.02330772\n",
      "Iteration 309, loss = 0.02228422\n",
      "Iteration 310, loss = 0.01928181\n",
      "Iteration 311, loss = 0.01956585\n",
      "Iteration 312, loss = 0.01915976\n",
      "Iteration 313, loss = 0.01865044\n",
      "Iteration 314, loss = 0.01918428\n",
      "Iteration 315, loss = 0.02019915\n",
      "Iteration 316, loss = 0.02008043\n",
      "Iteration 317, loss = 0.02143107\n",
      "Iteration 318, loss = 0.01959817\n",
      "Iteration 319, loss = 0.02065676\n",
      "Iteration 320, loss = 0.01942307\n",
      "Iteration 321, loss = 0.01925077\n",
      "Iteration 322, loss = 0.02043540\n",
      "Iteration 323, loss = 0.01949423\n",
      "Iteration 324, loss = 0.01845446\n",
      "Iteration 325, loss = 0.01803284\n",
      "Iteration 326, loss = 0.01780112\n",
      "Iteration 327, loss = 0.01731341\n",
      "Iteration 328, loss = 0.01838727\n",
      "Iteration 329, loss = 0.01872459\n",
      "Iteration 330, loss = 0.02238513\n",
      "Iteration 331, loss = 0.01876573\n",
      "Iteration 332, loss = 0.01734758\n",
      "Iteration 333, loss = 0.01738201\n",
      "Iteration 334, loss = 0.01753794\n",
      "Iteration 335, loss = 0.02105370\n",
      "Iteration 336, loss = 0.01789932\n",
      "Iteration 337, loss = 0.01805419\n",
      "Iteration 338, loss = 0.01750202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69074801\n",
      "Iteration 2, loss = 0.67945978\n",
      "Iteration 3, loss = 0.67037664\n",
      "Iteration 4, loss = 0.66026294\n",
      "Iteration 5, loss = 0.64430554\n",
      "Iteration 6, loss = 0.62807214\n",
      "Iteration 7, loss = 0.60550835\n",
      "Iteration 8, loss = 0.58208733\n",
      "Iteration 9, loss = 0.55701473\n",
      "Iteration 10, loss = 0.52920699\n",
      "Iteration 11, loss = 0.49792966\n",
      "Iteration 12, loss = 0.46065307\n",
      "Iteration 13, loss = 0.42425846\n",
      "Iteration 14, loss = 0.39161823\n",
      "Iteration 15, loss = 0.36219991\n",
      "Iteration 16, loss = 0.33605501\n",
      "Iteration 17, loss = 0.31310562\n",
      "Iteration 18, loss = 0.29253000\n",
      "Iteration 19, loss = 0.27416632\n",
      "Iteration 20, loss = 0.25839113\n",
      "Iteration 21, loss = 0.24390108\n",
      "Iteration 22, loss = 0.23104372\n",
      "Iteration 23, loss = 0.22011216\n",
      "Iteration 24, loss = 0.20924929\n",
      "Iteration 25, loss = 0.19875617\n",
      "Iteration 26, loss = 0.18574323\n",
      "Iteration 27, loss = 0.17606621\n",
      "Iteration 28, loss = 0.16679654\n",
      "Iteration 29, loss = 0.15846753\n",
      "Iteration 30, loss = 0.15053696\n",
      "Iteration 31, loss = 0.14305103\n",
      "Iteration 32, loss = 0.13682826\n",
      "Iteration 33, loss = 0.13048313\n",
      "Iteration 34, loss = 0.12499264\n",
      "Iteration 35, loss = 0.11991860\n",
      "Iteration 36, loss = 0.11464136\n",
      "Iteration 37, loss = 0.11083989\n",
      "Iteration 38, loss = 0.10563572\n",
      "Iteration 39, loss = 0.10241581\n",
      "Iteration 40, loss = 0.09824603\n",
      "Iteration 41, loss = 0.09462850\n",
      "Iteration 42, loss = 0.09167247\n",
      "Iteration 43, loss = 0.08859997\n",
      "Iteration 44, loss = 0.08567543\n",
      "Iteration 45, loss = 0.08271937\n",
      "Iteration 46, loss = 0.08113814\n",
      "Iteration 47, loss = 0.07819120\n",
      "Iteration 48, loss = 0.07560233\n",
      "Iteration 49, loss = 0.07386448\n",
      "Iteration 50, loss = 0.07087052\n",
      "Iteration 51, loss = 0.06818296\n",
      "Iteration 52, loss = 0.06608206\n",
      "Iteration 53, loss = 0.06511545\n",
      "Iteration 54, loss = 0.06387874\n",
      "Iteration 55, loss = 0.06146644\n",
      "Iteration 56, loss = 0.05956973\n",
      "Iteration 57, loss = 0.05789332\n",
      "Iteration 58, loss = 0.05817176\n",
      "Iteration 59, loss = 0.05531399\n",
      "Iteration 60, loss = 0.05462432\n",
      "Iteration 61, loss = 0.05280800\n",
      "Iteration 62, loss = 0.05196350\n",
      "Iteration 63, loss = 0.05062259\n",
      "Iteration 64, loss = 0.04965642\n",
      "Iteration 65, loss = 0.04878254\n",
      "Iteration 66, loss = 0.04758298\n",
      "Iteration 67, loss = 0.04670404\n",
      "Iteration 68, loss = 0.04524809\n",
      "Iteration 69, loss = 0.04495104\n",
      "Iteration 70, loss = 0.04374189\n",
      "Iteration 71, loss = 0.04414453\n",
      "Iteration 72, loss = 0.04206947\n",
      "Iteration 73, loss = 0.04163756\n",
      "Iteration 74, loss = 0.04144896\n",
      "Iteration 75, loss = 0.04032355\n",
      "Iteration 76, loss = 0.04004429\n",
      "Iteration 77, loss = 0.03923008\n",
      "Iteration 78, loss = 0.03915519\n",
      "Iteration 79, loss = 0.03810476\n",
      "Iteration 80, loss = 0.03714965\n",
      "Iteration 81, loss = 0.03778450\n",
      "Iteration 82, loss = 0.03582506\n",
      "Iteration 83, loss = 0.03576882\n",
      "Iteration 84, loss = 0.03491786\n",
      "Iteration 85, loss = 0.03423154\n",
      "Iteration 86, loss = 0.03410525\n",
      "Iteration 87, loss = 0.03330100\n",
      "Iteration 88, loss = 0.03348323\n",
      "Iteration 89, loss = 0.03220690\n",
      "Iteration 90, loss = 0.03199316\n",
      "Iteration 91, loss = 0.03199564\n",
      "Iteration 92, loss = 0.03166071\n",
      "Iteration 93, loss = 0.03050510\n",
      "Iteration 94, loss = 0.03007270\n",
      "Iteration 95, loss = 0.03043088\n",
      "Iteration 96, loss = 0.02996530\n",
      "Iteration 97, loss = 0.02920677\n",
      "Iteration 98, loss = 0.02835970\n",
      "Iteration 99, loss = 0.02903386\n",
      "Iteration 100, loss = 0.02813947\n",
      "Iteration 101, loss = 0.02791977\n",
      "Iteration 102, loss = 0.02746389\n",
      "Iteration 103, loss = 0.02683366\n",
      "Iteration 104, loss = 0.02667689\n",
      "Iteration 105, loss = 0.02654201\n",
      "Iteration 106, loss = 0.02604887\n",
      "Iteration 107, loss = 0.02549019\n",
      "Iteration 108, loss = 0.02535074\n",
      "Iteration 109, loss = 0.02537060\n",
      "Iteration 110, loss = 0.02538236\n",
      "Iteration 111, loss = 0.02433969\n",
      "Iteration 112, loss = 0.02450360\n",
      "Iteration 113, loss = 0.02364838\n",
      "Iteration 114, loss = 0.02426523\n",
      "Iteration 115, loss = 0.02389506\n",
      "Iteration 116, loss = 0.02293808\n",
      "Iteration 117, loss = 0.02284074\n",
      "Iteration 118, loss = 0.02342957\n",
      "Iteration 119, loss = 0.02289066\n",
      "Iteration 120, loss = 0.02236625\n",
      "Iteration 121, loss = 0.02199629\n",
      "Iteration 122, loss = 0.02178888\n",
      "Iteration 123, loss = 0.02184740\n",
      "Iteration 124, loss = 0.02318360\n",
      "Iteration 125, loss = 0.02130352\n",
      "Iteration 126, loss = 0.02125086\n",
      "Iteration 127, loss = 0.02135597\n",
      "Iteration 128, loss = 0.02035428\n",
      "Iteration 129, loss = 0.02037391\n",
      "Iteration 130, loss = 0.02052692\n",
      "Iteration 131, loss = 0.02032503\n",
      "Iteration 132, loss = 0.02092270\n",
      "Iteration 133, loss = 0.01996226\n",
      "Iteration 134, loss = 0.01923960\n",
      "Iteration 135, loss = 0.01947043\n",
      "Iteration 136, loss = 0.01893481\n",
      "Iteration 137, loss = 0.01992090\n",
      "Iteration 138, loss = 0.01935390\n",
      "Iteration 139, loss = 0.01858574\n",
      "Iteration 140, loss = 0.01848607\n",
      "Iteration 141, loss = 0.01944418\n",
      "Iteration 142, loss = 0.01856848\n",
      "Iteration 143, loss = 0.01821385\n",
      "Iteration 144, loss = 0.01752036\n",
      "Iteration 145, loss = 0.01732409\n",
      "Iteration 146, loss = 0.01851809\n",
      "Iteration 147, loss = 0.01700940\n",
      "Iteration 148, loss = 0.01690488\n",
      "Iteration 149, loss = 0.01703871\n",
      "Iteration 150, loss = 0.01650371\n",
      "Iteration 151, loss = 0.01652773\n",
      "Iteration 152, loss = 0.01631323\n",
      "Iteration 153, loss = 0.01634280\n",
      "Iteration 154, loss = 0.01613913\n",
      "Iteration 155, loss = 0.01543964\n",
      "Iteration 156, loss = 0.01566063\n",
      "Iteration 157, loss = 0.01523639\n",
      "Iteration 158, loss = 0.01511646\n",
      "Iteration 159, loss = 0.01512486\n",
      "Iteration 160, loss = 0.01487153\n",
      "Iteration 161, loss = 0.01470312\n",
      "Iteration 162, loss = 0.01485370\n",
      "Iteration 163, loss = 0.01466829\n",
      "Iteration 164, loss = 0.01483254\n",
      "Iteration 165, loss = 0.01444323\n",
      "Iteration 166, loss = 0.01445043\n",
      "Iteration 167, loss = 0.01398253\n",
      "Iteration 168, loss = 0.01392768\n",
      "Iteration 169, loss = 0.01393284\n",
      "Iteration 170, loss = 0.01362141\n",
      "Iteration 171, loss = 0.01391079\n",
      "Iteration 172, loss = 0.01321890\n",
      "Iteration 173, loss = 0.01341072\n",
      "Iteration 174, loss = 0.01435398\n",
      "Iteration 175, loss = 0.01390033\n",
      "Iteration 176, loss = 0.01319677\n",
      "Iteration 177, loss = 0.01338107\n",
      "Iteration 178, loss = 0.01281128\n",
      "Iteration 179, loss = 0.01336012\n",
      "Iteration 180, loss = 0.01270908\n",
      "Iteration 181, loss = 0.01268382\n",
      "Iteration 182, loss = 0.01210884\n",
      "Iteration 183, loss = 0.01249010\n",
      "Iteration 184, loss = 0.01256398\n",
      "Iteration 185, loss = 0.01335896\n",
      "Iteration 186, loss = 0.01194225\n",
      "Iteration 187, loss = 0.01231074\n",
      "Iteration 188, loss = 0.01288936\n",
      "Iteration 189, loss = 0.01170390\n",
      "Iteration 190, loss = 0.01203998\n",
      "Iteration 191, loss = 0.01221052\n",
      "Iteration 192, loss = 0.01210646\n",
      "Iteration 193, loss = 0.01148005\n",
      "Iteration 194, loss = 0.01156575\n",
      "Iteration 195, loss = 0.01284271\n",
      "Iteration 196, loss = 0.01150123\n",
      "Iteration 197, loss = 0.01279713\n",
      "Iteration 198, loss = 0.01275173\n",
      "Iteration 199, loss = 0.01173806\n",
      "Iteration 200, loss = 0.01145083\n",
      "Iteration 201, loss = 0.01102082\n",
      "Iteration 202, loss = 0.01060595\n",
      "Iteration 203, loss = 0.01131431\n",
      "Iteration 204, loss = 0.01079229\n",
      "Iteration 205, loss = 0.01090309\n",
      "Iteration 206, loss = 0.01111740\n",
      "Iteration 207, loss = 0.01055339\n",
      "Iteration 208, loss = 0.01103819\n",
      "Iteration 209, loss = 0.01057731\n",
      "Iteration 210, loss = 0.01032963\n",
      "Iteration 211, loss = 0.01028543\n",
      "Iteration 212, loss = 0.01040882\n",
      "Iteration 213, loss = 0.01112966\n",
      "Iteration 214, loss = 0.01035305\n",
      "Iteration 215, loss = 0.00999891\n",
      "Iteration 216, loss = 0.01028942\n",
      "Iteration 217, loss = 0.01023580\n",
      "Iteration 218, loss = 0.01034474\n",
      "Iteration 219, loss = 0.00982032\n",
      "Iteration 220, loss = 0.01073368\n",
      "Iteration 221, loss = 0.00946801\n",
      "Iteration 222, loss = 0.00986403\n",
      "Iteration 223, loss = 0.00989892\n",
      "Iteration 224, loss = 0.01007805\n",
      "Iteration 225, loss = 0.01065332\n",
      "Iteration 226, loss = 0.01004980\n",
      "Iteration 227, loss = 0.01354961\n",
      "Iteration 228, loss = 0.01086727\n",
      "Iteration 229, loss = 0.00917220\n",
      "Iteration 230, loss = 0.00935609\n",
      "Iteration 231, loss = 0.01011447\n",
      "Iteration 232, loss = 0.00923537\n",
      "Iteration 233, loss = 0.00907151\n",
      "Iteration 234, loss = 0.01000047\n",
      "Iteration 235, loss = 0.00878208\n",
      "Iteration 236, loss = 0.00914923\n",
      "Iteration 237, loss = 0.00904029\n",
      "Iteration 238, loss = 0.00965590\n",
      "Iteration 239, loss = 0.00915808\n",
      "Iteration 240, loss = 0.00996398\n",
      "Iteration 241, loss = 0.00954321\n",
      "Iteration 242, loss = 0.00900942\n",
      "Iteration 243, loss = 0.00936647\n",
      "Iteration 244, loss = 0.00921196\n",
      "Iteration 245, loss = 0.00887027\n",
      "Iteration 246, loss = 0.00865529\n",
      "Iteration 247, loss = 0.00889366\n",
      "Iteration 248, loss = 0.00858769\n",
      "Iteration 249, loss = 0.00909595\n",
      "Iteration 250, loss = 0.00828828\n",
      "Iteration 251, loss = 0.01014748\n",
      "Iteration 252, loss = 0.00875998\n",
      "Iteration 253, loss = 0.00840081\n",
      "Iteration 254, loss = 0.00830930\n",
      "Iteration 255, loss = 0.00814176\n",
      "Iteration 256, loss = 0.00858561\n",
      "Iteration 257, loss = 0.00806456\n",
      "Iteration 258, loss = 0.00774720\n",
      "Iteration 259, loss = 0.00794474\n",
      "Iteration 260, loss = 0.00862842\n",
      "Iteration 261, loss = 0.01184795\n",
      "Iteration 262, loss = 0.00828780\n",
      "Iteration 263, loss = 0.00844651\n",
      "Iteration 264, loss = 0.00745503\n",
      "Iteration 265, loss = 0.00764028\n",
      "Iteration 266, loss = 0.01035584\n",
      "Iteration 267, loss = 0.00843453\n",
      "Iteration 268, loss = 0.00851963\n",
      "Iteration 269, loss = 0.00800297\n",
      "Iteration 270, loss = 0.00819492\n",
      "Iteration 271, loss = 0.00772850\n",
      "Iteration 272, loss = 0.00745910\n",
      "Iteration 273, loss = 0.00842786\n",
      "Iteration 274, loss = 0.00777856\n",
      "Iteration 275, loss = 0.00811193\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.83021665\n",
      "Iteration 2, loss = 0.76191863\n",
      "Iteration 3, loss = 0.71195677\n",
      "Iteration 4, loss = 0.67330745\n",
      "Iteration 5, loss = 0.64308695\n",
      "Iteration 6, loss = 0.61275912\n",
      "Iteration 7, loss = 0.57945709\n",
      "Iteration 8, loss = 0.54687810\n",
      "Iteration 9, loss = 0.51641155\n",
      "Iteration 10, loss = 0.48688217\n",
      "Iteration 11, loss = 0.45194655\n",
      "Iteration 12, loss = 0.41591176\n",
      "Iteration 13, loss = 0.38039012\n",
      "Iteration 14, loss = 0.34274973\n",
      "Iteration 15, loss = 0.30234826\n",
      "Iteration 16, loss = 0.26126838\n",
      "Iteration 17, loss = 0.22088405\n",
      "Iteration 18, loss = 0.18533683\n",
      "Iteration 19, loss = 0.15601956\n",
      "Iteration 20, loss = 0.13275758\n",
      "Iteration 21, loss = 0.11539356\n",
      "Iteration 22, loss = 0.10215857\n",
      "Iteration 23, loss = 0.09244979\n",
      "Iteration 24, loss = 0.08478447\n",
      "Iteration 25, loss = 0.07905647\n",
      "Iteration 26, loss = 0.07380529\n",
      "Iteration 27, loss = 0.06992341\n",
      "Iteration 28, loss = 0.06609861\n",
      "Iteration 29, loss = 0.06273526\n",
      "Iteration 30, loss = 0.05966760\n",
      "Iteration 31, loss = 0.05763544\n",
      "Iteration 32, loss = 0.05468844\n",
      "Iteration 33, loss = 0.05274167\n",
      "Iteration 34, loss = 0.05126671\n",
      "Iteration 35, loss = 0.04896113\n",
      "Iteration 36, loss = 0.04710914\n",
      "Iteration 37, loss = 0.04570421\n",
      "Iteration 38, loss = 0.04379992\n",
      "Iteration 39, loss = 0.04250922\n",
      "Iteration 40, loss = 0.04132464\n",
      "Iteration 41, loss = 0.04018691\n",
      "Iteration 42, loss = 0.03922323\n",
      "Iteration 43, loss = 0.03899770\n",
      "Iteration 44, loss = 0.03670288\n",
      "Iteration 45, loss = 0.03623071\n",
      "Iteration 46, loss = 0.03504362\n",
      "Iteration 47, loss = 0.03402023\n",
      "Iteration 48, loss = 0.03333966\n",
      "Iteration 49, loss = 0.03232531\n",
      "Iteration 50, loss = 0.03155544\n",
      "Iteration 51, loss = 0.03145974\n",
      "Iteration 52, loss = 0.03017183\n",
      "Iteration 53, loss = 0.02945046\n",
      "Iteration 54, loss = 0.02911598\n",
      "Iteration 55, loss = 0.02970894\n",
      "Iteration 56, loss = 0.02752877\n",
      "Iteration 57, loss = 0.02711858\n",
      "Iteration 58, loss = 0.02645968\n",
      "Iteration 59, loss = 0.02594506\n",
      "Iteration 60, loss = 0.02566642\n",
      "Iteration 61, loss = 0.02484692\n",
      "Iteration 62, loss = 0.02462247\n",
      "Iteration 63, loss = 0.02393362\n",
      "Iteration 64, loss = 0.02392230\n",
      "Iteration 65, loss = 0.02347448\n",
      "Iteration 66, loss = 0.02321482\n",
      "Iteration 67, loss = 0.02284848\n",
      "Iteration 68, loss = 0.02249758\n",
      "Iteration 69, loss = 0.02180212\n",
      "Iteration 70, loss = 0.02170801\n",
      "Iteration 71, loss = 0.02140885\n",
      "Iteration 72, loss = 0.02111780\n",
      "Iteration 73, loss = 0.02084920\n",
      "Iteration 74, loss = 0.02047450\n",
      "Iteration 75, loss = 0.02054726\n",
      "Iteration 76, loss = 0.02067163\n",
      "Iteration 77, loss = 0.02005828\n",
      "Iteration 78, loss = 0.01996203\n",
      "Iteration 79, loss = 0.01920890\n",
      "Iteration 80, loss = 0.01908045\n",
      "Iteration 81, loss = 0.01912160\n",
      "Iteration 82, loss = 0.01890690\n",
      "Iteration 83, loss = 0.01890898\n",
      "Iteration 84, loss = 0.01845603\n",
      "Iteration 85, loss = 0.01795258\n",
      "Iteration 86, loss = 0.01788764\n",
      "Iteration 87, loss = 0.01811141\n",
      "Iteration 88, loss = 0.01741481\n",
      "Iteration 89, loss = 0.01745089\n",
      "Iteration 90, loss = 0.01743307\n",
      "Iteration 91, loss = 0.01682276\n",
      "Iteration 92, loss = 0.01678225\n",
      "Iteration 93, loss = 0.01673926\n",
      "Iteration 94, loss = 0.01660766\n",
      "Iteration 95, loss = 0.01630056\n",
      "Iteration 96, loss = 0.01718305\n",
      "Iteration 97, loss = 0.01606747\n",
      "Iteration 98, loss = 0.01581455\n",
      "Iteration 99, loss = 0.01583150\n",
      "Iteration 100, loss = 0.01551290\n",
      "Iteration 101, loss = 0.01564456\n",
      "Iteration 102, loss = 0.01538579\n",
      "Iteration 103, loss = 0.01508290\n",
      "Iteration 104, loss = 0.01519339\n",
      "Iteration 105, loss = 0.01489907\n",
      "Iteration 106, loss = 0.01468221\n",
      "Iteration 107, loss = 0.01442036\n",
      "Iteration 108, loss = 0.01448707\n",
      "Iteration 109, loss = 0.01416870\n",
      "Iteration 110, loss = 0.01447024\n",
      "Iteration 111, loss = 0.01408271\n",
      "Iteration 112, loss = 0.01363338\n",
      "Iteration 113, loss = 0.01388026\n",
      "Iteration 114, loss = 0.01429672\n",
      "Iteration 115, loss = 0.01365879\n",
      "Iteration 116, loss = 0.01344071\n",
      "Iteration 117, loss = 0.01385600\n",
      "Iteration 118, loss = 0.01316980\n",
      "Iteration 119, loss = 0.01312502\n",
      "Iteration 120, loss = 0.01316517\n",
      "Iteration 121, loss = 0.01353319\n",
      "Iteration 122, loss = 0.01289465\n",
      "Iteration 123, loss = 0.01343490\n",
      "Iteration 124, loss = 0.01261857\n",
      "Iteration 125, loss = 0.01230468\n",
      "Iteration 126, loss = 0.01217072\n",
      "Iteration 127, loss = 0.01219531\n",
      "Iteration 128, loss = 0.01187359\n",
      "Iteration 129, loss = 0.01332656\n",
      "Iteration 130, loss = 0.01285666\n",
      "Iteration 131, loss = 0.01200090\n",
      "Iteration 132, loss = 0.01190133\n",
      "Iteration 133, loss = 0.01178448\n",
      "Iteration 134, loss = 0.01191615\n",
      "Iteration 135, loss = 0.01139698\n",
      "Iteration 136, loss = 0.01134507\n",
      "Iteration 137, loss = 0.01126102\n",
      "Iteration 138, loss = 0.01124209\n",
      "Iteration 139, loss = 0.01132483\n",
      "Iteration 140, loss = 0.01139691\n",
      "Iteration 141, loss = 0.01076704\n",
      "Iteration 142, loss = 0.01102194\n",
      "Iteration 143, loss = 0.01085148\n",
      "Iteration 144, loss = 0.01162107\n",
      "Iteration 145, loss = 0.01045223\n",
      "Iteration 146, loss = 0.01086913\n",
      "Iteration 147, loss = 0.01084377\n",
      "Iteration 148, loss = 0.01096981\n",
      "Iteration 149, loss = 0.01090287\n",
      "Iteration 150, loss = 0.01038064\n",
      "Iteration 151, loss = 0.01040770\n",
      "Iteration 152, loss = 0.01025721\n",
      "Iteration 153, loss = 0.01015367\n",
      "Iteration 154, loss = 0.00978836\n",
      "Iteration 155, loss = 0.00995869\n",
      "Iteration 156, loss = 0.00980387\n",
      "Iteration 157, loss = 0.00972639\n",
      "Iteration 158, loss = 0.00990440\n",
      "Iteration 159, loss = 0.00969556\n",
      "Iteration 160, loss = 0.00944363\n",
      "Iteration 161, loss = 0.00960049\n",
      "Iteration 162, loss = 0.00949425\n",
      "Iteration 163, loss = 0.00928043\n",
      "Iteration 164, loss = 0.00934369\n",
      "Iteration 165, loss = 0.00925019\n",
      "Iteration 166, loss = 0.00921915\n",
      "Iteration 167, loss = 0.00907976\n",
      "Iteration 168, loss = 0.00978887\n",
      "Iteration 169, loss = 0.00935554\n",
      "Iteration 170, loss = 0.00900467\n",
      "Iteration 171, loss = 0.00873649\n",
      "Iteration 172, loss = 0.00864352\n",
      "Iteration 173, loss = 0.00868909\n",
      "Iteration 174, loss = 0.00855995\n",
      "Iteration 175, loss = 0.00899855\n",
      "Iteration 176, loss = 0.00876210\n",
      "Iteration 177, loss = 0.00854437\n",
      "Iteration 178, loss = 0.00821957\n",
      "Iteration 179, loss = 0.00846123\n",
      "Iteration 180, loss = 0.00819972\n",
      "Iteration 181, loss = 0.00803283\n",
      "Iteration 182, loss = 0.00808886\n",
      "Iteration 183, loss = 0.00827109\n",
      "Iteration 184, loss = 0.00789340\n",
      "Iteration 185, loss = 0.00918644\n",
      "Iteration 186, loss = 0.00825618\n",
      "Iteration 187, loss = 0.00821188\n",
      "Iteration 188, loss = 0.00792228\n",
      "Iteration 189, loss = 0.00770925\n",
      "Iteration 190, loss = 0.00821454\n",
      "Iteration 191, loss = 0.00771220\n",
      "Iteration 192, loss = 0.00751030\n",
      "Iteration 193, loss = 0.00786170\n",
      "Iteration 194, loss = 0.00786132\n",
      "Iteration 195, loss = 0.00796050\n",
      "Iteration 196, loss = 0.00766622\n",
      "Iteration 197, loss = 0.00733098\n",
      "Iteration 198, loss = 0.00742517\n",
      "Iteration 199, loss = 0.00723782\n",
      "Iteration 200, loss = 0.00704916\n",
      "Iteration 201, loss = 0.00734373\n",
      "Iteration 202, loss = 0.00736151\n",
      "Iteration 203, loss = 0.00770181\n",
      "Iteration 204, loss = 0.00719790\n",
      "Iteration 205, loss = 0.00691516\n",
      "Iteration 206, loss = 0.00692974\n",
      "Iteration 207, loss = 0.00697219\n",
      "Iteration 208, loss = 0.00718564\n",
      "Iteration 209, loss = 0.00703615\n",
      "Iteration 210, loss = 0.00676079\n",
      "Iteration 211, loss = 0.00698037\n",
      "Iteration 212, loss = 0.00650259\n",
      "Iteration 213, loss = 0.00677751\n",
      "Iteration 214, loss = 0.00660784\n",
      "Iteration 215, loss = 0.00644165\n",
      "Iteration 216, loss = 0.00699135\n",
      "Iteration 217, loss = 0.00695113\n",
      "Iteration 218, loss = 0.00652623\n",
      "Iteration 219, loss = 0.00624335\n",
      "Iteration 220, loss = 0.00650980\n",
      "Iteration 221, loss = 0.00650008\n",
      "Iteration 222, loss = 0.00682206\n",
      "Iteration 223, loss = 0.00626637\n",
      "Iteration 224, loss = 0.00609541\n",
      "Iteration 225, loss = 0.00614733\n",
      "Iteration 226, loss = 0.00617119\n",
      "Iteration 227, loss = 0.00622623\n",
      "Iteration 228, loss = 0.00642906\n",
      "Iteration 229, loss = 0.00610579\n",
      "Iteration 230, loss = 0.00634871\n",
      "Iteration 231, loss = 0.00628701\n",
      "Iteration 232, loss = 0.00618067\n",
      "Iteration 233, loss = 0.00590865\n",
      "Iteration 234, loss = 0.00644210\n",
      "Iteration 235, loss = 0.00611147\n",
      "Iteration 236, loss = 0.00590719\n",
      "Iteration 237, loss = 0.00593540\n",
      "Iteration 238, loss = 0.00621560\n",
      "Iteration 239, loss = 0.00645728\n",
      "Iteration 240, loss = 0.00566711\n",
      "Iteration 241, loss = 0.00560148\n",
      "Iteration 242, loss = 0.00557266\n",
      "Iteration 243, loss = 0.00553063\n",
      "Iteration 244, loss = 0.00551055\n",
      "Iteration 245, loss = 0.00597270\n",
      "Iteration 246, loss = 0.00612041\n",
      "Iteration 247, loss = 0.00577856\n",
      "Iteration 248, loss = 0.00576034\n",
      "Iteration 249, loss = 0.00567680\n",
      "Iteration 250, loss = 0.00537785\n",
      "Iteration 251, loss = 0.00543163\n",
      "Iteration 252, loss = 0.00526872\n",
      "Iteration 253, loss = 0.00558605\n",
      "Iteration 254, loss = 0.00559752\n",
      "Iteration 255, loss = 0.00574039\n",
      "Iteration 256, loss = 0.00533454\n",
      "Iteration 257, loss = 0.00536619\n",
      "Iteration 258, loss = 0.00532167\n",
      "Iteration 259, loss = 0.00555343\n",
      "Iteration 260, loss = 0.00586614\n",
      "Iteration 261, loss = 0.00522784\n",
      "Iteration 262, loss = 0.00562522\n",
      "Iteration 263, loss = 0.00504325\n",
      "Iteration 264, loss = 0.00542001\n",
      "Iteration 265, loss = 0.00502318\n",
      "Iteration 266, loss = 0.00561740\n",
      "Iteration 267, loss = 0.00543963\n",
      "Iteration 268, loss = 0.00488531\n",
      "Iteration 269, loss = 0.00503273\n",
      "Iteration 270, loss = 0.00508312\n",
      "Iteration 271, loss = 0.00525304\n",
      "Iteration 272, loss = 0.00549305\n",
      "Iteration 273, loss = 0.00538773\n",
      "Iteration 274, loss = 0.00532904\n",
      "Iteration 275, loss = 0.00547858\n",
      "Iteration 276, loss = 0.00469976\n",
      "Iteration 277, loss = 0.00539319\n",
      "Iteration 278, loss = 0.00529918\n",
      "Iteration 279, loss = 0.00462309\n",
      "Iteration 280, loss = 0.00506066\n",
      "Iteration 281, loss = 0.00493591\n",
      "Iteration 282, loss = 0.00475945\n",
      "Iteration 283, loss = 0.00491161\n",
      "Iteration 284, loss = 0.00466516\n",
      "Iteration 285, loss = 0.00468752\n",
      "Iteration 286, loss = 0.00454555\n",
      "Iteration 287, loss = 0.00470346\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70423830\n",
      "Iteration 2, loss = 0.67296771\n",
      "Iteration 3, loss = 0.62540465\n",
      "Iteration 4, loss = 0.53821869\n",
      "Iteration 5, loss = 0.42311106\n",
      "Iteration 6, loss = 0.30686695\n",
      "Iteration 7, loss = 0.22443917\n",
      "Iteration 8, loss = 0.17111819\n",
      "Iteration 9, loss = 0.13753511\n",
      "Iteration 10, loss = 0.11432968\n",
      "Iteration 11, loss = 0.09357483\n",
      "Iteration 12, loss = 0.07902247\n",
      "Iteration 13, loss = 0.06983838\n",
      "Iteration 14, loss = 0.06328442\n",
      "Iteration 15, loss = 0.05763330\n",
      "Iteration 16, loss = 0.05286430\n",
      "Iteration 17, loss = 0.04915269\n",
      "Iteration 18, loss = 0.04538947\n",
      "Iteration 19, loss = 0.04281137\n",
      "Iteration 20, loss = 0.04027265\n",
      "Iteration 21, loss = 0.03805547\n",
      "Iteration 22, loss = 0.03593768\n",
      "Iteration 23, loss = 0.03382703\n",
      "Iteration 24, loss = 0.03219454\n",
      "Iteration 25, loss = 0.03048070\n",
      "Iteration 26, loss = 0.02926481\n",
      "Iteration 27, loss = 0.02798273\n",
      "Iteration 28, loss = 0.02648973\n",
      "Iteration 29, loss = 0.02511463\n",
      "Iteration 30, loss = 0.02324789\n",
      "Iteration 31, loss = 0.02189582\n",
      "Iteration 32, loss = 0.02059322\n",
      "Iteration 33, loss = 0.01961794\n",
      "Iteration 34, loss = 0.01850630\n",
      "Iteration 35, loss = 0.01813074\n",
      "Iteration 36, loss = 0.01695669\n",
      "Iteration 37, loss = 0.01610645\n",
      "Iteration 38, loss = 0.01612573\n",
      "Iteration 39, loss = 0.01525555\n",
      "Iteration 40, loss = 0.01454706\n",
      "Iteration 41, loss = 0.01420388\n",
      "Iteration 42, loss = 0.01344247\n",
      "Iteration 43, loss = 0.01318534\n",
      "Iteration 44, loss = 0.01255873\n",
      "Iteration 45, loss = 0.01229176\n",
      "Iteration 46, loss = 0.01195712\n",
      "Iteration 47, loss = 0.01146897\n",
      "Iteration 48, loss = 0.01131890\n",
      "Iteration 49, loss = 0.01085069\n",
      "Iteration 50, loss = 0.01073265\n",
      "Iteration 51, loss = 0.01033907\n",
      "Iteration 52, loss = 0.01021875\n",
      "Iteration 53, loss = 0.00992087\n",
      "Iteration 54, loss = 0.00960186\n",
      "Iteration 55, loss = 0.00948054\n",
      "Iteration 56, loss = 0.00913802\n",
      "Iteration 57, loss = 0.00894290\n",
      "Iteration 58, loss = 0.00884398\n",
      "Iteration 59, loss = 0.00867376\n",
      "Iteration 60, loss = 0.00839184\n",
      "Iteration 61, loss = 0.00832823\n",
      "Iteration 62, loss = 0.00827749\n",
      "Iteration 63, loss = 0.00802168\n",
      "Iteration 64, loss = 0.00788079\n",
      "Iteration 65, loss = 0.00778743\n",
      "Iteration 66, loss = 0.00745957\n",
      "Iteration 67, loss = 0.00744005\n",
      "Iteration 68, loss = 0.00748307\n",
      "Iteration 69, loss = 0.00758828\n",
      "Iteration 70, loss = 0.00744516\n",
      "Iteration 71, loss = 0.00680427\n",
      "Iteration 72, loss = 0.00675368\n",
      "Iteration 73, loss = 0.00682577\n",
      "Iteration 74, loss = 0.00684710\n",
      "Iteration 75, loss = 0.00653917\n",
      "Iteration 76, loss = 0.00653863\n",
      "Iteration 77, loss = 0.00621413\n",
      "Iteration 78, loss = 0.00614562\n",
      "Iteration 79, loss = 0.00612504\n",
      "Iteration 80, loss = 0.00598964\n",
      "Iteration 81, loss = 0.00611324\n",
      "Iteration 82, loss = 0.00619506\n",
      "Iteration 83, loss = 0.00591315\n",
      "Iteration 84, loss = 0.00561381\n",
      "Iteration 85, loss = 0.00579602\n",
      "Iteration 86, loss = 0.00585996\n",
      "Iteration 87, loss = 0.00534312\n",
      "Iteration 88, loss = 0.00531294\n",
      "Iteration 89, loss = 0.00537577\n",
      "Iteration 90, loss = 0.00518937\n",
      "Iteration 91, loss = 0.00512894\n",
      "Iteration 92, loss = 0.00517400\n",
      "Iteration 93, loss = 0.00492990\n",
      "Iteration 94, loss = 0.00492665\n",
      "Iteration 95, loss = 0.00491204\n",
      "Iteration 96, loss = 0.00475470\n",
      "Iteration 97, loss = 0.00516041\n",
      "Iteration 98, loss = 0.00476180\n",
      "Iteration 99, loss = 0.00469805\n",
      "Iteration 100, loss = 0.00495961\n",
      "Iteration 101, loss = 0.00475110\n",
      "Iteration 102, loss = 0.00437259\n",
      "Iteration 103, loss = 0.00458564\n",
      "Iteration 104, loss = 0.00440311\n",
      "Iteration 105, loss = 0.00436793\n",
      "Iteration 106, loss = 0.00441339\n",
      "Iteration 107, loss = 0.00428892\n",
      "Iteration 108, loss = 0.00426264\n",
      "Iteration 109, loss = 0.00421351\n",
      "Iteration 110, loss = 0.00416205\n",
      "Iteration 111, loss = 0.00413553\n",
      "Iteration 112, loss = 0.00399447\n",
      "Iteration 113, loss = 0.00413165\n",
      "Iteration 114, loss = 0.00451562\n",
      "Iteration 115, loss = 0.00424603\n",
      "Iteration 116, loss = 0.00391903\n",
      "Iteration 117, loss = 0.00379528\n",
      "Iteration 118, loss = 0.00378583\n",
      "Iteration 119, loss = 0.00387018\n",
      "Iteration 120, loss = 0.00396293\n",
      "Iteration 121, loss = 0.00395349\n",
      "Iteration 122, loss = 0.00371168\n",
      "Iteration 123, loss = 0.00363396\n",
      "Iteration 124, loss = 0.00353575\n",
      "Iteration 125, loss = 0.00367950\n",
      "Iteration 126, loss = 0.00347550\n",
      "Iteration 127, loss = 0.00345315\n",
      "Iteration 128, loss = 0.00367219\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68941289\n",
      "Iteration 2, loss = 0.60658574\n",
      "Iteration 3, loss = 0.53414026\n",
      "Iteration 4, loss = 0.46747145\n",
      "Iteration 5, loss = 0.40697110\n",
      "Iteration 6, loss = 0.34905978\n",
      "Iteration 7, loss = 0.29940549\n",
      "Iteration 8, loss = 0.25982791\n",
      "Iteration 9, loss = 0.22823956\n",
      "Iteration 10, loss = 0.20236843\n",
      "Iteration 11, loss = 0.18074619\n",
      "Iteration 12, loss = 0.16260238\n",
      "Iteration 13, loss = 0.14724971\n",
      "Iteration 14, loss = 0.13371961\n",
      "Iteration 15, loss = 0.12251438\n",
      "Iteration 16, loss = 0.11303919\n",
      "Iteration 17, loss = 0.10495476\n",
      "Iteration 18, loss = 0.09813954\n",
      "Iteration 19, loss = 0.09231340\n",
      "Iteration 20, loss = 0.08727081\n",
      "Iteration 21, loss = 0.08302343\n",
      "Iteration 22, loss = 0.07907209\n",
      "Iteration 23, loss = 0.07570212\n",
      "Iteration 24, loss = 0.07251960\n",
      "Iteration 25, loss = 0.06993830\n",
      "Iteration 26, loss = 0.06719089\n",
      "Iteration 27, loss = 0.06496563\n",
      "Iteration 28, loss = 0.06239873\n",
      "Iteration 29, loss = 0.06020020\n",
      "Iteration 30, loss = 0.05814965\n",
      "Iteration 31, loss = 0.05623478\n",
      "Iteration 32, loss = 0.05452867\n",
      "Iteration 33, loss = 0.05280145\n",
      "Iteration 34, loss = 0.05127984\n",
      "Iteration 35, loss = 0.04979575\n",
      "Iteration 36, loss = 0.04833300\n",
      "Iteration 37, loss = 0.04696084\n",
      "Iteration 38, loss = 0.04588333\n",
      "Iteration 39, loss = 0.04457948\n",
      "Iteration 40, loss = 0.04335750\n",
      "Iteration 41, loss = 0.04230211\n",
      "Iteration 42, loss = 0.04113228\n",
      "Iteration 43, loss = 0.04004731\n",
      "Iteration 44, loss = 0.03912823\n",
      "Iteration 45, loss = 0.03830034\n",
      "Iteration 46, loss = 0.03760971\n",
      "Iteration 47, loss = 0.03650893\n",
      "Iteration 48, loss = 0.03581281\n",
      "Iteration 49, loss = 0.03519014\n",
      "Iteration 50, loss = 0.03465374\n",
      "Iteration 51, loss = 0.03361912\n",
      "Iteration 52, loss = 0.03296861\n",
      "Iteration 53, loss = 0.03233690\n",
      "Iteration 54, loss = 0.03158607\n",
      "Iteration 55, loss = 0.03113461\n",
      "Iteration 56, loss = 0.03069838\n",
      "Iteration 57, loss = 0.03007309\n",
      "Iteration 58, loss = 0.02955782\n",
      "Iteration 59, loss = 0.02892531\n",
      "Iteration 60, loss = 0.02849296\n",
      "Iteration 61, loss = 0.02801066\n",
      "Iteration 62, loss = 0.02754255\n",
      "Iteration 63, loss = 0.02730610\n",
      "Iteration 64, loss = 0.02675159\n",
      "Iteration 65, loss = 0.02664008\n",
      "Iteration 66, loss = 0.02596287\n",
      "Iteration 67, loss = 0.02550507\n",
      "Iteration 68, loss = 0.02539886\n",
      "Iteration 69, loss = 0.02482296\n",
      "Iteration 70, loss = 0.02449110\n",
      "Iteration 71, loss = 0.02425444\n",
      "Iteration 72, loss = 0.02398449\n",
      "Iteration 73, loss = 0.02363171\n",
      "Iteration 74, loss = 0.02332968\n",
      "Iteration 75, loss = 0.02291648\n",
      "Iteration 76, loss = 0.02287107\n",
      "Iteration 77, loss = 0.02237500\n",
      "Iteration 78, loss = 0.02217220\n",
      "Iteration 79, loss = 0.02196692\n",
      "Iteration 80, loss = 0.02162660\n",
      "Iteration 81, loss = 0.02135730\n",
      "Iteration 82, loss = 0.02112973\n",
      "Iteration 83, loss = 0.02083285\n",
      "Iteration 84, loss = 0.02062731\n",
      "Iteration 85, loss = 0.02048552\n",
      "Iteration 86, loss = 0.02023187\n",
      "Iteration 87, loss = 0.02001213\n",
      "Iteration 88, loss = 0.01975533\n",
      "Iteration 89, loss = 0.01972072\n",
      "Iteration 90, loss = 0.01935858\n",
      "Iteration 91, loss = 0.01924080\n",
      "Iteration 92, loss = 0.01903615\n",
      "Iteration 93, loss = 0.01880935\n",
      "Iteration 94, loss = 0.01864920\n",
      "Iteration 95, loss = 0.01831399\n",
      "Iteration 96, loss = 0.01812443\n",
      "Iteration 97, loss = 0.01803311\n",
      "Iteration 98, loss = 0.01784175\n",
      "Iteration 99, loss = 0.01761643\n",
      "Iteration 100, loss = 0.01745621\n",
      "Iteration 101, loss = 0.01727467\n",
      "Iteration 102, loss = 0.01724963\n",
      "Iteration 103, loss = 0.01678932\n",
      "Iteration 104, loss = 0.01686121\n",
      "Iteration 105, loss = 0.01627617\n",
      "Iteration 106, loss = 0.01621947\n",
      "Iteration 107, loss = 0.01621735\n",
      "Iteration 108, loss = 0.01597694\n",
      "Iteration 109, loss = 0.01601042\n",
      "Iteration 110, loss = 0.01555671\n",
      "Iteration 111, loss = 0.01548372\n",
      "Iteration 112, loss = 0.01527952\n",
      "Iteration 113, loss = 0.01519539\n",
      "Iteration 114, loss = 0.01513889\n",
      "Iteration 115, loss = 0.01495379\n",
      "Iteration 116, loss = 0.01474104\n",
      "Iteration 117, loss = 0.01472074\n",
      "Iteration 118, loss = 0.01448072\n",
      "Iteration 119, loss = 0.01429720\n",
      "Iteration 120, loss = 0.01410270\n",
      "Iteration 121, loss = 0.01408354\n",
      "Iteration 122, loss = 0.01388762\n",
      "Iteration 123, loss = 0.01371751\n",
      "Iteration 124, loss = 0.01357303\n",
      "Iteration 125, loss = 0.01343006\n",
      "Iteration 126, loss = 0.01338406\n",
      "Iteration 127, loss = 0.01348722\n",
      "Iteration 128, loss = 0.01318359\n",
      "Iteration 129, loss = 0.01299855\n",
      "Iteration 130, loss = 0.01288213\n",
      "Iteration 131, loss = 0.01277093\n",
      "Iteration 132, loss = 0.01278449\n",
      "Iteration 133, loss = 0.01242219\n",
      "Iteration 134, loss = 0.01232941\n",
      "Iteration 135, loss = 0.01215881\n",
      "Iteration 136, loss = 0.01209702\n",
      "Iteration 137, loss = 0.01202877\n",
      "Iteration 138, loss = 0.01183836\n",
      "Iteration 139, loss = 0.01164673\n",
      "Iteration 140, loss = 0.01164197\n",
      "Iteration 141, loss = 0.01151219\n",
      "Iteration 142, loss = 0.01136607\n",
      "Iteration 143, loss = 0.01146374\n",
      "Iteration 144, loss = 0.01116182\n",
      "Iteration 145, loss = 0.01130721\n",
      "Iteration 146, loss = 0.01089061\n",
      "Iteration 147, loss = 0.01070468\n",
      "Iteration 148, loss = 0.01056051\n",
      "Iteration 149, loss = 0.01051512\n",
      "Iteration 150, loss = 0.01039373\n",
      "Iteration 151, loss = 0.01024907\n",
      "Iteration 152, loss = 0.01021550\n",
      "Iteration 153, loss = 0.01007833\n",
      "Iteration 154, loss = 0.01003241\n",
      "Iteration 155, loss = 0.00996183\n",
      "Iteration 156, loss = 0.01010567\n",
      "Iteration 157, loss = 0.00973110\n",
      "Iteration 158, loss = 0.00966576\n",
      "Iteration 159, loss = 0.00961569\n",
      "Iteration 160, loss = 0.00963685\n",
      "Iteration 161, loss = 0.00940148\n",
      "Iteration 162, loss = 0.00923923\n",
      "Iteration 163, loss = 0.00915825\n",
      "Iteration 164, loss = 0.00914612\n",
      "Iteration 165, loss = 0.00909970\n",
      "Iteration 166, loss = 0.00900359\n",
      "Iteration 167, loss = 0.00890958\n",
      "Iteration 168, loss = 0.00893002\n",
      "Iteration 169, loss = 0.00878121\n",
      "Iteration 170, loss = 0.00865219\n",
      "Iteration 171, loss = 0.00849819\n",
      "Iteration 172, loss = 0.00858091\n",
      "Iteration 173, loss = 0.00850575\n",
      "Iteration 174, loss = 0.00854996\n",
      "Iteration 175, loss = 0.00837026\n",
      "Iteration 176, loss = 0.00812273\n",
      "Iteration 177, loss = 0.00806427\n",
      "Iteration 178, loss = 0.00808444\n",
      "Iteration 179, loss = 0.00792115\n",
      "Iteration 180, loss = 0.00793206\n",
      "Iteration 181, loss = 0.00784849\n",
      "Iteration 182, loss = 0.00790476\n",
      "Iteration 183, loss = 0.00766418\n",
      "Iteration 184, loss = 0.00764354\n",
      "Iteration 185, loss = 0.00759845\n",
      "Iteration 186, loss = 0.00773898\n",
      "Iteration 187, loss = 0.00754198\n",
      "Iteration 188, loss = 0.00749675\n",
      "Iteration 189, loss = 0.00739665\n",
      "Iteration 190, loss = 0.00738018\n",
      "Iteration 191, loss = 0.00721982\n",
      "Iteration 192, loss = 0.00731439\n",
      "Iteration 193, loss = 0.00716169\n",
      "Iteration 194, loss = 0.00704984\n",
      "Iteration 195, loss = 0.00698559\n",
      "Iteration 196, loss = 0.00700249\n",
      "Iteration 197, loss = 0.00705318\n",
      "Iteration 198, loss = 0.00679080\n",
      "Iteration 199, loss = 0.00676323\n",
      "Iteration 200, loss = 0.00669696\n",
      "Iteration 201, loss = 0.00655298\n",
      "Iteration 202, loss = 0.00658710\n",
      "Iteration 203, loss = 0.00643758\n",
      "Iteration 204, loss = 0.00646884\n",
      "Iteration 205, loss = 0.00647638\n",
      "Iteration 206, loss = 0.00630233\n",
      "Iteration 207, loss = 0.00623401\n",
      "Iteration 208, loss = 0.00625591\n",
      "Iteration 209, loss = 0.00613971\n",
      "Iteration 210, loss = 0.00606772\n",
      "Iteration 211, loss = 0.00613756\n",
      "Iteration 212, loss = 0.00619098\n",
      "Iteration 213, loss = 0.00597955\n",
      "Iteration 214, loss = 0.00586406\n",
      "Iteration 215, loss = 0.00596562\n",
      "Iteration 216, loss = 0.00567290\n",
      "Iteration 217, loss = 0.00654955\n",
      "Iteration 218, loss = 0.00576923\n",
      "Iteration 219, loss = 0.00561041\n",
      "Iteration 220, loss = 0.00576615\n",
      "Iteration 221, loss = 0.00561866\n",
      "Iteration 222, loss = 0.00557335\n",
      "Iteration 223, loss = 0.00550124\n",
      "Iteration 224, loss = 0.00541979\n",
      "Iteration 225, loss = 0.00544661\n",
      "Iteration 226, loss = 0.00532189\n",
      "Iteration 227, loss = 0.00528498\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67981512\n",
      "Iteration 2, loss = 0.61016699\n",
      "Iteration 3, loss = 0.56765782\n",
      "Iteration 4, loss = 0.52408976\n",
      "Iteration 5, loss = 0.47369446\n",
      "Iteration 6, loss = 0.42010746\n",
      "Iteration 7, loss = 0.36738997\n",
      "Iteration 8, loss = 0.31668435\n",
      "Iteration 9, loss = 0.27040065\n",
      "Iteration 10, loss = 0.22963640\n",
      "Iteration 11, loss = 0.19291350\n",
      "Iteration 12, loss = 0.16152116\n",
      "Iteration 13, loss = 0.13793941\n",
      "Iteration 14, loss = 0.11974048\n",
      "Iteration 15, loss = 0.10664004\n",
      "Iteration 16, loss = 0.09616657\n",
      "Iteration 17, loss = 0.08809107\n",
      "Iteration 18, loss = 0.08228744\n",
      "Iteration 19, loss = 0.07759712\n",
      "Iteration 20, loss = 0.07407698\n",
      "Iteration 21, loss = 0.07090886\n",
      "Iteration 22, loss = 0.06874503\n",
      "Iteration 23, loss = 0.06770611\n",
      "Iteration 24, loss = 0.06603617\n",
      "Iteration 25, loss = 0.06402875\n",
      "Iteration 26, loss = 0.06347744\n",
      "Iteration 27, loss = 0.06219350\n",
      "Iteration 28, loss = 0.06107955\n",
      "Iteration 29, loss = 0.06020538\n",
      "Iteration 30, loss = 0.05962529\n",
      "Iteration 31, loss = 0.05873055\n",
      "Iteration 32, loss = 0.05839712\n",
      "Iteration 33, loss = 0.05788295\n",
      "Iteration 34, loss = 0.05754489\n",
      "Iteration 35, loss = 0.05693333\n",
      "Iteration 36, loss = 0.05622617\n",
      "Iteration 37, loss = 0.05598310\n",
      "Iteration 38, loss = 0.05551331\n",
      "Iteration 39, loss = 0.05518888\n",
      "Iteration 40, loss = 0.05520726\n",
      "Iteration 41, loss = 0.05463110\n",
      "Iteration 42, loss = 0.05443684\n",
      "Iteration 43, loss = 0.05399617\n",
      "Iteration 44, loss = 0.05387232\n",
      "Iteration 45, loss = 0.05340817\n",
      "Iteration 46, loss = 0.05327443\n",
      "Iteration 47, loss = 0.05281239\n",
      "Iteration 48, loss = 0.05282354\n",
      "Iteration 49, loss = 0.05252312\n",
      "Iteration 50, loss = 0.05218462\n",
      "Iteration 51, loss = 0.05211818\n",
      "Iteration 52, loss = 0.05172783\n",
      "Iteration 53, loss = 0.05187135\n",
      "Iteration 54, loss = 0.05164283\n",
      "Iteration 55, loss = 0.05105918\n",
      "Iteration 56, loss = 0.05094938\n",
      "Iteration 57, loss = 0.05083391\n",
      "Iteration 58, loss = 0.05071856\n",
      "Iteration 59, loss = 0.05033082\n",
      "Iteration 60, loss = 0.05001275\n",
      "Iteration 61, loss = 0.05065920\n",
      "Iteration 62, loss = 0.05036807\n",
      "Iteration 63, loss = 0.04905166\n",
      "Iteration 64, loss = 0.04869573\n",
      "Iteration 65, loss = 0.04805846\n",
      "Iteration 66, loss = 0.04778218\n",
      "Iteration 67, loss = 0.04771334\n",
      "Iteration 68, loss = 0.04756473\n",
      "Iteration 69, loss = 0.04764117\n",
      "Iteration 70, loss = 0.04793351\n",
      "Iteration 71, loss = 0.04803410\n",
      "Iteration 72, loss = 0.04677700\n",
      "Iteration 73, loss = 0.04664608\n",
      "Iteration 74, loss = 0.04625687\n",
      "Iteration 75, loss = 0.04629060\n",
      "Iteration 76, loss = 0.04600657\n",
      "Iteration 77, loss = 0.04597796\n",
      "Iteration 78, loss = 0.04642065\n",
      "Iteration 79, loss = 0.04555733\n",
      "Iteration 80, loss = 0.04505204\n",
      "Iteration 81, loss = 0.04509251\n",
      "Iteration 82, loss = 0.04498359\n",
      "Iteration 83, loss = 0.04495529\n",
      "Iteration 84, loss = 0.04454773\n",
      "Iteration 85, loss = 0.04459922\n",
      "Iteration 86, loss = 0.04439317\n",
      "Iteration 87, loss = 0.04447623\n",
      "Iteration 88, loss = 0.04388898\n",
      "Iteration 89, loss = 0.04393257\n",
      "Iteration 90, loss = 0.04365771\n",
      "Iteration 91, loss = 0.04353241\n",
      "Iteration 92, loss = 0.04361987\n",
      "Iteration 93, loss = 0.04318545\n",
      "Iteration 94, loss = 0.04305797\n",
      "Iteration 95, loss = 0.04296296\n",
      "Iteration 96, loss = 0.04268978\n",
      "Iteration 97, loss = 0.04278835\n",
      "Iteration 98, loss = 0.04299755\n",
      "Iteration 99, loss = 0.04249279\n",
      "Iteration 100, loss = 0.04232522\n",
      "Iteration 101, loss = 0.04194684\n",
      "Iteration 102, loss = 0.04168055\n",
      "Iteration 103, loss = 0.04224383\n",
      "Iteration 104, loss = 0.04162424\n",
      "Iteration 105, loss = 0.04193174\n",
      "Iteration 106, loss = 0.04122563\n",
      "Iteration 107, loss = 0.04131673\n",
      "Iteration 108, loss = 0.04081003\n",
      "Iteration 109, loss = 0.04067299\n",
      "Iteration 110, loss = 0.04077564\n",
      "Iteration 111, loss = 0.04058350\n",
      "Iteration 112, loss = 0.04028233\n",
      "Iteration 113, loss = 0.04021244\n",
      "Iteration 114, loss = 0.04010847\n",
      "Iteration 115, loss = 0.04018619\n",
      "Iteration 116, loss = 0.03965435\n",
      "Iteration 117, loss = 0.03931638\n",
      "Iteration 118, loss = 0.03940751\n",
      "Iteration 119, loss = 0.03946406\n",
      "Iteration 120, loss = 0.03908671\n",
      "Iteration 121, loss = 0.03911462\n",
      "Iteration 122, loss = 0.03915966\n",
      "Iteration 123, loss = 0.03904168\n",
      "Iteration 124, loss = 0.03841975\n",
      "Iteration 125, loss = 0.03806130\n",
      "Iteration 126, loss = 0.03818956\n",
      "Iteration 127, loss = 0.03821604\n",
      "Iteration 128, loss = 0.03752669\n",
      "Iteration 129, loss = 0.03796627\n",
      "Iteration 130, loss = 0.03744908\n",
      "Iteration 131, loss = 0.03750398\n",
      "Iteration 132, loss = 0.03712914\n",
      "Iteration 133, loss = 0.03708435\n",
      "Iteration 134, loss = 0.03683926\n",
      "Iteration 135, loss = 0.03683300\n",
      "Iteration 136, loss = 0.03716201\n",
      "Iteration 137, loss = 0.03710465\n",
      "Iteration 138, loss = 0.03605991\n",
      "Iteration 139, loss = 0.03577423\n",
      "Iteration 140, loss = 0.03580746\n",
      "Iteration 141, loss = 0.03602390\n",
      "Iteration 142, loss = 0.03539103\n",
      "Iteration 143, loss = 0.03525831\n",
      "Iteration 144, loss = 0.03504694\n",
      "Iteration 145, loss = 0.03489842\n",
      "Iteration 146, loss = 0.03466910\n",
      "Iteration 147, loss = 0.03463284\n",
      "Iteration 148, loss = 0.03446968\n",
      "Iteration 149, loss = 0.03423910\n",
      "Iteration 150, loss = 0.03421868\n",
      "Iteration 151, loss = 0.03417109\n",
      "Iteration 152, loss = 0.03418996\n",
      "Iteration 153, loss = 0.03418017\n",
      "Iteration 154, loss = 0.03394689\n",
      "Iteration 155, loss = 0.03333043\n",
      "Iteration 156, loss = 0.03321529\n",
      "Iteration 157, loss = 0.03338077\n",
      "Iteration 158, loss = 0.03283695\n",
      "Iteration 159, loss = 0.03260588\n",
      "Iteration 160, loss = 0.03297419\n",
      "Iteration 161, loss = 0.03234601\n",
      "Iteration 162, loss = 0.03241312\n",
      "Iteration 163, loss = 0.03267082\n",
      "Iteration 164, loss = 0.03266043\n",
      "Iteration 165, loss = 0.03151262\n",
      "Iteration 166, loss = 0.03327916\n",
      "Iteration 167, loss = 0.03217608\n",
      "Iteration 168, loss = 0.03149376\n",
      "Iteration 169, loss = 0.03110174\n",
      "Iteration 170, loss = 0.03132026\n",
      "Iteration 171, loss = 0.03088986\n",
      "Iteration 172, loss = 0.03073349\n",
      "Iteration 173, loss = 0.03048782\n",
      "Iteration 174, loss = 0.03034973\n",
      "Iteration 175, loss = 0.03006389\n",
      "Iteration 176, loss = 0.03010452\n",
      "Iteration 177, loss = 0.02986260\n",
      "Iteration 178, loss = 0.02972914\n",
      "Iteration 179, loss = 0.02982709\n",
      "Iteration 180, loss = 0.02952252\n",
      "Iteration 181, loss = 0.02931653\n",
      "Iteration 182, loss = 0.02921013\n",
      "Iteration 183, loss = 0.02906017\n",
      "Iteration 184, loss = 0.02870240\n",
      "Iteration 185, loss = 0.02870600\n",
      "Iteration 186, loss = 0.02875170\n",
      "Iteration 187, loss = 0.02863759\n",
      "Iteration 188, loss = 0.02836038\n",
      "Iteration 189, loss = 0.02831733\n",
      "Iteration 190, loss = 0.02796557\n",
      "Iteration 191, loss = 0.02807343\n",
      "Iteration 192, loss = 0.02788921\n",
      "Iteration 193, loss = 0.02764620\n",
      "Iteration 194, loss = 0.02739414\n",
      "Iteration 195, loss = 0.02735297\n",
      "Iteration 196, loss = 0.02726953\n",
      "Iteration 197, loss = 0.02698908\n",
      "Iteration 198, loss = 0.02694847\n",
      "Iteration 199, loss = 0.02711667\n",
      "Iteration 200, loss = 0.02657012\n",
      "Iteration 201, loss = 0.02675641\n",
      "Iteration 202, loss = 0.02667301\n",
      "Iteration 203, loss = 0.02624048\n",
      "Iteration 204, loss = 0.02616371\n",
      "Iteration 205, loss = 0.02646286\n",
      "Iteration 206, loss = 0.02622180\n",
      "Iteration 207, loss = 0.02583779\n",
      "Iteration 208, loss = 0.02602690\n",
      "Iteration 209, loss = 0.02580260\n",
      "Iteration 210, loss = 0.02576296\n",
      "Iteration 211, loss = 0.02573588\n",
      "Iteration 212, loss = 0.02536818\n",
      "Iteration 213, loss = 0.02498036\n",
      "Iteration 214, loss = 0.02608025\n",
      "Iteration 215, loss = 0.02501791\n",
      "Iteration 216, loss = 0.02471768\n",
      "Iteration 217, loss = 0.02464604\n",
      "Iteration 218, loss = 0.02476569\n",
      "Iteration 219, loss = 0.02423899\n",
      "Iteration 220, loss = 0.02439079\n",
      "Iteration 221, loss = 0.02438367\n",
      "Iteration 222, loss = 0.02383506\n",
      "Iteration 223, loss = 0.02409074\n",
      "Iteration 224, loss = 0.02394549\n",
      "Iteration 225, loss = 0.02388943\n",
      "Iteration 226, loss = 0.02336235\n",
      "Iteration 227, loss = 0.02467121\n",
      "Iteration 228, loss = 0.02352847\n",
      "Iteration 229, loss = 0.02328441\n",
      "Iteration 230, loss = 0.02329897\n",
      "Iteration 231, loss = 0.02364820\n",
      "Iteration 232, loss = 0.02319383\n",
      "Iteration 233, loss = 0.02290905\n",
      "Iteration 234, loss = 0.02284562\n",
      "Iteration 235, loss = 0.02295732\n",
      "Iteration 236, loss = 0.02285145\n",
      "Iteration 237, loss = 0.02244764\n",
      "Iteration 238, loss = 0.02274367\n",
      "Iteration 239, loss = 0.02266357\n",
      "Iteration 240, loss = 0.02242714\n",
      "Iteration 241, loss = 0.02228425\n",
      "Iteration 242, loss = 0.02234441\n",
      "Iteration 243, loss = 0.02198649\n",
      "Iteration 244, loss = 0.02216035\n",
      "Iteration 245, loss = 0.02174328\n",
      "Iteration 246, loss = 0.02208676\n",
      "Iteration 247, loss = 0.02187804\n",
      "Iteration 248, loss = 0.02168603\n",
      "Iteration 249, loss = 0.02166820\n",
      "Iteration 250, loss = 0.02204506\n",
      "Iteration 251, loss = 0.02212413\n",
      "Iteration 252, loss = 0.02135542\n",
      "Iteration 253, loss = 0.02158397\n",
      "Iteration 254, loss = 0.02147948\n",
      "Iteration 255, loss = 0.02204626\n",
      "Iteration 256, loss = 0.02178091\n",
      "Iteration 257, loss = 0.02227335\n",
      "Iteration 258, loss = 0.02118166\n",
      "Iteration 259, loss = 0.02100800\n",
      "Iteration 260, loss = 0.02075809\n",
      "Iteration 261, loss = 0.02085375\n",
      "Iteration 262, loss = 0.02051337\n",
      "Iteration 263, loss = 0.02059263\n",
      "Iteration 264, loss = 0.02091111\n",
      "Iteration 265, loss = 0.02098743\n",
      "Iteration 266, loss = 0.02010243\n",
      "Iteration 267, loss = 0.02041825\n",
      "Iteration 268, loss = 0.02037261\n",
      "Iteration 269, loss = 0.02052593\n",
      "Iteration 270, loss = 0.02006334\n",
      "Iteration 271, loss = 0.02012213\n",
      "Iteration 272, loss = 0.02028004\n",
      "Iteration 273, loss = 0.01985089\n",
      "Iteration 274, loss = 0.02011469\n",
      "Iteration 275, loss = 0.01992111\n",
      "Iteration 276, loss = 0.01983934\n",
      "Iteration 277, loss = 0.01971811\n",
      "Iteration 278, loss = 0.01952062\n",
      "Iteration 279, loss = 0.01966478\n",
      "Iteration 280, loss = 0.01928529\n",
      "Iteration 281, loss = 0.02006576\n",
      "Iteration 282, loss = 0.01926429\n",
      "Iteration 283, loss = 0.01945582\n",
      "Iteration 284, loss = 0.01969316\n",
      "Iteration 285, loss = 0.01923420\n",
      "Iteration 286, loss = 0.01942729\n",
      "Iteration 287, loss = 0.01904543\n",
      "Iteration 288, loss = 0.01925893\n",
      "Iteration 289, loss = 0.01917933\n",
      "Iteration 290, loss = 0.01949505\n",
      "Iteration 291, loss = 0.01927218\n",
      "Iteration 292, loss = 0.01945598\n",
      "Iteration 293, loss = 0.01897207\n",
      "Iteration 294, loss = 0.01868923\n",
      "Iteration 295, loss = 0.01894548\n",
      "Iteration 296, loss = 0.01870373\n",
      "Iteration 297, loss = 0.01847105\n",
      "Iteration 298, loss = 0.01831049\n",
      "Iteration 299, loss = 0.01857461\n",
      "Iteration 300, loss = 0.01845678\n",
      "Iteration 301, loss = 0.01871505\n",
      "Iteration 302, loss = 0.01875297\n",
      "Iteration 303, loss = 0.01849787\n",
      "Iteration 304, loss = 0.01868180\n",
      "Iteration 305, loss = 0.01857111\n",
      "Iteration 306, loss = 0.01850535\n",
      "Iteration 307, loss = 0.01844986\n",
      "Iteration 308, loss = 0.01814966\n",
      "Iteration 309, loss = 0.01769383\n",
      "Iteration 310, loss = 0.01788758\n",
      "Iteration 311, loss = 0.01788169\n",
      "Iteration 312, loss = 0.01771608\n",
      "Iteration 313, loss = 0.01797394\n",
      "Iteration 314, loss = 0.01818787\n",
      "Iteration 315, loss = 0.01783154\n",
      "Iteration 316, loss = 0.01801780\n",
      "Iteration 317, loss = 0.01820163\n",
      "Iteration 318, loss = 0.01858468\n",
      "Iteration 319, loss = 0.01732351\n",
      "Iteration 320, loss = 0.01787025\n",
      "Iteration 321, loss = 0.01727039\n",
      "Iteration 322, loss = 0.01750379\n",
      "Iteration 323, loss = 0.01782712\n",
      "Iteration 324, loss = 0.01834280\n",
      "Iteration 325, loss = 0.01753494\n",
      "Iteration 326, loss = 0.01726977\n",
      "Iteration 327, loss = 0.01738049\n",
      "Iteration 328, loss = 0.01685243\n",
      "Iteration 329, loss = 0.01758251\n",
      "Iteration 330, loss = 0.01770954\n",
      "Iteration 331, loss = 0.01700943\n",
      "Iteration 332, loss = 0.01657525\n",
      "Iteration 333, loss = 0.01678867\n",
      "Iteration 334, loss = 0.01690118\n",
      "Iteration 335, loss = 0.01684389\n",
      "Iteration 336, loss = 0.01681942\n",
      "Iteration 337, loss = 0.01670807\n",
      "Iteration 338, loss = 0.01661256\n",
      "Iteration 339, loss = 0.01680581\n",
      "Iteration 340, loss = 0.01669376\n",
      "Iteration 341, loss = 0.01710706\n",
      "Iteration 342, loss = 0.01669658\n",
      "Iteration 343, loss = 0.01705210\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72541349\n",
      "Iteration 2, loss = 0.70129234\n",
      "Iteration 3, loss = 0.66812996\n",
      "Iteration 4, loss = 0.63087892\n",
      "Iteration 5, loss = 0.58806256\n",
      "Iteration 6, loss = 0.54079875\n",
      "Iteration 7, loss = 0.48550795\n",
      "Iteration 8, loss = 0.42166189\n",
      "Iteration 9, loss = 0.35797387\n",
      "Iteration 10, loss = 0.29092385\n",
      "Iteration 11, loss = 0.23088927\n",
      "Iteration 12, loss = 0.18870617\n",
      "Iteration 13, loss = 0.15979471\n",
      "Iteration 14, loss = 0.14074258\n",
      "Iteration 15, loss = 0.12605508\n",
      "Iteration 16, loss = 0.11448403\n",
      "Iteration 17, loss = 0.10507439\n",
      "Iteration 18, loss = 0.09707849\n",
      "Iteration 19, loss = 0.08991494\n",
      "Iteration 20, loss = 0.08380652\n",
      "Iteration 21, loss = 0.07899249\n",
      "Iteration 22, loss = 0.07474920\n",
      "Iteration 23, loss = 0.07130536\n",
      "Iteration 24, loss = 0.06834358\n",
      "Iteration 25, loss = 0.06563501\n",
      "Iteration 26, loss = 0.06359060\n",
      "Iteration 27, loss = 0.06167324\n",
      "Iteration 28, loss = 0.06005368\n",
      "Iteration 29, loss = 0.05858761\n",
      "Iteration 30, loss = 0.05721806\n",
      "Iteration 31, loss = 0.05586797\n",
      "Iteration 32, loss = 0.05470894\n",
      "Iteration 33, loss = 0.05353675\n",
      "Iteration 34, loss = 0.05242320\n",
      "Iteration 35, loss = 0.05137162\n",
      "Iteration 36, loss = 0.05041732\n",
      "Iteration 37, loss = 0.04930073\n",
      "Iteration 38, loss = 0.04833131\n",
      "Iteration 39, loss = 0.04755904\n",
      "Iteration 40, loss = 0.04645412\n",
      "Iteration 41, loss = 0.04565751\n",
      "Iteration 42, loss = 0.04457362\n",
      "Iteration 43, loss = 0.04378364\n",
      "Iteration 44, loss = 0.04309794\n",
      "Iteration 45, loss = 0.04238891\n",
      "Iteration 46, loss = 0.04171239\n",
      "Iteration 47, loss = 0.04124294\n",
      "Iteration 48, loss = 0.04068735\n",
      "Iteration 49, loss = 0.04009019\n",
      "Iteration 50, loss = 0.03947015\n",
      "Iteration 51, loss = 0.03859993\n",
      "Iteration 52, loss = 0.03813869\n",
      "Iteration 53, loss = 0.03711946\n",
      "Iteration 54, loss = 0.03626160\n",
      "Iteration 55, loss = 0.03526628\n",
      "Iteration 56, loss = 0.03445133\n",
      "Iteration 57, loss = 0.03334513\n",
      "Iteration 58, loss = 0.03252638\n",
      "Iteration 59, loss = 0.03201891\n",
      "Iteration 60, loss = 0.03109791\n",
      "Iteration 61, loss = 0.03049961\n",
      "Iteration 62, loss = 0.02993977\n",
      "Iteration 63, loss = 0.02952518\n",
      "Iteration 64, loss = 0.02887960\n",
      "Iteration 65, loss = 0.02846250\n",
      "Iteration 66, loss = 0.02839964\n",
      "Iteration 67, loss = 0.02758769\n",
      "Iteration 68, loss = 0.02724076\n",
      "Iteration 69, loss = 0.02700256\n",
      "Iteration 70, loss = 0.02664346\n",
      "Iteration 71, loss = 0.02648333\n",
      "Iteration 72, loss = 0.02609649\n",
      "Iteration 73, loss = 0.02589263\n",
      "Iteration 74, loss = 0.02549189\n",
      "Iteration 75, loss = 0.02542841\n",
      "Iteration 76, loss = 0.02513336\n",
      "Iteration 77, loss = 0.02479596\n",
      "Iteration 78, loss = 0.02456350\n",
      "Iteration 79, loss = 0.02429172\n",
      "Iteration 80, loss = 0.02406959\n",
      "Iteration 81, loss = 0.02411361\n",
      "Iteration 82, loss = 0.02383629\n",
      "Iteration 83, loss = 0.02402146\n",
      "Iteration 84, loss = 0.02344033\n",
      "Iteration 85, loss = 0.02327409\n",
      "Iteration 86, loss = 0.02315655\n",
      "Iteration 87, loss = 0.02273635\n",
      "Iteration 88, loss = 0.02260457\n",
      "Iteration 89, loss = 0.02252229\n",
      "Iteration 90, loss = 0.02280141\n",
      "Iteration 91, loss = 0.02198753\n",
      "Iteration 92, loss = 0.02195195\n",
      "Iteration 93, loss = 0.02178432\n",
      "Iteration 94, loss = 0.02152922\n",
      "Iteration 95, loss = 0.02155618\n",
      "Iteration 96, loss = 0.02156414\n",
      "Iteration 97, loss = 0.02112970\n",
      "Iteration 98, loss = 0.02106824\n",
      "Iteration 99, loss = 0.02107034\n",
      "Iteration 100, loss = 0.02159535\n",
      "Iteration 101, loss = 0.02119708\n",
      "Iteration 102, loss = 0.02078895\n",
      "Iteration 103, loss = 0.02054645\n",
      "Iteration 104, loss = 0.02054640\n",
      "Iteration 105, loss = 0.02046354\n",
      "Iteration 106, loss = 0.02003639\n",
      "Iteration 107, loss = 0.01993083\n",
      "Iteration 108, loss = 0.01972784\n",
      "Iteration 109, loss = 0.01990670\n",
      "Iteration 110, loss = 0.01947051\n",
      "Iteration 111, loss = 0.01981346\n",
      "Iteration 112, loss = 0.01939312\n",
      "Iteration 113, loss = 0.01927590\n",
      "Iteration 114, loss = 0.01907377\n",
      "Iteration 115, loss = 0.01905079\n",
      "Iteration 116, loss = 0.01904037\n",
      "Iteration 117, loss = 0.01891540\n",
      "Iteration 118, loss = 0.01873697\n",
      "Iteration 119, loss = 0.01856442\n",
      "Iteration 120, loss = 0.01853654\n",
      "Iteration 121, loss = 0.01838279\n",
      "Iteration 122, loss = 0.01818598\n",
      "Iteration 123, loss = 0.01820348\n",
      "Iteration 124, loss = 0.01811360\n",
      "Iteration 125, loss = 0.01784427\n",
      "Iteration 126, loss = 0.01803666\n",
      "Iteration 127, loss = 0.01790849\n",
      "Iteration 128, loss = 0.01757603\n",
      "Iteration 129, loss = 0.01794676\n",
      "Iteration 130, loss = 0.01744434\n",
      "Iteration 131, loss = 0.01735968\n",
      "Iteration 132, loss = 0.01738936\n",
      "Iteration 133, loss = 0.01711444\n",
      "Iteration 134, loss = 0.01744528\n",
      "Iteration 135, loss = 0.01707278\n",
      "Iteration 136, loss = 0.01707807\n",
      "Iteration 137, loss = 0.01671712\n",
      "Iteration 138, loss = 0.01664571\n",
      "Iteration 139, loss = 0.01697080\n",
      "Iteration 140, loss = 0.01699573\n",
      "Iteration 141, loss = 0.01641049\n",
      "Iteration 142, loss = 0.01666831\n",
      "Iteration 143, loss = 0.01647292\n",
      "Iteration 144, loss = 0.01619329\n",
      "Iteration 145, loss = 0.01645052\n",
      "Iteration 146, loss = 0.01636113\n",
      "Iteration 147, loss = 0.01611025\n",
      "Iteration 148, loss = 0.01624457\n",
      "Iteration 149, loss = 0.01657128\n",
      "Iteration 150, loss = 0.01606881\n",
      "Iteration 151, loss = 0.01572886\n",
      "Iteration 152, loss = 0.01570356\n",
      "Iteration 153, loss = 0.01567844\n",
      "Iteration 154, loss = 0.01567391\n",
      "Iteration 155, loss = 0.01541465\n",
      "Iteration 156, loss = 0.01544915\n",
      "Iteration 157, loss = 0.01541888\n",
      "Iteration 158, loss = 0.01534685\n",
      "Iteration 159, loss = 0.01542922\n",
      "Iteration 160, loss = 0.01536557\n",
      "Iteration 161, loss = 0.01505227\n",
      "Iteration 162, loss = 0.01509245\n",
      "Iteration 163, loss = 0.01503327\n",
      "Iteration 164, loss = 0.01612153\n",
      "Iteration 165, loss = 0.01487030\n",
      "Iteration 166, loss = 0.01462888\n",
      "Iteration 167, loss = 0.01473949\n",
      "Iteration 168, loss = 0.01475972\n",
      "Iteration 169, loss = 0.01464355\n",
      "Iteration 170, loss = 0.01443112\n",
      "Iteration 171, loss = 0.01465692\n",
      "Iteration 172, loss = 0.01445391\n",
      "Iteration 173, loss = 0.01430901\n",
      "Iteration 174, loss = 0.01425819\n",
      "Iteration 175, loss = 0.01418789\n",
      "Iteration 176, loss = 0.01432256\n",
      "Iteration 177, loss = 0.01408988\n",
      "Iteration 178, loss = 0.01403648\n",
      "Iteration 179, loss = 0.01395573\n",
      "Iteration 180, loss = 0.01398830\n",
      "Iteration 181, loss = 0.01367166\n",
      "Iteration 182, loss = 0.01371366\n",
      "Iteration 183, loss = 0.01362328\n",
      "Iteration 184, loss = 0.01376868\n",
      "Iteration 185, loss = 0.01362841\n",
      "Iteration 186, loss = 0.01374445\n",
      "Iteration 187, loss = 0.01379778\n",
      "Iteration 188, loss = 0.01338974\n",
      "Iteration 189, loss = 0.01352627\n",
      "Iteration 190, loss = 0.01334173\n",
      "Iteration 191, loss = 0.01348405\n",
      "Iteration 192, loss = 0.01337426\n",
      "Iteration 193, loss = 0.01322507\n",
      "Iteration 194, loss = 0.01309382\n",
      "Iteration 195, loss = 0.01300103\n",
      "Iteration 196, loss = 0.01309612\n",
      "Iteration 197, loss = 0.01318684\n",
      "Iteration 198, loss = 0.01323471\n",
      "Iteration 199, loss = 0.01271749\n",
      "Iteration 200, loss = 0.01304790\n",
      "Iteration 201, loss = 0.01335512\n",
      "Iteration 202, loss = 0.01255801\n",
      "Iteration 203, loss = 0.01292044\n",
      "Iteration 204, loss = 0.01229428\n",
      "Iteration 205, loss = 0.01320960\n",
      "Iteration 206, loss = 0.01252143\n",
      "Iteration 207, loss = 0.01260112\n",
      "Iteration 208, loss = 0.01284793\n",
      "Iteration 209, loss = 0.01281246\n",
      "Iteration 210, loss = 0.01230005\n",
      "Iteration 211, loss = 0.01209337\n",
      "Iteration 212, loss = 0.01203544\n",
      "Iteration 213, loss = 0.01243996\n",
      "Iteration 214, loss = 0.01207326\n",
      "Iteration 215, loss = 0.01229027\n",
      "Iteration 216, loss = 0.01206381\n",
      "Iteration 217, loss = 0.01175851\n",
      "Iteration 218, loss = 0.01276877\n",
      "Iteration 219, loss = 0.01192923\n",
      "Iteration 220, loss = 0.01202038\n",
      "Iteration 221, loss = 0.01180666\n",
      "Iteration 222, loss = 0.01145221\n",
      "Iteration 223, loss = 0.01174858\n",
      "Iteration 224, loss = 0.01190696\n",
      "Iteration 225, loss = 0.01152489\n",
      "Iteration 226, loss = 0.01180975\n",
      "Iteration 227, loss = 0.01188240\n",
      "Iteration 228, loss = 0.01198702\n",
      "Iteration 229, loss = 0.01146196\n",
      "Iteration 230, loss = 0.01178766\n",
      "Iteration 231, loss = 0.01136357\n",
      "Iteration 232, loss = 0.01127795\n",
      "Iteration 233, loss = 0.01119799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65326813\n",
      "Iteration 2, loss = 0.59697077\n",
      "Iteration 3, loss = 0.54305905\n",
      "Iteration 4, loss = 0.48171310\n",
      "Iteration 5, loss = 0.41103388\n",
      "Iteration 6, loss = 0.34281279\n",
      "Iteration 7, loss = 0.29154280\n",
      "Iteration 8, loss = 0.25634753\n",
      "Iteration 9, loss = 0.23061477\n",
      "Iteration 10, loss = 0.20990653\n",
      "Iteration 11, loss = 0.19215431\n",
      "Iteration 12, loss = 0.17664990\n",
      "Iteration 13, loss = 0.16139699\n",
      "Iteration 14, loss = 0.14575590\n",
      "Iteration 15, loss = 0.13252400\n",
      "Iteration 16, loss = 0.12186847\n",
      "Iteration 17, loss = 0.11361207\n",
      "Iteration 18, loss = 0.10625194\n",
      "Iteration 19, loss = 0.09836060\n",
      "Iteration 20, loss = 0.08947689\n",
      "Iteration 21, loss = 0.08140188\n",
      "Iteration 22, loss = 0.07636313\n",
      "Iteration 23, loss = 0.07216002\n",
      "Iteration 24, loss = 0.06910444\n",
      "Iteration 25, loss = 0.06672444\n",
      "Iteration 26, loss = 0.06488997\n",
      "Iteration 27, loss = 0.06362447\n",
      "Iteration 28, loss = 0.06233352\n",
      "Iteration 29, loss = 0.06105037\n",
      "Iteration 30, loss = 0.06029428\n",
      "Iteration 31, loss = 0.05926302\n",
      "Iteration 32, loss = 0.05848365\n",
      "Iteration 33, loss = 0.05789792\n",
      "Iteration 34, loss = 0.05690454\n",
      "Iteration 35, loss = 0.05620957\n",
      "Iteration 36, loss = 0.05555471\n",
      "Iteration 37, loss = 0.05508800\n",
      "Iteration 38, loss = 0.05440091\n",
      "Iteration 39, loss = 0.05387434\n",
      "Iteration 40, loss = 0.05341166\n",
      "Iteration 41, loss = 0.05288083\n",
      "Iteration 42, loss = 0.05219384\n",
      "Iteration 43, loss = 0.05188817\n",
      "Iteration 44, loss = 0.05153289\n",
      "Iteration 45, loss = 0.05090625\n",
      "Iteration 46, loss = 0.05035731\n",
      "Iteration 47, loss = 0.04995800\n",
      "Iteration 48, loss = 0.04953414\n",
      "Iteration 49, loss = 0.04910242\n",
      "Iteration 50, loss = 0.04861927\n",
      "Iteration 51, loss = 0.04823496\n",
      "Iteration 52, loss = 0.04786842\n",
      "Iteration 53, loss = 0.04756751\n",
      "Iteration 54, loss = 0.04712622\n",
      "Iteration 55, loss = 0.04670330\n",
      "Iteration 56, loss = 0.04653434\n",
      "Iteration 57, loss = 0.04629549\n",
      "Iteration 58, loss = 0.04588565\n",
      "Iteration 59, loss = 0.04528819\n",
      "Iteration 60, loss = 0.04492697\n",
      "Iteration 61, loss = 0.04466252\n",
      "Iteration 62, loss = 0.04439538\n",
      "Iteration 63, loss = 0.04422090\n",
      "Iteration 64, loss = 0.04379019\n",
      "Iteration 65, loss = 0.04353221\n",
      "Iteration 66, loss = 0.04315436\n",
      "Iteration 67, loss = 0.04279441\n",
      "Iteration 68, loss = 0.04254517\n",
      "Iteration 69, loss = 0.04219174\n",
      "Iteration 70, loss = 0.04196963\n",
      "Iteration 71, loss = 0.04171150\n",
      "Iteration 72, loss = 0.04148164\n",
      "Iteration 73, loss = 0.04119705\n",
      "Iteration 74, loss = 0.04096532\n",
      "Iteration 75, loss = 0.04055625\n",
      "Iteration 76, loss = 0.04039300\n",
      "Iteration 77, loss = 0.03984125\n",
      "Iteration 78, loss = 0.03953024\n",
      "Iteration 79, loss = 0.03938601\n",
      "Iteration 80, loss = 0.03908734\n",
      "Iteration 81, loss = 0.03880265\n",
      "Iteration 82, loss = 0.03854824\n",
      "Iteration 83, loss = 0.03836791\n",
      "Iteration 84, loss = 0.03807454\n",
      "Iteration 85, loss = 0.03766701\n",
      "Iteration 86, loss = 0.03755076\n",
      "Iteration 87, loss = 0.03715625\n",
      "Iteration 88, loss = 0.03699420\n",
      "Iteration 89, loss = 0.03676112\n",
      "Iteration 90, loss = 0.03648387\n",
      "Iteration 91, loss = 0.03624559\n",
      "Iteration 92, loss = 0.03599132\n",
      "Iteration 93, loss = 0.03570120\n",
      "Iteration 94, loss = 0.03546085\n",
      "Iteration 95, loss = 0.03505360\n",
      "Iteration 96, loss = 0.03497785\n",
      "Iteration 97, loss = 0.03467512\n",
      "Iteration 98, loss = 0.03440188\n",
      "Iteration 99, loss = 0.03420049\n",
      "Iteration 100, loss = 0.03382406\n",
      "Iteration 101, loss = 0.03382162\n",
      "Iteration 102, loss = 0.03315065\n",
      "Iteration 103, loss = 0.03311191\n",
      "Iteration 104, loss = 0.03287198\n",
      "Iteration 105, loss = 0.03228376\n",
      "Iteration 106, loss = 0.03228330\n",
      "Iteration 107, loss = 0.03195827\n",
      "Iteration 108, loss = 0.03177081\n",
      "Iteration 109, loss = 0.03138271\n",
      "Iteration 110, loss = 0.03094806\n",
      "Iteration 111, loss = 0.03093507\n",
      "Iteration 112, loss = 0.03038350\n",
      "Iteration 113, loss = 0.03021003\n",
      "Iteration 114, loss = 0.02992703\n",
      "Iteration 115, loss = 0.02952157\n",
      "Iteration 116, loss = 0.02897727\n",
      "Iteration 117, loss = 0.02885765\n",
      "Iteration 118, loss = 0.02861265\n",
      "Iteration 119, loss = 0.02815180\n",
      "Iteration 120, loss = 0.02772185\n",
      "Iteration 121, loss = 0.02797303\n",
      "Iteration 122, loss = 0.02709758\n",
      "Iteration 123, loss = 0.02692709\n",
      "Iteration 124, loss = 0.02702193\n",
      "Iteration 125, loss = 0.02659016\n",
      "Iteration 126, loss = 0.02607903\n",
      "Iteration 127, loss = 0.02617212\n",
      "Iteration 128, loss = 0.02595010\n",
      "Iteration 129, loss = 0.02561270\n",
      "Iteration 130, loss = 0.02548306\n",
      "Iteration 131, loss = 0.02527484\n",
      "Iteration 132, loss = 0.02490003\n",
      "Iteration 133, loss = 0.02467961\n",
      "Iteration 134, loss = 0.02427263\n",
      "Iteration 135, loss = 0.02429396\n",
      "Iteration 136, loss = 0.02403081\n",
      "Iteration 137, loss = 0.02394715\n",
      "Iteration 138, loss = 0.02371413\n",
      "Iteration 139, loss = 0.02339968\n",
      "Iteration 140, loss = 0.02319052\n",
      "Iteration 141, loss = 0.02287333\n",
      "Iteration 142, loss = 0.02277327\n",
      "Iteration 143, loss = 0.02264199\n",
      "Iteration 144, loss = 0.02239234\n",
      "Iteration 145, loss = 0.02244664\n",
      "Iteration 146, loss = 0.02202961\n",
      "Iteration 147, loss = 0.02198026\n",
      "Iteration 148, loss = 0.02179692\n",
      "Iteration 149, loss = 0.02166602\n",
      "Iteration 150, loss = 0.02152098\n",
      "Iteration 151, loss = 0.02144435\n",
      "Iteration 152, loss = 0.02126817\n",
      "Iteration 153, loss = 0.02089282\n",
      "Iteration 154, loss = 0.02076435\n",
      "Iteration 155, loss = 0.02082116\n",
      "Iteration 156, loss = 0.02050982\n",
      "Iteration 157, loss = 0.02022310\n",
      "Iteration 158, loss = 0.01987304\n",
      "Iteration 159, loss = 0.01984383\n",
      "Iteration 160, loss = 0.01971039\n",
      "Iteration 161, loss = 0.01954933\n",
      "Iteration 162, loss = 0.01943729\n",
      "Iteration 163, loss = 0.01938430\n",
      "Iteration 164, loss = 0.01974873\n",
      "Iteration 165, loss = 0.01908877\n",
      "Iteration 166, loss = 0.01936817\n",
      "Iteration 167, loss = 0.01935403\n",
      "Iteration 168, loss = 0.01892436\n",
      "Iteration 169, loss = 0.01899347\n",
      "Iteration 170, loss = 0.01870746\n",
      "Iteration 171, loss = 0.01910181\n",
      "Iteration 172, loss = 0.01882243\n",
      "Iteration 173, loss = 0.01843105\n",
      "Iteration 174, loss = 0.01845982\n",
      "Iteration 175, loss = 0.01870748\n",
      "Iteration 176, loss = 0.01836543\n",
      "Iteration 177, loss = 0.01834452\n",
      "Iteration 178, loss = 0.01823780\n",
      "Iteration 179, loss = 0.01812838\n",
      "Iteration 180, loss = 0.01801335\n",
      "Iteration 181, loss = 0.01788506\n",
      "Iteration 182, loss = 0.01797964\n",
      "Iteration 183, loss = 0.01788726\n",
      "Iteration 184, loss = 0.01790443\n",
      "Iteration 185, loss = 0.01817902\n",
      "Iteration 186, loss = 0.01779711\n",
      "Iteration 187, loss = 0.01799831\n",
      "Iteration 188, loss = 0.01773173\n",
      "Iteration 189, loss = 0.01764536\n",
      "Iteration 190, loss = 0.01741202\n",
      "Iteration 191, loss = 0.01748460\n",
      "Iteration 192, loss = 0.01720829\n",
      "Iteration 193, loss = 0.01717058\n",
      "Iteration 194, loss = 0.01699395\n",
      "Iteration 195, loss = 0.01702274\n",
      "Iteration 196, loss = 0.01714151\n",
      "Iteration 197, loss = 0.01693021\n",
      "Iteration 198, loss = 0.01686337\n",
      "Iteration 199, loss = 0.01745110\n",
      "Iteration 200, loss = 0.01666913\n",
      "Iteration 201, loss = 0.01687997\n",
      "Iteration 202, loss = 0.01737291\n",
      "Iteration 203, loss = 0.01661565\n",
      "Iteration 204, loss = 0.01676763\n",
      "Iteration 205, loss = 0.01661411\n",
      "Iteration 206, loss = 0.01682806\n",
      "Iteration 207, loss = 0.01652265\n",
      "Iteration 208, loss = 0.01618823\n",
      "Iteration 209, loss = 0.01632728\n",
      "Iteration 210, loss = 0.01643086\n",
      "Iteration 211, loss = 0.01608243\n",
      "Iteration 212, loss = 0.01645037\n",
      "Iteration 213, loss = 0.01598173\n",
      "Iteration 214, loss = 0.01589220\n",
      "Iteration 215, loss = 0.01622024\n",
      "Iteration 216, loss = 0.01695924\n",
      "Iteration 217, loss = 0.01641274\n",
      "Iteration 218, loss = 0.01660039\n",
      "Iteration 219, loss = 0.01571985\n",
      "Iteration 220, loss = 0.01567403\n",
      "Iteration 221, loss = 0.01580631\n",
      "Iteration 222, loss = 0.01583946\n",
      "Iteration 223, loss = 0.01575833\n",
      "Iteration 224, loss = 0.01604071\n",
      "Iteration 225, loss = 0.01592696\n",
      "Iteration 226, loss = 0.01576320\n",
      "Iteration 227, loss = 0.01558705\n",
      "Iteration 228, loss = 0.01589924\n",
      "Iteration 229, loss = 0.01583333\n",
      "Iteration 230, loss = 0.01567701\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.40845828\n",
      "Iteration 2, loss = 1.05907652\n",
      "Iteration 3, loss = 0.88892550\n",
      "Iteration 4, loss = 0.79413214\n",
      "Iteration 5, loss = 0.73003846\n",
      "Iteration 6, loss = 0.67261017\n",
      "Iteration 7, loss = 0.60265645\n",
      "Iteration 8, loss = 0.52109376\n",
      "Iteration 9, loss = 0.42379521\n",
      "Iteration 10, loss = 0.32419197\n",
      "Iteration 11, loss = 0.24402327\n",
      "Iteration 12, loss = 0.18856468\n",
      "Iteration 13, loss = 0.15328675\n",
      "Iteration 14, loss = 0.13038268\n",
      "Iteration 15, loss = 0.11410856\n",
      "Iteration 16, loss = 0.10122057\n",
      "Iteration 17, loss = 0.08992430\n",
      "Iteration 18, loss = 0.08274760\n",
      "Iteration 19, loss = 0.07841341\n",
      "Iteration 20, loss = 0.07491989\n",
      "Iteration 21, loss = 0.07243284\n",
      "Iteration 22, loss = 0.07020833\n",
      "Iteration 23, loss = 0.06823224\n",
      "Iteration 24, loss = 0.06685422\n",
      "Iteration 25, loss = 0.06544961\n",
      "Iteration 26, loss = 0.06427723\n",
      "Iteration 27, loss = 0.06326260\n",
      "Iteration 28, loss = 0.06278107\n",
      "Iteration 29, loss = 0.06172453\n",
      "Iteration 30, loss = 0.06112957\n",
      "Iteration 31, loss = 0.06055899\n",
      "Iteration 32, loss = 0.06016433\n",
      "Iteration 33, loss = 0.05948641\n",
      "Iteration 34, loss = 0.05893691\n",
      "Iteration 35, loss = 0.05833305\n",
      "Iteration 36, loss = 0.05791499\n",
      "Iteration 37, loss = 0.05731123\n",
      "Iteration 38, loss = 0.05687717\n",
      "Iteration 39, loss = 0.05678032\n",
      "Iteration 40, loss = 0.05610452\n",
      "Iteration 41, loss = 0.05574753\n",
      "Iteration 42, loss = 0.05547401\n",
      "Iteration 43, loss = 0.05501844\n",
      "Iteration 44, loss = 0.05427580\n",
      "Iteration 45, loss = 0.05428228\n",
      "Iteration 46, loss = 0.05395330\n",
      "Iteration 47, loss = 0.05364777\n",
      "Iteration 48, loss = 0.05305244\n",
      "Iteration 49, loss = 0.05243954\n",
      "Iteration 50, loss = 0.05212682\n",
      "Iteration 51, loss = 0.05148407\n",
      "Iteration 52, loss = 0.05093013\n",
      "Iteration 53, loss = 0.05070789\n",
      "Iteration 54, loss = 0.05034361\n",
      "Iteration 55, loss = 0.04979088\n",
      "Iteration 56, loss = 0.04963184\n",
      "Iteration 57, loss = 0.04936313\n",
      "Iteration 58, loss = 0.04868399\n",
      "Iteration 59, loss = 0.04844147\n",
      "Iteration 60, loss = 0.04805022\n",
      "Iteration 61, loss = 0.04755465\n",
      "Iteration 62, loss = 0.04730089\n",
      "Iteration 63, loss = 0.04704253\n",
      "Iteration 64, loss = 0.04650046\n",
      "Iteration 65, loss = 0.04626881\n",
      "Iteration 66, loss = 0.04608103\n",
      "Iteration 67, loss = 0.04548147\n",
      "Iteration 68, loss = 0.04518438\n",
      "Iteration 69, loss = 0.04486397\n",
      "Iteration 70, loss = 0.04429317\n",
      "Iteration 71, loss = 0.04424195\n",
      "Iteration 72, loss = 0.04404238\n",
      "Iteration 73, loss = 0.04349694\n",
      "Iteration 74, loss = 0.04324962\n",
      "Iteration 75, loss = 0.04267109\n",
      "Iteration 76, loss = 0.04252889\n",
      "Iteration 77, loss = 0.04212824\n",
      "Iteration 78, loss = 0.04176644\n",
      "Iteration 79, loss = 0.04148845\n",
      "Iteration 80, loss = 0.04119802\n",
      "Iteration 81, loss = 0.04067498\n",
      "Iteration 82, loss = 0.04046350\n",
      "Iteration 83, loss = 0.03999327\n",
      "Iteration 84, loss = 0.03966694\n",
      "Iteration 85, loss = 0.03938288\n",
      "Iteration 86, loss = 0.03911535\n",
      "Iteration 87, loss = 0.03843338\n",
      "Iteration 88, loss = 0.03825502\n",
      "Iteration 89, loss = 0.03782529\n",
      "Iteration 90, loss = 0.03744421\n",
      "Iteration 91, loss = 0.03783294\n",
      "Iteration 92, loss = 0.03735208\n",
      "Iteration 93, loss = 0.03653382\n",
      "Iteration 94, loss = 0.03635093\n",
      "Iteration 95, loss = 0.03615590\n",
      "Iteration 96, loss = 0.03582778\n",
      "Iteration 97, loss = 0.03543768\n",
      "Iteration 98, loss = 0.03517460\n",
      "Iteration 99, loss = 0.03494176\n",
      "Iteration 100, loss = 0.03470506\n",
      "Iteration 101, loss = 0.03435654\n",
      "Iteration 102, loss = 0.03385729\n",
      "Iteration 103, loss = 0.03367238\n",
      "Iteration 104, loss = 0.03341673\n",
      "Iteration 105, loss = 0.03304618\n",
      "Iteration 106, loss = 0.03305395\n",
      "Iteration 107, loss = 0.03252969\n",
      "Iteration 108, loss = 0.03210707\n",
      "Iteration 109, loss = 0.03203017\n",
      "Iteration 110, loss = 0.03153889\n",
      "Iteration 111, loss = 0.03140376\n",
      "Iteration 112, loss = 0.03095424\n",
      "Iteration 113, loss = 0.03081476\n",
      "Iteration 114, loss = 0.03040293\n",
      "Iteration 115, loss = 0.02988831\n",
      "Iteration 116, loss = 0.02966723\n",
      "Iteration 117, loss = 0.02926214\n",
      "Iteration 118, loss = 0.02869727\n",
      "Iteration 119, loss = 0.02844768\n",
      "Iteration 120, loss = 0.02773258\n",
      "Iteration 121, loss = 0.02740097\n",
      "Iteration 122, loss = 0.02695159\n",
      "Iteration 123, loss = 0.02673782\n",
      "Iteration 124, loss = 0.02659191\n",
      "Iteration 125, loss = 0.02592007\n",
      "Iteration 126, loss = 0.02573171\n",
      "Iteration 127, loss = 0.02609414\n",
      "Iteration 128, loss = 0.02512599\n",
      "Iteration 129, loss = 0.02507655\n",
      "Iteration 130, loss = 0.02522950\n",
      "Iteration 131, loss = 0.02474414\n",
      "Iteration 132, loss = 0.02434791\n",
      "Iteration 133, loss = 0.02428696\n",
      "Iteration 134, loss = 0.02419008\n",
      "Iteration 135, loss = 0.02404047\n",
      "Iteration 136, loss = 0.02340873\n",
      "Iteration 137, loss = 0.02328413\n",
      "Iteration 138, loss = 0.02349284\n",
      "Iteration 139, loss = 0.02309972\n",
      "Iteration 140, loss = 0.02346153\n",
      "Iteration 141, loss = 0.02289426\n",
      "Iteration 142, loss = 0.02267313\n",
      "Iteration 143, loss = 0.02271483\n",
      "Iteration 144, loss = 0.02273086\n",
      "Iteration 145, loss = 0.02213172\n",
      "Iteration 146, loss = 0.02213060\n",
      "Iteration 147, loss = 0.02222296\n",
      "Iteration 148, loss = 0.02193438\n",
      "Iteration 149, loss = 0.02179165\n",
      "Iteration 150, loss = 0.02224391\n",
      "Iteration 151, loss = 0.02170819\n",
      "Iteration 152, loss = 0.02130194\n",
      "Iteration 153, loss = 0.02169944\n",
      "Iteration 154, loss = 0.02143321\n",
      "Iteration 155, loss = 0.02109438\n",
      "Iteration 156, loss = 0.02143920\n",
      "Iteration 157, loss = 0.02139645\n",
      "Iteration 158, loss = 0.02126743\n",
      "Iteration 159, loss = 0.02103091\n",
      "Iteration 160, loss = 0.02154950\n",
      "Iteration 161, loss = 0.02114416\n",
      "Iteration 162, loss = 0.02063194\n",
      "Iteration 163, loss = 0.02094993\n",
      "Iteration 164, loss = 0.02046004\n",
      "Iteration 165, loss = 0.02055999\n",
      "Iteration 166, loss = 0.02021215\n",
      "Iteration 167, loss = 0.02015370\n",
      "Iteration 168, loss = 0.02031049\n",
      "Iteration 169, loss = 0.02026069\n",
      "Iteration 170, loss = 0.02016328\n",
      "Iteration 171, loss = 0.01999598\n",
      "Iteration 172, loss = 0.02002180\n",
      "Iteration 173, loss = 0.01985426\n",
      "Iteration 174, loss = 0.01980896\n",
      "Iteration 175, loss = 0.01962399\n",
      "Iteration 176, loss = 0.01950777\n",
      "Iteration 177, loss = 0.02018386\n",
      "Iteration 178, loss = 0.01955183\n",
      "Iteration 179, loss = 0.01951848\n",
      "Iteration 180, loss = 0.01977922\n",
      "Iteration 181, loss = 0.02017736\n",
      "Iteration 182, loss = 0.01956278\n",
      "Iteration 183, loss = 0.01992742\n",
      "Iteration 184, loss = 0.01955115\n",
      "Iteration 185, loss = 0.01916992\n",
      "Iteration 186, loss = 0.01921946\n",
      "Iteration 187, loss = 0.01960675\n",
      "Iteration 188, loss = 0.01895243\n",
      "Iteration 189, loss = 0.01888813\n",
      "Iteration 190, loss = 0.01893189\n",
      "Iteration 191, loss = 0.01920612\n",
      "Iteration 192, loss = 0.01911447\n",
      "Iteration 193, loss = 0.01985214\n",
      "Iteration 194, loss = 0.01899800\n",
      "Iteration 195, loss = 0.01902088\n",
      "Iteration 196, loss = 0.01909684\n",
      "Iteration 197, loss = 0.01882143\n",
      "Iteration 198, loss = 0.01863136\n",
      "Iteration 199, loss = 0.01890364\n",
      "Iteration 200, loss = 0.01881687\n",
      "Iteration 201, loss = 0.01851435\n",
      "Iteration 202, loss = 0.01848851\n",
      "Iteration 203, loss = 0.01852851\n",
      "Iteration 204, loss = 0.01867322\n",
      "Iteration 205, loss = 0.01828927\n",
      "Iteration 206, loss = 0.01842056\n",
      "Iteration 207, loss = 0.01833871\n",
      "Iteration 208, loss = 0.01856015\n",
      "Iteration 209, loss = 0.01812754\n",
      "Iteration 210, loss = 0.01834231\n",
      "Iteration 211, loss = 0.01828345\n",
      "Iteration 212, loss = 0.01844989\n",
      "Iteration 213, loss = 0.01891316\n",
      "Iteration 214, loss = 0.01831103\n",
      "Iteration 215, loss = 0.01811585\n",
      "Iteration 216, loss = 0.01796757\n",
      "Iteration 217, loss = 0.01784781\n",
      "Iteration 218, loss = 0.01815814\n",
      "Iteration 219, loss = 0.01796293\n",
      "Iteration 220, loss = 0.01759903\n",
      "Iteration 221, loss = 0.01830110\n",
      "Iteration 222, loss = 0.01795093\n",
      "Iteration 223, loss = 0.01750926\n",
      "Iteration 224, loss = 0.01772827\n",
      "Iteration 225, loss = 0.01803690\n",
      "Iteration 226, loss = 0.01763088\n",
      "Iteration 227, loss = 0.01762966\n",
      "Iteration 228, loss = 0.01771492\n",
      "Iteration 229, loss = 0.01775676\n",
      "Iteration 230, loss = 0.01749903\n",
      "Iteration 231, loss = 0.01758302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61456996\n",
      "Iteration 2, loss = 0.50576078\n",
      "Iteration 3, loss = 0.40812298\n",
      "Iteration 4, loss = 0.32276526\n",
      "Iteration 5, loss = 0.24758026\n",
      "Iteration 6, loss = 0.19012691\n",
      "Iteration 7, loss = 0.14911850\n",
      "Iteration 8, loss = 0.11909088\n",
      "Iteration 9, loss = 0.09597044\n",
      "Iteration 10, loss = 0.07640457\n",
      "Iteration 11, loss = 0.06097805\n",
      "Iteration 12, loss = 0.05046522\n",
      "Iteration 13, loss = 0.04296329\n",
      "Iteration 14, loss = 0.03736656\n",
      "Iteration 15, loss = 0.03317880\n",
      "Iteration 16, loss = 0.02971388\n",
      "Iteration 17, loss = 0.02675900\n",
      "Iteration 18, loss = 0.02424862\n",
      "Iteration 19, loss = 0.02213806\n",
      "Iteration 20, loss = 0.02059716\n",
      "Iteration 21, loss = 0.01919234\n",
      "Iteration 22, loss = 0.01803826\n",
      "Iteration 23, loss = 0.01703227\n",
      "Iteration 24, loss = 0.01632486\n",
      "Iteration 25, loss = 0.01548965\n",
      "Iteration 26, loss = 0.01480022\n",
      "Iteration 27, loss = 0.01414913\n",
      "Iteration 28, loss = 0.01391119\n",
      "Iteration 29, loss = 0.01315649\n",
      "Iteration 30, loss = 0.01261757\n",
      "Iteration 31, loss = 0.01223947\n",
      "Iteration 32, loss = 0.01184422\n",
      "Iteration 33, loss = 0.01150004\n",
      "Iteration 34, loss = 0.01113507\n",
      "Iteration 35, loss = 0.01097782\n",
      "Iteration 36, loss = 0.01074224\n",
      "Iteration 37, loss = 0.01020107\n",
      "Iteration 38, loss = 0.01004149\n",
      "Iteration 39, loss = 0.00966601\n",
      "Iteration 40, loss = 0.00944726\n",
      "Iteration 41, loss = 0.00922949\n",
      "Iteration 42, loss = 0.00910627\n",
      "Iteration 43, loss = 0.00891832\n",
      "Iteration 44, loss = 0.00867316\n",
      "Iteration 45, loss = 0.00853376\n",
      "Iteration 46, loss = 0.00828476\n",
      "Iteration 47, loss = 0.00802777\n",
      "Iteration 48, loss = 0.00801465\n",
      "Iteration 49, loss = 0.00783528\n",
      "Iteration 50, loss = 0.00772436\n",
      "Iteration 51, loss = 0.00746774\n",
      "Iteration 52, loss = 0.00729557\n",
      "Iteration 53, loss = 0.00711602\n",
      "Iteration 54, loss = 0.00709078\n",
      "Iteration 55, loss = 0.00683385\n",
      "Iteration 56, loss = 0.00674129\n",
      "Iteration 57, loss = 0.00667044\n",
      "Iteration 58, loss = 0.00660368\n",
      "Iteration 59, loss = 0.00640719\n",
      "Iteration 60, loss = 0.00639208\n",
      "Iteration 61, loss = 0.00631762\n",
      "Iteration 62, loss = 0.00623268\n",
      "Iteration 63, loss = 0.00610560\n",
      "Iteration 64, loss = 0.00590461\n",
      "Iteration 65, loss = 0.00585541\n",
      "Iteration 66, loss = 0.00574227\n",
      "Iteration 67, loss = 0.00567959\n",
      "Iteration 68, loss = 0.00568633\n",
      "Iteration 69, loss = 0.00549075\n",
      "Iteration 70, loss = 0.00545097\n",
      "Iteration 71, loss = 0.00544980\n",
      "Iteration 72, loss = 0.00527755\n",
      "Iteration 73, loss = 0.00514333\n",
      "Iteration 74, loss = 0.00510933\n",
      "Iteration 75, loss = 0.00498905\n",
      "Iteration 76, loss = 0.00501732\n",
      "Iteration 77, loss = 0.00491742\n",
      "Iteration 78, loss = 0.00481700\n",
      "Iteration 79, loss = 0.00496161\n",
      "Iteration 80, loss = 0.00496357\n",
      "Iteration 81, loss = 0.00471588\n",
      "Iteration 82, loss = 0.00455797\n",
      "Iteration 83, loss = 0.00450828\n",
      "Iteration 84, loss = 0.00445432\n",
      "Iteration 85, loss = 0.00449446\n",
      "Iteration 86, loss = 0.00436696\n",
      "Iteration 87, loss = 0.00433936\n",
      "Iteration 88, loss = 0.00418975\n",
      "Iteration 89, loss = 0.00411819\n",
      "Iteration 90, loss = 0.00436897\n",
      "Iteration 91, loss = 0.00410427\n",
      "Iteration 92, loss = 0.00405399\n",
      "Iteration 93, loss = 0.00396382\n",
      "Iteration 94, loss = 0.00407050\n",
      "Iteration 95, loss = 0.00397177\n",
      "Iteration 96, loss = 0.00386757\n",
      "Iteration 97, loss = 0.00405085\n",
      "Iteration 98, loss = 0.00399679\n",
      "Iteration 99, loss = 0.00386962\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68547368\n",
      "Iteration 2, loss = 0.60599046\n",
      "Iteration 3, loss = 0.52529382\n",
      "Iteration 4, loss = 0.44572077\n",
      "Iteration 5, loss = 0.39106549\n",
      "Iteration 6, loss = 0.35621696\n",
      "Iteration 7, loss = 0.33243020\n",
      "Iteration 8, loss = 0.31421975\n",
      "Iteration 9, loss = 0.29848344\n",
      "Iteration 10, loss = 0.28522335\n",
      "Iteration 11, loss = 0.27462522\n",
      "Iteration 12, loss = 0.26580238\n",
      "Iteration 13, loss = 0.25806810\n",
      "Iteration 14, loss = 0.25103439\n",
      "Iteration 15, loss = 0.24450616\n",
      "Iteration 16, loss = 0.23837206\n",
      "Iteration 17, loss = 0.23261170\n",
      "Iteration 18, loss = 0.22704099\n",
      "Iteration 19, loss = 0.22181132\n",
      "Iteration 20, loss = 0.21675726\n",
      "Iteration 21, loss = 0.21196536\n",
      "Iteration 22, loss = 0.20722287\n",
      "Iteration 23, loss = 0.20272973\n",
      "Iteration 24, loss = 0.19839038\n",
      "Iteration 25, loss = 0.19416398\n",
      "Iteration 26, loss = 0.19012299\n",
      "Iteration 27, loss = 0.18610923\n",
      "Iteration 28, loss = 0.18235361\n",
      "Iteration 29, loss = 0.17863512\n",
      "Iteration 30, loss = 0.17507089\n",
      "Iteration 31, loss = 0.17149804\n",
      "Iteration 32, loss = 0.16816704\n",
      "Iteration 33, loss = 0.16487981\n",
      "Iteration 34, loss = 0.16166457\n",
      "Iteration 35, loss = 0.15861183\n",
      "Iteration 36, loss = 0.15539595\n",
      "Iteration 37, loss = 0.15256676\n",
      "Iteration 38, loss = 0.14956330\n",
      "Iteration 39, loss = 0.14671285\n",
      "Iteration 40, loss = 0.14400238\n",
      "Iteration 41, loss = 0.14133893\n",
      "Iteration 42, loss = 0.13869983\n",
      "Iteration 43, loss = 0.13627226\n",
      "Iteration 44, loss = 0.13372945\n",
      "Iteration 45, loss = 0.13133685\n",
      "Iteration 46, loss = 0.12893998\n",
      "Iteration 47, loss = 0.12662611\n",
      "Iteration 48, loss = 0.12435813\n",
      "Iteration 49, loss = 0.12223174\n",
      "Iteration 50, loss = 0.12006241\n",
      "Iteration 51, loss = 0.11787461\n",
      "Iteration 52, loss = 0.11585871\n",
      "Iteration 53, loss = 0.11399629\n",
      "Iteration 54, loss = 0.11195004\n",
      "Iteration 55, loss = 0.10997353\n",
      "Iteration 56, loss = 0.10817577\n",
      "Iteration 57, loss = 0.10625218\n",
      "Iteration 58, loss = 0.10445717\n",
      "Iteration 59, loss = 0.10268065\n",
      "Iteration 60, loss = 0.10099004\n",
      "Iteration 61, loss = 0.09937809\n",
      "Iteration 62, loss = 0.09767846\n",
      "Iteration 63, loss = 0.09611874\n",
      "Iteration 64, loss = 0.09448975\n",
      "Iteration 65, loss = 0.09292812\n",
      "Iteration 66, loss = 0.09141081\n",
      "Iteration 67, loss = 0.09005541\n",
      "Iteration 68, loss = 0.08851732\n",
      "Iteration 69, loss = 0.08709103\n",
      "Iteration 70, loss = 0.08573874\n",
      "Iteration 71, loss = 0.08444911\n",
      "Iteration 72, loss = 0.08303113\n",
      "Iteration 73, loss = 0.08174886\n",
      "Iteration 74, loss = 0.08049246\n",
      "Iteration 75, loss = 0.07914980\n",
      "Iteration 76, loss = 0.07801875\n",
      "Iteration 77, loss = 0.07692231\n",
      "Iteration 78, loss = 0.07559550\n",
      "Iteration 79, loss = 0.07443989\n",
      "Iteration 80, loss = 0.07333473\n",
      "Iteration 81, loss = 0.07233698\n",
      "Iteration 82, loss = 0.07115690\n",
      "Iteration 83, loss = 0.06997643\n",
      "Iteration 84, loss = 0.06891435\n",
      "Iteration 85, loss = 0.06799431\n",
      "Iteration 86, loss = 0.06691997\n",
      "Iteration 87, loss = 0.06592906\n",
      "Iteration 88, loss = 0.06506352\n",
      "Iteration 89, loss = 0.06396181\n",
      "Iteration 90, loss = 0.06300231\n",
      "Iteration 91, loss = 0.06206693\n",
      "Iteration 92, loss = 0.06114615\n",
      "Iteration 93, loss = 0.06028668\n",
      "Iteration 94, loss = 0.05948043\n",
      "Iteration 95, loss = 0.05860014\n",
      "Iteration 96, loss = 0.05786084\n",
      "Iteration 97, loss = 0.05689929\n",
      "Iteration 98, loss = 0.05603488\n",
      "Iteration 99, loss = 0.05527307\n",
      "Iteration 100, loss = 0.05453426\n",
      "Iteration 101, loss = 0.05373172\n",
      "Iteration 102, loss = 0.05298722\n",
      "Iteration 103, loss = 0.05225096\n",
      "Iteration 104, loss = 0.05150873\n",
      "Iteration 105, loss = 0.05077858\n",
      "Iteration 106, loss = 0.05012143\n",
      "Iteration 107, loss = 0.04950450\n",
      "Iteration 108, loss = 0.04891646\n",
      "Iteration 109, loss = 0.04795941\n",
      "Iteration 110, loss = 0.04731317\n",
      "Iteration 111, loss = 0.04671703\n",
      "Iteration 112, loss = 0.04605021\n",
      "Iteration 113, loss = 0.04542894\n",
      "Iteration 114, loss = 0.04480366\n",
      "Iteration 115, loss = 0.04419760\n",
      "Iteration 116, loss = 0.04359802\n",
      "Iteration 117, loss = 0.04311273\n",
      "Iteration 118, loss = 0.04247468\n",
      "Iteration 119, loss = 0.04182720\n",
      "Iteration 120, loss = 0.04134435\n",
      "Iteration 121, loss = 0.04080945\n",
      "Iteration 122, loss = 0.04036169\n",
      "Iteration 123, loss = 0.03968784\n",
      "Iteration 124, loss = 0.03912418\n",
      "Iteration 125, loss = 0.03869060\n",
      "Iteration 126, loss = 0.03836215\n",
      "Iteration 127, loss = 0.03768848\n",
      "Iteration 128, loss = 0.03715604\n",
      "Iteration 129, loss = 0.03664005\n",
      "Iteration 130, loss = 0.03615635\n",
      "Iteration 131, loss = 0.03574494\n",
      "Iteration 132, loss = 0.03534742\n",
      "Iteration 133, loss = 0.03479766\n",
      "Iteration 134, loss = 0.03430560\n",
      "Iteration 135, loss = 0.03392083\n",
      "Iteration 136, loss = 0.03351417\n",
      "Iteration 137, loss = 0.03302415\n",
      "Iteration 138, loss = 0.03262904\n",
      "Iteration 139, loss = 0.03225959\n",
      "Iteration 140, loss = 0.03182001\n",
      "Iteration 141, loss = 0.03135413\n",
      "Iteration 142, loss = 0.03106142\n",
      "Iteration 143, loss = 0.03054387\n",
      "Iteration 144, loss = 0.03016760\n",
      "Iteration 145, loss = 0.02980451\n",
      "Iteration 146, loss = 0.02944246\n",
      "Iteration 147, loss = 0.02902406\n",
      "Iteration 148, loss = 0.02865594\n",
      "Iteration 149, loss = 0.02842214\n",
      "Iteration 150, loss = 0.02809796\n",
      "Iteration 151, loss = 0.02774907\n",
      "Iteration 152, loss = 0.02726171\n",
      "Iteration 153, loss = 0.02696914\n",
      "Iteration 154, loss = 0.02671555\n",
      "Iteration 155, loss = 0.02628204\n",
      "Iteration 156, loss = 0.02597210\n",
      "Iteration 157, loss = 0.02570248\n",
      "Iteration 158, loss = 0.02537099\n",
      "Iteration 159, loss = 0.02505551\n",
      "Iteration 160, loss = 0.02476385\n",
      "Iteration 161, loss = 0.02446288\n",
      "Iteration 162, loss = 0.02432521\n",
      "Iteration 163, loss = 0.02387087\n",
      "Iteration 164, loss = 0.02359484\n",
      "Iteration 165, loss = 0.02323113\n",
      "Iteration 166, loss = 0.02292842\n",
      "Iteration 167, loss = 0.02269008\n",
      "Iteration 168, loss = 0.02236245\n",
      "Iteration 169, loss = 0.02208738\n",
      "Iteration 170, loss = 0.02190804\n",
      "Iteration 171, loss = 0.02161189\n",
      "Iteration 172, loss = 0.02147865\n",
      "Iteration 173, loss = 0.02112229\n",
      "Iteration 174, loss = 0.02088704\n",
      "Iteration 175, loss = 0.02052806\n",
      "Iteration 176, loss = 0.02039153\n",
      "Iteration 177, loss = 0.02003839\n",
      "Iteration 178, loss = 0.01978258\n",
      "Iteration 179, loss = 0.01959238\n",
      "Iteration 180, loss = 0.01937182\n",
      "Iteration 181, loss = 0.01911527\n",
      "Iteration 182, loss = 0.01887543\n",
      "Iteration 183, loss = 0.01862530\n",
      "Iteration 184, loss = 0.01840127\n",
      "Iteration 185, loss = 0.01820151\n",
      "Iteration 186, loss = 0.01812707\n",
      "Iteration 187, loss = 0.01787213\n",
      "Iteration 188, loss = 0.01753602\n",
      "Iteration 189, loss = 0.01735291\n",
      "Iteration 190, loss = 0.01721147\n",
      "Iteration 191, loss = 0.01701364\n",
      "Iteration 192, loss = 0.01670544\n",
      "Iteration 193, loss = 0.01654634\n",
      "Iteration 194, loss = 0.01643412\n",
      "Iteration 195, loss = 0.01617184\n",
      "Iteration 196, loss = 0.01611355\n",
      "Iteration 197, loss = 0.01609524\n",
      "Iteration 198, loss = 0.01585249\n",
      "Iteration 199, loss = 0.01538814\n",
      "Iteration 200, loss = 0.01521669\n",
      "Iteration 201, loss = 0.01533236\n",
      "Iteration 202, loss = 0.01507757\n",
      "Iteration 203, loss = 0.01476065\n",
      "Iteration 204, loss = 0.01469361\n",
      "Iteration 205, loss = 0.01452369\n",
      "Iteration 206, loss = 0.01427353\n",
      "Iteration 207, loss = 0.01416539\n",
      "Iteration 208, loss = 0.01396743\n",
      "Iteration 209, loss = 0.01376196\n",
      "Iteration 210, loss = 0.01363062\n",
      "Iteration 211, loss = 0.01345985\n",
      "Iteration 212, loss = 0.01331499\n",
      "Iteration 213, loss = 0.01325042\n",
      "Iteration 214, loss = 0.01307090\n",
      "Iteration 215, loss = 0.01282664\n",
      "Iteration 216, loss = 0.01268756\n",
      "Iteration 217, loss = 0.01247603\n",
      "Iteration 218, loss = 0.01238761\n",
      "Iteration 219, loss = 0.01216179\n",
      "Iteration 220, loss = 0.01216277\n",
      "Iteration 221, loss = 0.01192438\n",
      "Iteration 222, loss = 0.01177435\n",
      "Iteration 223, loss = 0.01176215\n",
      "Iteration 224, loss = 0.01169046\n",
      "Iteration 225, loss = 0.01163764\n",
      "Iteration 226, loss = 0.01139613\n",
      "Iteration 227, loss = 0.01125763\n",
      "Iteration 228, loss = 0.01100427\n",
      "Iteration 229, loss = 0.01086581\n",
      "Iteration 230, loss = 0.01082335\n",
      "Iteration 231, loss = 0.01077375\n",
      "Iteration 232, loss = 0.01048256\n",
      "Iteration 233, loss = 0.01041496\n",
      "Iteration 234, loss = 0.01031857\n",
      "Iteration 235, loss = 0.01020510\n",
      "Iteration 236, loss = 0.01002462\n",
      "Iteration 237, loss = 0.01009500\n",
      "Iteration 238, loss = 0.00987875\n",
      "Iteration 239, loss = 0.00968385\n",
      "Iteration 240, loss = 0.00955029\n",
      "Iteration 241, loss = 0.00959183\n",
      "Iteration 242, loss = 0.00946775\n",
      "Iteration 243, loss = 0.00931083\n",
      "Iteration 244, loss = 0.00915101\n",
      "Iteration 245, loss = 0.00901812\n",
      "Iteration 246, loss = 0.00896241\n",
      "Iteration 247, loss = 0.00883787\n",
      "Iteration 248, loss = 0.00874223\n",
      "Iteration 249, loss = 0.00872248\n",
      "Iteration 250, loss = 0.00861707\n",
      "Iteration 251, loss = 0.00855559\n",
      "Iteration 252, loss = 0.00844663\n",
      "Iteration 253, loss = 0.00828481\n",
      "Iteration 254, loss = 0.00827297\n",
      "Iteration 255, loss = 0.00806682\n",
      "Iteration 256, loss = 0.00799649\n",
      "Iteration 257, loss = 0.00790783\n",
      "Iteration 258, loss = 0.00784705\n",
      "Iteration 259, loss = 0.00777562\n",
      "Iteration 260, loss = 0.00771305\n",
      "Iteration 261, loss = 0.00753071\n",
      "Iteration 262, loss = 0.00756357\n",
      "Iteration 263, loss = 0.00758617\n",
      "Iteration 264, loss = 0.00733931\n",
      "Iteration 265, loss = 0.00727240\n",
      "Iteration 266, loss = 0.00712091\n",
      "Iteration 267, loss = 0.00708404\n",
      "Iteration 268, loss = 0.00704045\n",
      "Iteration 269, loss = 0.00688801\n",
      "Iteration 270, loss = 0.00694470\n",
      "Iteration 271, loss = 0.00685651\n",
      "Iteration 272, loss = 0.00667203\n",
      "Iteration 273, loss = 0.00660356\n",
      "Iteration 274, loss = 0.00662479\n",
      "Iteration 275, loss = 0.00646169\n",
      "Iteration 276, loss = 0.00640707\n",
      "Iteration 277, loss = 0.00639196\n",
      "Iteration 278, loss = 0.00627785\n",
      "Iteration 279, loss = 0.00617149\n",
      "Iteration 280, loss = 0.00622580\n",
      "Iteration 281, loss = 0.00607818\n",
      "Iteration 282, loss = 0.00605918\n",
      "Iteration 283, loss = 0.00594266\n",
      "Iteration 284, loss = 0.00584366\n",
      "Iteration 285, loss = 0.00582257\n",
      "Iteration 286, loss = 0.00583882\n",
      "Iteration 287, loss = 0.00573980\n",
      "Iteration 288, loss = 0.00566364\n",
      "Iteration 289, loss = 0.00558904\n",
      "Iteration 290, loss = 0.00557203\n",
      "Iteration 291, loss = 0.00560469\n",
      "Iteration 292, loss = 0.00540613\n",
      "Iteration 293, loss = 0.00545397\n",
      "Iteration 294, loss = 0.00527148\n",
      "Iteration 295, loss = 0.00520516\n",
      "Iteration 296, loss = 0.00521555\n",
      "Iteration 297, loss = 0.00521327\n",
      "Iteration 298, loss = 0.00499655\n",
      "Iteration 299, loss = 0.00502135\n",
      "Iteration 300, loss = 0.00496862\n",
      "Iteration 301, loss = 0.00486789\n",
      "Iteration 302, loss = 0.00497499\n",
      "Iteration 303, loss = 0.00476885\n",
      "Iteration 304, loss = 0.00485551\n",
      "Iteration 305, loss = 0.00479988\n",
      "Iteration 306, loss = 0.00465861\n",
      "Iteration 307, loss = 0.00466321\n",
      "Iteration 308, loss = 0.00447031\n",
      "Iteration 309, loss = 0.00450624\n",
      "Iteration 310, loss = 0.00461632\n",
      "Iteration 311, loss = 0.00438970\n",
      "Iteration 312, loss = 0.00446010\n",
      "Iteration 313, loss = 0.00430554\n",
      "Iteration 314, loss = 0.00430194\n",
      "Iteration 315, loss = 0.00420985\n",
      "Iteration 316, loss = 0.00417976\n",
      "Iteration 317, loss = 0.00410398\n",
      "Iteration 318, loss = 0.00407222\n",
      "Iteration 319, loss = 0.00412014\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61168958\n",
      "Iteration 2, loss = 0.56056086\n",
      "Iteration 3, loss = 0.53094708\n",
      "Iteration 4, loss = 0.50857306\n",
      "Iteration 5, loss = 0.48865913\n",
      "Iteration 6, loss = 0.46746088\n",
      "Iteration 7, loss = 0.44002272\n",
      "Iteration 8, loss = 0.40539527\n",
      "Iteration 9, loss = 0.36875096\n",
      "Iteration 10, loss = 0.33092917\n",
      "Iteration 11, loss = 0.29476509\n",
      "Iteration 12, loss = 0.26092506\n",
      "Iteration 13, loss = 0.23041530\n",
      "Iteration 14, loss = 0.20260193\n",
      "Iteration 15, loss = 0.17771328\n",
      "Iteration 16, loss = 0.15539403\n",
      "Iteration 17, loss = 0.13790931\n",
      "Iteration 18, loss = 0.12468872\n",
      "Iteration 19, loss = 0.11425678\n",
      "Iteration 20, loss = 0.10626790\n",
      "Iteration 21, loss = 0.09996410\n",
      "Iteration 22, loss = 0.09442703\n",
      "Iteration 23, loss = 0.09003607\n",
      "Iteration 24, loss = 0.08562969\n",
      "Iteration 25, loss = 0.08124466\n",
      "Iteration 26, loss = 0.07832114\n",
      "Iteration 27, loss = 0.07615375\n",
      "Iteration 28, loss = 0.07421299\n",
      "Iteration 29, loss = 0.07231918\n",
      "Iteration 30, loss = 0.07097279\n",
      "Iteration 31, loss = 0.07026956\n",
      "Iteration 32, loss = 0.06894382\n",
      "Iteration 33, loss = 0.06818258\n",
      "Iteration 34, loss = 0.06697795\n",
      "Iteration 35, loss = 0.06685706\n",
      "Iteration 36, loss = 0.06582741\n",
      "Iteration 37, loss = 0.06582757\n",
      "Iteration 38, loss = 0.06462102\n",
      "Iteration 39, loss = 0.06371018\n",
      "Iteration 40, loss = 0.06348533\n",
      "Iteration 41, loss = 0.06325570\n",
      "Iteration 42, loss = 0.06315308\n",
      "Iteration 43, loss = 0.06282348\n",
      "Iteration 44, loss = 0.06277957\n",
      "Iteration 45, loss = 0.06307513\n",
      "Iteration 46, loss = 0.06254293\n",
      "Iteration 47, loss = 0.06187013\n",
      "Iteration 48, loss = 0.06181761\n",
      "Iteration 49, loss = 0.06162223\n",
      "Iteration 50, loss = 0.06135252\n",
      "Iteration 51, loss = 0.06133903\n",
      "Iteration 52, loss = 0.06126017\n",
      "Iteration 53, loss = 0.06119552\n",
      "Iteration 54, loss = 0.06111267\n",
      "Iteration 55, loss = 0.06087847\n",
      "Iteration 56, loss = 0.06169665\n",
      "Iteration 57, loss = 0.06016363\n",
      "Iteration 58, loss = 0.06071130\n",
      "Iteration 59, loss = 0.06034291\n",
      "Iteration 60, loss = 0.06002514\n",
      "Iteration 61, loss = 0.06025317\n",
      "Iteration 62, loss = 0.05985874\n",
      "Iteration 63, loss = 0.06019259\n",
      "Iteration 64, loss = 0.05995513\n",
      "Iteration 65, loss = 0.05970623\n",
      "Iteration 66, loss = 0.05947480\n",
      "Iteration 67, loss = 0.05968234\n",
      "Iteration 68, loss = 0.05941673\n",
      "Iteration 69, loss = 0.05943028\n",
      "Iteration 70, loss = 0.06045713\n",
      "Iteration 71, loss = 0.06007709\n",
      "Iteration 72, loss = 0.05931946\n",
      "Iteration 73, loss = 0.05904706\n",
      "Iteration 74, loss = 0.05903601\n",
      "Iteration 75, loss = 0.05917825\n",
      "Iteration 76, loss = 0.05896614\n",
      "Iteration 77, loss = 0.05889185\n",
      "Iteration 78, loss = 0.05811958\n",
      "Iteration 79, loss = 0.05890100\n",
      "Iteration 80, loss = 0.05835374\n",
      "Iteration 81, loss = 0.05844802\n",
      "Iteration 82, loss = 0.05826881\n",
      "Iteration 83, loss = 0.05835034\n",
      "Iteration 84, loss = 0.05847019\n",
      "Iteration 85, loss = 0.05836065\n",
      "Iteration 86, loss = 0.05763719\n",
      "Iteration 87, loss = 0.05757474\n",
      "Iteration 88, loss = 0.05817217\n",
      "Iteration 89, loss = 0.05800220\n",
      "Iteration 90, loss = 0.05807934\n",
      "Iteration 91, loss = 0.05885347\n",
      "Iteration 92, loss = 0.05830168\n",
      "Iteration 93, loss = 0.05741597\n",
      "Iteration 94, loss = 0.05699108\n",
      "Iteration 95, loss = 0.05689120\n",
      "Iteration 96, loss = 0.05770843\n",
      "Iteration 97, loss = 0.05747059\n",
      "Iteration 98, loss = 0.05699268\n",
      "Iteration 99, loss = 0.05737907\n",
      "Iteration 100, loss = 0.05683910\n",
      "Iteration 101, loss = 0.05665591\n",
      "Iteration 102, loss = 0.05630410\n",
      "Iteration 103, loss = 0.05669562\n",
      "Iteration 104, loss = 0.05628838\n",
      "Iteration 105, loss = 0.05616570\n",
      "Iteration 106, loss = 0.05626018\n",
      "Iteration 107, loss = 0.05604120\n",
      "Iteration 108, loss = 0.05615689\n",
      "Iteration 109, loss = 0.05654262\n",
      "Iteration 110, loss = 0.05644593\n",
      "Iteration 111, loss = 0.05580922\n",
      "Iteration 112, loss = 0.05567798\n",
      "Iteration 113, loss = 0.05581262\n",
      "Iteration 114, loss = 0.05670441\n",
      "Iteration 115, loss = 0.05660027\n",
      "Iteration 116, loss = 0.05542296\n",
      "Iteration 117, loss = 0.05602155\n",
      "Iteration 118, loss = 0.05527098\n",
      "Iteration 119, loss = 0.05521744\n",
      "Iteration 120, loss = 0.05545700\n",
      "Iteration 121, loss = 0.05524537\n",
      "Iteration 122, loss = 0.05580349\n",
      "Iteration 123, loss = 0.05582247\n",
      "Iteration 124, loss = 0.05501640\n",
      "Iteration 125, loss = 0.05578428\n",
      "Iteration 126, loss = 0.05523419\n",
      "Iteration 127, loss = 0.05493279\n",
      "Iteration 128, loss = 0.05503923\n",
      "Iteration 129, loss = 0.05458318\n",
      "Iteration 130, loss = 0.05467456\n",
      "Iteration 131, loss = 0.05491928\n",
      "Iteration 132, loss = 0.05452259\n",
      "Iteration 133, loss = 0.05604109\n",
      "Iteration 134, loss = 0.05464718\n",
      "Iteration 135, loss = 0.05421212\n",
      "Iteration 136, loss = 0.05407146\n",
      "Iteration 137, loss = 0.05428231\n",
      "Iteration 138, loss = 0.05473385\n",
      "Iteration 139, loss = 0.05415849\n",
      "Iteration 140, loss = 0.05425209\n",
      "Iteration 141, loss = 0.05404956\n",
      "Iteration 142, loss = 0.05402950\n",
      "Iteration 143, loss = 0.05395314\n",
      "Iteration 144, loss = 0.05374757\n",
      "Iteration 145, loss = 0.05370911\n",
      "Iteration 146, loss = 0.05379661\n",
      "Iteration 147, loss = 0.05352103\n",
      "Iteration 148, loss = 0.05338666\n",
      "Iteration 149, loss = 0.05357432\n",
      "Iteration 150, loss = 0.05334143\n",
      "Iteration 151, loss = 0.05276643\n",
      "Iteration 152, loss = 0.05396081\n",
      "Iteration 153, loss = 0.05330408\n",
      "Iteration 154, loss = 0.05328770\n",
      "Iteration 155, loss = 0.05293983\n",
      "Iteration 156, loss = 0.05307536\n",
      "Iteration 157, loss = 0.05288647\n",
      "Iteration 158, loss = 0.05321266\n",
      "Iteration 159, loss = 0.05285259\n",
      "Iteration 160, loss = 0.05272867\n",
      "Iteration 161, loss = 0.05272346\n",
      "Iteration 162, loss = 0.05279885\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59712314\n",
      "Iteration 2, loss = 0.44819101\n",
      "Iteration 3, loss = 0.33597303\n",
      "Iteration 4, loss = 0.25180454\n",
      "Iteration 5, loss = 0.19838469\n",
      "Iteration 6, loss = 0.16544666\n",
      "Iteration 7, loss = 0.14363878\n",
      "Iteration 8, loss = 0.12825298\n",
      "Iteration 9, loss = 0.11679584\n",
      "Iteration 10, loss = 0.10880175\n",
      "Iteration 11, loss = 0.10303859\n",
      "Iteration 12, loss = 0.09907912\n",
      "Iteration 13, loss = 0.09437051\n",
      "Iteration 14, loss = 0.08944876\n",
      "Iteration 15, loss = 0.08580996\n",
      "Iteration 16, loss = 0.08035401\n",
      "Iteration 17, loss = 0.07632470\n",
      "Iteration 18, loss = 0.07214221\n",
      "Iteration 19, loss = 0.06881350\n",
      "Iteration 20, loss = 0.06479311\n",
      "Iteration 21, loss = 0.06173919\n",
      "Iteration 22, loss = 0.05842753\n",
      "Iteration 23, loss = 0.05567063\n",
      "Iteration 24, loss = 0.05323220\n",
      "Iteration 25, loss = 0.05173714\n",
      "Iteration 26, loss = 0.04931669\n",
      "Iteration 27, loss = 0.04713657\n",
      "Iteration 28, loss = 0.04492960\n",
      "Iteration 29, loss = 0.04308734\n",
      "Iteration 30, loss = 0.04177224\n",
      "Iteration 31, loss = 0.03996363\n",
      "Iteration 32, loss = 0.03816440\n",
      "Iteration 33, loss = 0.03696409\n",
      "Iteration 34, loss = 0.03537706\n",
      "Iteration 35, loss = 0.03407914\n",
      "Iteration 36, loss = 0.03255068\n",
      "Iteration 37, loss = 0.03122257\n",
      "Iteration 38, loss = 0.03009692\n",
      "Iteration 39, loss = 0.03046422\n",
      "Iteration 40, loss = 0.02798857\n",
      "Iteration 41, loss = 0.02730855\n",
      "Iteration 42, loss = 0.02611511\n",
      "Iteration 43, loss = 0.02537144\n",
      "Iteration 44, loss = 0.02408634\n",
      "Iteration 45, loss = 0.02343234\n",
      "Iteration 46, loss = 0.02281061\n",
      "Iteration 47, loss = 0.02219208\n",
      "Iteration 48, loss = 0.02123990\n",
      "Iteration 49, loss = 0.02085935\n",
      "Iteration 50, loss = 0.02012045\n",
      "Iteration 51, loss = 0.01965976\n",
      "Iteration 52, loss = 0.01931405\n",
      "Iteration 53, loss = 0.01865425\n",
      "Iteration 54, loss = 0.01817446\n",
      "Iteration 55, loss = 0.01783155\n",
      "Iteration 56, loss = 0.01756532\n",
      "Iteration 57, loss = 0.01815216\n",
      "Iteration 58, loss = 0.01705356\n",
      "Iteration 59, loss = 0.01614061\n",
      "Iteration 60, loss = 0.01591516\n",
      "Iteration 61, loss = 0.01564208\n",
      "Iteration 62, loss = 0.01578392\n",
      "Iteration 63, loss = 0.01491346\n",
      "Iteration 64, loss = 0.01523124\n",
      "Iteration 65, loss = 0.01486101\n",
      "Iteration 66, loss = 0.01457681\n",
      "Iteration 67, loss = 0.01406308\n",
      "Iteration 68, loss = 0.01356404\n",
      "Iteration 69, loss = 0.01352807\n",
      "Iteration 70, loss = 0.01316532\n",
      "Iteration 71, loss = 0.01310407\n",
      "Iteration 72, loss = 0.01290962\n",
      "Iteration 73, loss = 0.01263505\n",
      "Iteration 74, loss = 0.01232244\n",
      "Iteration 75, loss = 0.01208555\n",
      "Iteration 76, loss = 0.01201514\n",
      "Iteration 77, loss = 0.01174636\n",
      "Iteration 78, loss = 0.01255022\n",
      "Iteration 79, loss = 0.01193793\n",
      "Iteration 80, loss = 0.01123079\n",
      "Iteration 81, loss = 0.01115435\n",
      "Iteration 82, loss = 0.01144506\n",
      "Iteration 83, loss = 0.01116273\n",
      "Iteration 84, loss = 0.01065598\n",
      "Iteration 85, loss = 0.01072846\n",
      "Iteration 86, loss = 0.01086437\n",
      "Iteration 87, loss = 0.01049538\n",
      "Iteration 88, loss = 0.01022075\n",
      "Iteration 89, loss = 0.01011766\n",
      "Iteration 90, loss = 0.00987711\n",
      "Iteration 91, loss = 0.00994012\n",
      "Iteration 92, loss = 0.00964625\n",
      "Iteration 93, loss = 0.00961637\n",
      "Iteration 94, loss = 0.00940381\n",
      "Iteration 95, loss = 0.00967829\n",
      "Iteration 96, loss = 0.00914771\n",
      "Iteration 97, loss = 0.00898761\n",
      "Iteration 98, loss = 0.00893972\n",
      "Iteration 99, loss = 0.00936413\n",
      "Iteration 100, loss = 0.00910559\n",
      "Iteration 101, loss = 0.00916973\n",
      "Iteration 102, loss = 0.00846488\n",
      "Iteration 103, loss = 0.00861672\n",
      "Iteration 104, loss = 0.00894910\n",
      "Iteration 105, loss = 0.00832422\n",
      "Iteration 106, loss = 0.00841270\n",
      "Iteration 107, loss = 0.00795461\n",
      "Iteration 108, loss = 0.00807439\n",
      "Iteration 109, loss = 0.00828910\n",
      "Iteration 110, loss = 0.00808413\n",
      "Iteration 111, loss = 0.00793034\n",
      "Iteration 112, loss = 0.00767619\n",
      "Iteration 113, loss = 0.00753358\n",
      "Iteration 114, loss = 0.00733446\n",
      "Iteration 115, loss = 0.00750579\n",
      "Iteration 116, loss = 0.00766545\n",
      "Iteration 117, loss = 0.00824714\n",
      "Iteration 118, loss = 0.00716167\n",
      "Iteration 119, loss = 0.00728902\n",
      "Iteration 120, loss = 0.00776331\n",
      "Iteration 121, loss = 0.00816746\n",
      "Iteration 122, loss = 0.00731875\n",
      "Iteration 123, loss = 0.00708790\n",
      "Iteration 124, loss = 0.00692124\n",
      "Iteration 125, loss = 0.00678545\n",
      "Iteration 126, loss = 0.00707702\n",
      "Iteration 127, loss = 0.00666931\n",
      "Iteration 128, loss = 0.00651513\n",
      "Iteration 129, loss = 0.00656561\n",
      "Iteration 130, loss = 0.00655460\n",
      "Iteration 131, loss = 0.00668183\n",
      "Iteration 132, loss = 0.00662068\n",
      "Iteration 133, loss = 0.00675369\n",
      "Iteration 134, loss = 0.00678616\n",
      "Iteration 135, loss = 0.00633170\n",
      "Iteration 136, loss = 0.00630723\n",
      "Iteration 137, loss = 0.00623618\n",
      "Iteration 138, loss = 0.00599051\n",
      "Iteration 139, loss = 0.00629879\n",
      "Iteration 140, loss = 0.00586936\n",
      "Iteration 141, loss = 0.00603010\n",
      "Iteration 142, loss = 0.00664800\n",
      "Iteration 143, loss = 0.00618234\n",
      "Iteration 144, loss = 0.00567803\n",
      "Iteration 145, loss = 0.00586017\n",
      "Iteration 146, loss = 0.00569350\n",
      "Iteration 147, loss = 0.00584367\n",
      "Iteration 148, loss = 0.00585744\n",
      "Iteration 149, loss = 0.00560483\n",
      "Iteration 150, loss = 0.00627005\n",
      "Iteration 151, loss = 0.00544268\n",
      "Iteration 152, loss = 0.00548749\n",
      "Iteration 153, loss = 0.00568402\n",
      "Iteration 154, loss = 0.00537438\n",
      "Iteration 155, loss = 0.00523717\n",
      "Iteration 156, loss = 0.00563202\n",
      "Iteration 157, loss = 0.00595362\n",
      "Iteration 158, loss = 0.00527712\n",
      "Iteration 159, loss = 0.00517618\n",
      "Iteration 160, loss = 0.00518896\n",
      "Iteration 161, loss = 0.00552849\n",
      "Iteration 162, loss = 0.00494005\n",
      "Iteration 163, loss = 0.00496487\n",
      "Iteration 164, loss = 0.00549855\n",
      "Iteration 165, loss = 0.00525983\n",
      "Iteration 166, loss = 0.00502546\n",
      "Iteration 167, loss = 0.00486838\n",
      "Iteration 168, loss = 0.00483492\n",
      "Iteration 169, loss = 0.00497266\n",
      "Iteration 170, loss = 0.00464208\n",
      "Iteration 171, loss = 0.00489026\n",
      "Iteration 172, loss = 0.00471124\n",
      "Iteration 173, loss = 0.00504047\n",
      "Iteration 174, loss = 0.00472793\n",
      "Iteration 175, loss = 0.00461371\n",
      "Iteration 176, loss = 0.00630622\n",
      "Iteration 177, loss = 0.00473643\n",
      "Iteration 178, loss = 0.00441238\n",
      "Iteration 179, loss = 0.00457334\n",
      "Iteration 180, loss = 0.00458232\n",
      "Iteration 181, loss = 0.00482135\n",
      "Iteration 182, loss = 0.00442570\n",
      "Iteration 183, loss = 0.00440479\n",
      "Iteration 184, loss = 0.00494381\n",
      "Iteration 185, loss = 0.00516285\n",
      "Iteration 186, loss = 0.00461211\n",
      "Iteration 187, loss = 0.00503656\n",
      "Iteration 188, loss = 0.00537303\n",
      "Iteration 189, loss = 0.00445445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68917492\n",
      "Iteration 2, loss = 0.60747812\n",
      "Iteration 3, loss = 0.53808031\n",
      "Iteration 4, loss = 0.46301930\n",
      "Iteration 5, loss = 0.37617701\n",
      "Iteration 6, loss = 0.29496820\n",
      "Iteration 7, loss = 0.22709884\n",
      "Iteration 8, loss = 0.17364327\n",
      "Iteration 9, loss = 0.13291752\n",
      "Iteration 10, loss = 0.10572893\n",
      "Iteration 11, loss = 0.08540026\n",
      "Iteration 12, loss = 0.07023437\n",
      "Iteration 13, loss = 0.06161188\n",
      "Iteration 14, loss = 0.05593292\n",
      "Iteration 15, loss = 0.05152707\n",
      "Iteration 16, loss = 0.04817999\n",
      "Iteration 17, loss = 0.04546673\n",
      "Iteration 18, loss = 0.04351406\n",
      "Iteration 19, loss = 0.04129648\n",
      "Iteration 20, loss = 0.03995721\n",
      "Iteration 21, loss = 0.03875550\n",
      "Iteration 22, loss = 0.03737000\n",
      "Iteration 23, loss = 0.03655815\n",
      "Iteration 24, loss = 0.03564772\n",
      "Iteration 25, loss = 0.03500541\n",
      "Iteration 26, loss = 0.03399245\n",
      "Iteration 27, loss = 0.03357772\n",
      "Iteration 28, loss = 0.03310613\n",
      "Iteration 29, loss = 0.03231746\n",
      "Iteration 30, loss = 0.03197479\n",
      "Iteration 31, loss = 0.03155249\n",
      "Iteration 32, loss = 0.03120688\n",
      "Iteration 33, loss = 0.03075951\n",
      "Iteration 34, loss = 0.03041680\n",
      "Iteration 35, loss = 0.03049826\n",
      "Iteration 36, loss = 0.03008843\n",
      "Iteration 37, loss = 0.02956396\n",
      "Iteration 38, loss = 0.02990007\n",
      "Iteration 39, loss = 0.02966958\n",
      "Iteration 40, loss = 0.02925164\n",
      "Iteration 41, loss = 0.02882686\n",
      "Iteration 42, loss = 0.02860859\n",
      "Iteration 43, loss = 0.02837274\n",
      "Iteration 44, loss = 0.02804404\n",
      "Iteration 45, loss = 0.02779859\n",
      "Iteration 46, loss = 0.02777241\n",
      "Iteration 47, loss = 0.02750045\n",
      "Iteration 48, loss = 0.02741426\n",
      "Iteration 49, loss = 0.02789576\n",
      "Iteration 50, loss = 0.02719824\n",
      "Iteration 51, loss = 0.02693592\n",
      "Iteration 52, loss = 0.02657178\n",
      "Iteration 53, loss = 0.02641695\n",
      "Iteration 54, loss = 0.02627603\n",
      "Iteration 55, loss = 0.02619845\n",
      "Iteration 56, loss = 0.02600507\n",
      "Iteration 57, loss = 0.02600419\n",
      "Iteration 58, loss = 0.02580985\n",
      "Iteration 59, loss = 0.02565171\n",
      "Iteration 60, loss = 0.02556754\n",
      "Iteration 61, loss = 0.02561708\n",
      "Iteration 62, loss = 0.02535050\n",
      "Iteration 63, loss = 0.02502824\n",
      "Iteration 64, loss = 0.02484237\n",
      "Iteration 65, loss = 0.02476802\n",
      "Iteration 66, loss = 0.02462497\n",
      "Iteration 67, loss = 0.02434408\n",
      "Iteration 68, loss = 0.02439241\n",
      "Iteration 69, loss = 0.02419346\n",
      "Iteration 70, loss = 0.02402894\n",
      "Iteration 71, loss = 0.02404798\n",
      "Iteration 72, loss = 0.02398032\n",
      "Iteration 73, loss = 0.02373205\n",
      "Iteration 74, loss = 0.02365426\n",
      "Iteration 75, loss = 0.02369109\n",
      "Iteration 76, loss = 0.02382903\n",
      "Iteration 77, loss = 0.02365781\n",
      "Iteration 78, loss = 0.02297000\n",
      "Iteration 79, loss = 0.02314798\n",
      "Iteration 80, loss = 0.02276715\n",
      "Iteration 81, loss = 0.02281875\n",
      "Iteration 82, loss = 0.02273263\n",
      "Iteration 83, loss = 0.02261482\n",
      "Iteration 84, loss = 0.02223838\n",
      "Iteration 85, loss = 0.02293283\n",
      "Iteration 86, loss = 0.02296486\n",
      "Iteration 87, loss = 0.02216381\n",
      "Iteration 88, loss = 0.02179474\n",
      "Iteration 89, loss = 0.02171603\n",
      "Iteration 90, loss = 0.02158631\n",
      "Iteration 91, loss = 0.02158917\n",
      "Iteration 92, loss = 0.02135919\n",
      "Iteration 93, loss = 0.02155553\n",
      "Iteration 94, loss = 0.02105874\n",
      "Iteration 95, loss = 0.02114694\n",
      "Iteration 96, loss = 0.02100550\n",
      "Iteration 97, loss = 0.02093472\n",
      "Iteration 98, loss = 0.02092557\n",
      "Iteration 99, loss = 0.02062438\n",
      "Iteration 100, loss = 0.02071756\n",
      "Iteration 101, loss = 0.02103501\n",
      "Iteration 102, loss = 0.02006208\n",
      "Iteration 103, loss = 0.01996487\n",
      "Iteration 104, loss = 0.01951444\n",
      "Iteration 105, loss = 0.01975179\n",
      "Iteration 106, loss = 0.01969305\n",
      "Iteration 107, loss = 0.01943721\n",
      "Iteration 108, loss = 0.01913498\n",
      "Iteration 109, loss = 0.01906920\n",
      "Iteration 110, loss = 0.01896169\n",
      "Iteration 111, loss = 0.01907677\n",
      "Iteration 112, loss = 0.01869493\n",
      "Iteration 113, loss = 0.01866297\n",
      "Iteration 114, loss = 0.01842666\n",
      "Iteration 115, loss = 0.01800742\n",
      "Iteration 116, loss = 0.01843450\n",
      "Iteration 117, loss = 0.01802957\n",
      "Iteration 118, loss = 0.01786300\n",
      "Iteration 119, loss = 0.01728166\n",
      "Iteration 120, loss = 0.01768537\n",
      "Iteration 121, loss = 0.01749303\n",
      "Iteration 122, loss = 0.01694175\n",
      "Iteration 123, loss = 0.01727909\n",
      "Iteration 124, loss = 0.01702456\n",
      "Iteration 125, loss = 0.01742910\n",
      "Iteration 126, loss = 0.01673803\n",
      "Iteration 127, loss = 0.01644030\n",
      "Iteration 128, loss = 0.01615818\n",
      "Iteration 129, loss = 0.01621115\n",
      "Iteration 130, loss = 0.01589454\n",
      "Iteration 131, loss = 0.01629008\n",
      "Iteration 132, loss = 0.01569859\n",
      "Iteration 133, loss = 0.01543996\n",
      "Iteration 134, loss = 0.01546807\n",
      "Iteration 135, loss = 0.01516718\n",
      "Iteration 136, loss = 0.01530132\n",
      "Iteration 137, loss = 0.01487564\n",
      "Iteration 138, loss = 0.01469455\n",
      "Iteration 139, loss = 0.01463367\n",
      "Iteration 140, loss = 0.01457537\n",
      "Iteration 141, loss = 0.01502050\n",
      "Iteration 142, loss = 0.01456528\n",
      "Iteration 143, loss = 0.01478582\n",
      "Iteration 144, loss = 0.01393634\n",
      "Iteration 145, loss = 0.01398163\n",
      "Iteration 146, loss = 0.01386610\n",
      "Iteration 147, loss = 0.01378351\n",
      "Iteration 148, loss = 0.01332459\n",
      "Iteration 149, loss = 0.01365785\n",
      "Iteration 150, loss = 0.01341554\n",
      "Iteration 151, loss = 0.01341029\n",
      "Iteration 152, loss = 0.01278388\n",
      "Iteration 153, loss = 0.01248072\n",
      "Iteration 154, loss = 0.01333122\n",
      "Iteration 155, loss = 0.01321213\n",
      "Iteration 156, loss = 0.01251039\n",
      "Iteration 157, loss = 0.01274803\n",
      "Iteration 158, loss = 0.01249938\n",
      "Iteration 159, loss = 0.01206347\n",
      "Iteration 160, loss = 0.01177575\n",
      "Iteration 161, loss = 0.01176012\n",
      "Iteration 162, loss = 0.01172731\n",
      "Iteration 163, loss = 0.01172168\n",
      "Iteration 164, loss = 0.01116883\n",
      "Iteration 165, loss = 0.01106837\n",
      "Iteration 166, loss = 0.01124960\n",
      "Iteration 167, loss = 0.01134178\n",
      "Iteration 168, loss = 0.01136596\n",
      "Iteration 169, loss = 0.01084442\n",
      "Iteration 170, loss = 0.01083849\n",
      "Iteration 171, loss = 0.01085330\n",
      "Iteration 172, loss = 0.01145462\n",
      "Iteration 173, loss = 0.01054146\n",
      "Iteration 174, loss = 0.01083169\n",
      "Iteration 175, loss = 0.01103797\n",
      "Iteration 176, loss = 0.01090472\n",
      "Iteration 177, loss = 0.01042427\n",
      "Iteration 178, loss = 0.01048351\n",
      "Iteration 179, loss = 0.01057053\n",
      "Iteration 180, loss = 0.01025100\n",
      "Iteration 181, loss = 0.01015359\n",
      "Iteration 182, loss = 0.01009658\n",
      "Iteration 183, loss = 0.01002197\n",
      "Iteration 184, loss = 0.01018970\n",
      "Iteration 185, loss = 0.01026554\n",
      "Iteration 186, loss = 0.01040601\n",
      "Iteration 187, loss = 0.00994123\n",
      "Iteration 188, loss = 0.00995176\n",
      "Iteration 189, loss = 0.00953200\n",
      "Iteration 190, loss = 0.00972375\n",
      "Iteration 191, loss = 0.01044471\n",
      "Iteration 192, loss = 0.00932396\n",
      "Iteration 193, loss = 0.00989670\n",
      "Iteration 194, loss = 0.00978220\n",
      "Iteration 195, loss = 0.00946844\n",
      "Iteration 196, loss = 0.01006191\n",
      "Iteration 197, loss = 0.00986674\n",
      "Iteration 198, loss = 0.00944467\n",
      "Iteration 199, loss = 0.00931502\n",
      "Iteration 200, loss = 0.00953128\n",
      "Iteration 201, loss = 0.00932929\n",
      "Iteration 202, loss = 0.00945783\n",
      "Iteration 203, loss = 0.00956077\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72209415\n",
      "Iteration 2, loss = 0.65653955\n",
      "Iteration 3, loss = 0.59898988\n",
      "Iteration 4, loss = 0.54267198\n",
      "Iteration 5, loss = 0.47890978\n",
      "Iteration 6, loss = 0.40971520\n",
      "Iteration 7, loss = 0.33877107\n",
      "Iteration 8, loss = 0.26807507\n",
      "Iteration 9, loss = 0.21030800\n",
      "Iteration 10, loss = 0.16777249\n",
      "Iteration 11, loss = 0.13710249\n",
      "Iteration 12, loss = 0.11228048\n",
      "Iteration 13, loss = 0.09614351\n",
      "Iteration 14, loss = 0.08493283\n",
      "Iteration 15, loss = 0.07682544\n",
      "Iteration 16, loss = 0.07045072\n",
      "Iteration 17, loss = 0.06596895\n",
      "Iteration 18, loss = 0.06222695\n",
      "Iteration 19, loss = 0.05951728\n",
      "Iteration 20, loss = 0.05717816\n",
      "Iteration 21, loss = 0.05585770\n",
      "Iteration 22, loss = 0.05348664\n",
      "Iteration 23, loss = 0.05229314\n",
      "Iteration 24, loss = 0.05094313\n",
      "Iteration 25, loss = 0.04998884\n",
      "Iteration 26, loss = 0.04903184\n",
      "Iteration 27, loss = 0.04793804\n",
      "Iteration 28, loss = 0.04719322\n",
      "Iteration 29, loss = 0.04637950\n",
      "Iteration 30, loss = 0.04573368\n",
      "Iteration 31, loss = 0.04545927\n",
      "Iteration 32, loss = 0.04441205\n",
      "Iteration 33, loss = 0.04382746\n",
      "Iteration 34, loss = 0.04367199\n",
      "Iteration 35, loss = 0.04259358\n",
      "Iteration 36, loss = 0.04223152\n",
      "Iteration 37, loss = 0.04171808\n",
      "Iteration 38, loss = 0.04156145\n",
      "Iteration 39, loss = 0.04106761\n",
      "Iteration 40, loss = 0.04089791\n",
      "Iteration 41, loss = 0.04021750\n",
      "Iteration 42, loss = 0.03955935\n",
      "Iteration 43, loss = 0.03921884\n",
      "Iteration 44, loss = 0.03901194\n",
      "Iteration 45, loss = 0.03853526\n",
      "Iteration 46, loss = 0.03831757\n",
      "Iteration 47, loss = 0.03779760\n",
      "Iteration 48, loss = 0.03715751\n",
      "Iteration 49, loss = 0.03685749\n",
      "Iteration 50, loss = 0.03654982\n",
      "Iteration 51, loss = 0.03660244\n",
      "Iteration 52, loss = 0.03587543\n",
      "Iteration 53, loss = 0.03542799\n",
      "Iteration 54, loss = 0.03541930\n",
      "Iteration 55, loss = 0.03544284\n",
      "Iteration 56, loss = 0.03439225\n",
      "Iteration 57, loss = 0.03431120\n",
      "Iteration 58, loss = 0.03382754\n",
      "Iteration 59, loss = 0.03333660\n",
      "Iteration 60, loss = 0.03291170\n",
      "Iteration 61, loss = 0.03292687\n",
      "Iteration 62, loss = 0.03263155\n",
      "Iteration 63, loss = 0.03258463\n",
      "Iteration 64, loss = 0.03187551\n",
      "Iteration 65, loss = 0.03150695\n",
      "Iteration 66, loss = 0.03121525\n",
      "Iteration 67, loss = 0.03070277\n",
      "Iteration 68, loss = 0.03052406\n",
      "Iteration 69, loss = 0.03021911\n",
      "Iteration 70, loss = 0.03014327\n",
      "Iteration 71, loss = 0.02949207\n",
      "Iteration 72, loss = 0.02931510\n",
      "Iteration 73, loss = 0.02939644\n",
      "Iteration 74, loss = 0.02871191\n",
      "Iteration 75, loss = 0.02866821\n",
      "Iteration 76, loss = 0.02822542\n",
      "Iteration 77, loss = 0.02806616\n",
      "Iteration 78, loss = 0.02801708\n",
      "Iteration 79, loss = 0.02768542\n",
      "Iteration 80, loss = 0.02751476\n",
      "Iteration 81, loss = 0.02700132\n",
      "Iteration 82, loss = 0.02660618\n",
      "Iteration 83, loss = 0.02680227\n",
      "Iteration 84, loss = 0.02656920\n",
      "Iteration 85, loss = 0.02599117\n",
      "Iteration 86, loss = 0.02580220\n",
      "Iteration 87, loss = 0.02564203\n",
      "Iteration 88, loss = 0.02547765\n",
      "Iteration 89, loss = 0.02494894\n",
      "Iteration 90, loss = 0.02473419\n",
      "Iteration 91, loss = 0.02450328\n",
      "Iteration 92, loss = 0.02403051\n",
      "Iteration 93, loss = 0.02383194\n",
      "Iteration 94, loss = 0.02346531\n",
      "Iteration 95, loss = 0.02332685\n",
      "Iteration 96, loss = 0.02294630\n",
      "Iteration 97, loss = 0.02247768\n",
      "Iteration 98, loss = 0.02229922\n",
      "Iteration 99, loss = 0.02219407\n",
      "Iteration 100, loss = 0.02216128\n",
      "Iteration 101, loss = 0.02150922\n",
      "Iteration 102, loss = 0.02128929\n",
      "Iteration 103, loss = 0.02165172\n",
      "Iteration 104, loss = 0.02120439\n",
      "Iteration 105, loss = 0.02045785\n",
      "Iteration 106, loss = 0.02036294\n",
      "Iteration 107, loss = 0.01994716\n",
      "Iteration 108, loss = 0.01951407\n",
      "Iteration 109, loss = 0.01961646\n",
      "Iteration 110, loss = 0.01971485\n",
      "Iteration 111, loss = 0.01946452\n",
      "Iteration 112, loss = 0.01882608\n",
      "Iteration 113, loss = 0.01862288\n",
      "Iteration 114, loss = 0.01847427\n",
      "Iteration 115, loss = 0.01847181\n",
      "Iteration 116, loss = 0.01783529\n",
      "Iteration 117, loss = 0.01790573\n",
      "Iteration 118, loss = 0.01761773\n",
      "Iteration 119, loss = 0.01742905\n",
      "Iteration 120, loss = 0.01801281\n",
      "Iteration 121, loss = 0.01718367\n",
      "Iteration 122, loss = 0.01699583\n",
      "Iteration 123, loss = 0.01654877\n",
      "Iteration 124, loss = 0.01662122\n",
      "Iteration 125, loss = 0.01684834\n",
      "Iteration 126, loss = 0.01623058\n",
      "Iteration 127, loss = 0.01626982\n",
      "Iteration 128, loss = 0.01606983\n",
      "Iteration 129, loss = 0.01599116\n",
      "Iteration 130, loss = 0.01586067\n",
      "Iteration 131, loss = 0.01646840\n",
      "Iteration 132, loss = 0.01570208\n",
      "Iteration 133, loss = 0.01561650\n",
      "Iteration 134, loss = 0.01535814\n",
      "Iteration 135, loss = 0.01509693\n",
      "Iteration 136, loss = 0.01500606\n",
      "Iteration 137, loss = 0.01484917\n",
      "Iteration 138, loss = 0.01527874\n",
      "Iteration 139, loss = 0.01481692\n",
      "Iteration 140, loss = 0.01457039\n",
      "Iteration 141, loss = 0.01454228\n",
      "Iteration 142, loss = 0.01421966\n",
      "Iteration 143, loss = 0.01478328\n",
      "Iteration 144, loss = 0.01442852\n",
      "Iteration 145, loss = 0.01416296\n",
      "Iteration 146, loss = 0.01395750\n",
      "Iteration 147, loss = 0.01411519\n",
      "Iteration 148, loss = 0.01368910\n",
      "Iteration 149, loss = 0.01356204\n",
      "Iteration 150, loss = 0.01345178\n",
      "Iteration 151, loss = 0.01373218\n",
      "Iteration 152, loss = 0.01364908\n",
      "Iteration 153, loss = 0.01299534\n",
      "Iteration 154, loss = 0.01343740\n",
      "Iteration 155, loss = 0.01298167\n",
      "Iteration 156, loss = 0.01283002\n",
      "Iteration 157, loss = 0.01341148\n",
      "Iteration 158, loss = 0.01319997\n",
      "Iteration 159, loss = 0.01347327\n",
      "Iteration 160, loss = 0.01255706\n",
      "Iteration 161, loss = 0.01274198\n",
      "Iteration 162, loss = 0.01289394\n",
      "Iteration 163, loss = 0.01264352\n",
      "Iteration 164, loss = 0.01271302\n",
      "Iteration 165, loss = 0.01259792\n",
      "Iteration 166, loss = 0.01201484\n",
      "Iteration 167, loss = 0.01227154\n",
      "Iteration 168, loss = 0.01201868\n",
      "Iteration 169, loss = 0.01199542\n",
      "Iteration 170, loss = 0.01191093\n",
      "Iteration 171, loss = 0.01178656\n",
      "Iteration 172, loss = 0.01221892\n",
      "Iteration 173, loss = 0.01208277\n",
      "Iteration 174, loss = 0.01148905\n",
      "Iteration 175, loss = 0.01177878\n",
      "Iteration 176, loss = 0.01149399\n",
      "Iteration 177, loss = 0.01152490\n",
      "Iteration 178, loss = 0.01173784\n",
      "Iteration 179, loss = 0.01216169\n",
      "Iteration 180, loss = 0.01251888\n",
      "Iteration 181, loss = 0.01096948\n",
      "Iteration 182, loss = 0.01119540\n",
      "Iteration 183, loss = 0.01137653\n",
      "Iteration 184, loss = 0.01109139\n",
      "Iteration 185, loss = 0.01150555\n",
      "Iteration 186, loss = 0.01099649\n",
      "Iteration 187, loss = 0.01118785\n",
      "Iteration 188, loss = 0.01086172\n",
      "Iteration 189, loss = 0.01092124\n",
      "Iteration 190, loss = 0.01080331\n",
      "Iteration 191, loss = 0.01128572\n",
      "Iteration 192, loss = 0.01091285\n",
      "Iteration 193, loss = 0.01084477\n",
      "Iteration 194, loss = 0.01078465\n",
      "Iteration 195, loss = 0.01126479\n",
      "Iteration 196, loss = 0.01090479\n",
      "Iteration 197, loss = 0.01060949\n",
      "Iteration 198, loss = 0.01083654\n",
      "Iteration 199, loss = 0.01059173\n",
      "Iteration 200, loss = 0.01074236\n",
      "Iteration 201, loss = 0.01103496\n",
      "Iteration 202, loss = 0.01065779\n",
      "Iteration 203, loss = 0.01020493\n",
      "Iteration 204, loss = 0.01080604\n",
      "Iteration 205, loss = 0.01020397\n",
      "Iteration 206, loss = 0.01037552\n",
      "Iteration 207, loss = 0.00990650\n",
      "Iteration 208, loss = 0.01053355\n",
      "Iteration 209, loss = 0.00985904\n",
      "Iteration 210, loss = 0.01038189\n",
      "Iteration 211, loss = 0.01023426\n",
      "Iteration 212, loss = 0.01014241\n",
      "Iteration 213, loss = 0.00992932\n",
      "Iteration 214, loss = 0.00969379\n",
      "Iteration 215, loss = 0.00990578\n",
      "Iteration 216, loss = 0.01016880\n",
      "Iteration 217, loss = 0.01007609\n",
      "Iteration 218, loss = 0.00993623\n",
      "Iteration 219, loss = 0.00956202\n",
      "Iteration 220, loss = 0.00973840\n",
      "Iteration 221, loss = 0.00978082\n",
      "Iteration 222, loss = 0.00971478\n",
      "Iteration 223, loss = 0.00986252\n",
      "Iteration 224, loss = 0.00998957\n",
      "Iteration 225, loss = 0.01034651\n",
      "Iteration 226, loss = 0.00981452\n",
      "Iteration 227, loss = 0.00964694\n",
      "Iteration 228, loss = 0.00955833\n",
      "Iteration 229, loss = 0.00976246\n",
      "Iteration 230, loss = 0.00972085\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73526761\n",
      "Iteration 2, loss = 0.70410804\n",
      "Iteration 3, loss = 0.67626460\n",
      "Iteration 4, loss = 0.63309866\n",
      "Iteration 5, loss = 0.57280912\n",
      "Iteration 6, loss = 0.50317627\n",
      "Iteration 7, loss = 0.42216456\n",
      "Iteration 8, loss = 0.34068564\n",
      "Iteration 9, loss = 0.26715463\n",
      "Iteration 10, loss = 0.20491280\n",
      "Iteration 11, loss = 0.16281620\n",
      "Iteration 12, loss = 0.13545238\n",
      "Iteration 13, loss = 0.11517810\n",
      "Iteration 14, loss = 0.10011894\n",
      "Iteration 15, loss = 0.08929013\n",
      "Iteration 16, loss = 0.08121249\n",
      "Iteration 17, loss = 0.07558260\n",
      "Iteration 18, loss = 0.07050234\n",
      "Iteration 19, loss = 0.06628066\n",
      "Iteration 20, loss = 0.06331773\n",
      "Iteration 21, loss = 0.06042035\n",
      "Iteration 22, loss = 0.05861495\n",
      "Iteration 23, loss = 0.05624819\n",
      "Iteration 24, loss = 0.05457456\n",
      "Iteration 25, loss = 0.05350905\n",
      "Iteration 26, loss = 0.05199217\n",
      "Iteration 27, loss = 0.05011577\n",
      "Iteration 28, loss = 0.04955746\n",
      "Iteration 29, loss = 0.04837530\n",
      "Iteration 30, loss = 0.04760592\n",
      "Iteration 31, loss = 0.04694886\n",
      "Iteration 32, loss = 0.04613709\n",
      "Iteration 33, loss = 0.04570468\n",
      "Iteration 34, loss = 0.04471747\n",
      "Iteration 35, loss = 0.04370576\n",
      "Iteration 36, loss = 0.04311962\n",
      "Iteration 37, loss = 0.04283382\n",
      "Iteration 38, loss = 0.04216920\n",
      "Iteration 39, loss = 0.04169124\n",
      "Iteration 40, loss = 0.04117900\n",
      "Iteration 41, loss = 0.04075834\n",
      "Iteration 42, loss = 0.04027291\n",
      "Iteration 43, loss = 0.03992090\n",
      "Iteration 44, loss = 0.04050719\n",
      "Iteration 45, loss = 0.03915851\n",
      "Iteration 46, loss = 0.03931923\n",
      "Iteration 47, loss = 0.03916573\n",
      "Iteration 48, loss = 0.03842502\n",
      "Iteration 49, loss = 0.03769623\n",
      "Iteration 50, loss = 0.03760739\n",
      "Iteration 51, loss = 0.03707154\n",
      "Iteration 52, loss = 0.03708480\n",
      "Iteration 53, loss = 0.03692759\n",
      "Iteration 54, loss = 0.03672604\n",
      "Iteration 55, loss = 0.03630290\n",
      "Iteration 56, loss = 0.03583467\n",
      "Iteration 57, loss = 0.03553520\n",
      "Iteration 58, loss = 0.03523246\n",
      "Iteration 59, loss = 0.03513369\n",
      "Iteration 60, loss = 0.03472248\n",
      "Iteration 61, loss = 0.03452507\n",
      "Iteration 62, loss = 0.03465245\n",
      "Iteration 63, loss = 0.03411288\n",
      "Iteration 64, loss = 0.03396607\n",
      "Iteration 65, loss = 0.03349475\n",
      "Iteration 66, loss = 0.03299741\n",
      "Iteration 67, loss = 0.03326488\n",
      "Iteration 68, loss = 0.03293197\n",
      "Iteration 69, loss = 0.03250172\n",
      "Iteration 70, loss = 0.03291475\n",
      "Iteration 71, loss = 0.03263591\n",
      "Iteration 72, loss = 0.03220681\n",
      "Iteration 73, loss = 0.03169913\n",
      "Iteration 74, loss = 0.03206797\n",
      "Iteration 75, loss = 0.03190524\n",
      "Iteration 76, loss = 0.03166777\n",
      "Iteration 77, loss = 0.03131961\n",
      "Iteration 78, loss = 0.03100478\n",
      "Iteration 79, loss = 0.03099610\n",
      "Iteration 80, loss = 0.03088964\n",
      "Iteration 81, loss = 0.03061230\n",
      "Iteration 82, loss = 0.03042512\n",
      "Iteration 83, loss = 0.03027595\n",
      "Iteration 84, loss = 0.03050103\n",
      "Iteration 85, loss = 0.03038586\n",
      "Iteration 86, loss = 0.03025260\n",
      "Iteration 87, loss = 0.02982534\n",
      "Iteration 88, loss = 0.02931113\n",
      "Iteration 89, loss = 0.02952843\n",
      "Iteration 90, loss = 0.02926055\n",
      "Iteration 91, loss = 0.02901818\n",
      "Iteration 92, loss = 0.02875291\n",
      "Iteration 93, loss = 0.02948320\n",
      "Iteration 94, loss = 0.02900118\n",
      "Iteration 95, loss = 0.02866541\n",
      "Iteration 96, loss = 0.02895106\n",
      "Iteration 97, loss = 0.02920145\n",
      "Iteration 98, loss = 0.02846262\n",
      "Iteration 99, loss = 0.02807526\n",
      "Iteration 100, loss = 0.02808171\n",
      "Iteration 101, loss = 0.02799518\n",
      "Iteration 102, loss = 0.02794246\n",
      "Iteration 103, loss = 0.02747260\n",
      "Iteration 104, loss = 0.02767200\n",
      "Iteration 105, loss = 0.02749230\n",
      "Iteration 106, loss = 0.02732721\n",
      "Iteration 107, loss = 0.02708900\n",
      "Iteration 108, loss = 0.02751020\n",
      "Iteration 109, loss = 0.02712600\n",
      "Iteration 110, loss = 0.02668273\n",
      "Iteration 111, loss = 0.02674211\n",
      "Iteration 112, loss = 0.02665960\n",
      "Iteration 113, loss = 0.02664678\n",
      "Iteration 114, loss = 0.02631228\n",
      "Iteration 115, loss = 0.02651208\n",
      "Iteration 116, loss = 0.02607449\n",
      "Iteration 117, loss = 0.02659712\n",
      "Iteration 118, loss = 0.02568542\n",
      "Iteration 119, loss = 0.02560580\n",
      "Iteration 120, loss = 0.02596707\n",
      "Iteration 121, loss = 0.02559362\n",
      "Iteration 122, loss = 0.02552752\n",
      "Iteration 123, loss = 0.02534851\n",
      "Iteration 124, loss = 0.02514941\n",
      "Iteration 125, loss = 0.02490437\n",
      "Iteration 126, loss = 0.02517515\n",
      "Iteration 127, loss = 0.02499592\n",
      "Iteration 128, loss = 0.02554813\n",
      "Iteration 129, loss = 0.02467453\n",
      "Iteration 130, loss = 0.02472312\n",
      "Iteration 131, loss = 0.02472560\n",
      "Iteration 132, loss = 0.02491788\n",
      "Iteration 133, loss = 0.02442198\n",
      "Iteration 134, loss = 0.02436138\n",
      "Iteration 135, loss = 0.02428175\n",
      "Iteration 136, loss = 0.02400203\n",
      "Iteration 137, loss = 0.02378866\n",
      "Iteration 138, loss = 0.02372738\n",
      "Iteration 139, loss = 0.02378377\n",
      "Iteration 140, loss = 0.02339007\n",
      "Iteration 141, loss = 0.02369299\n",
      "Iteration 142, loss = 0.02337788\n",
      "Iteration 143, loss = 0.02338101\n",
      "Iteration 144, loss = 0.02340312\n",
      "Iteration 145, loss = 0.02306878\n",
      "Iteration 146, loss = 0.02305898\n",
      "Iteration 147, loss = 0.02313924\n",
      "Iteration 148, loss = 0.02330893\n",
      "Iteration 149, loss = 0.02349953\n",
      "Iteration 150, loss = 0.02323975\n",
      "Iteration 151, loss = 0.02308472\n",
      "Iteration 152, loss = 0.02306738\n",
      "Iteration 153, loss = 0.02261886\n",
      "Iteration 154, loss = 0.02257225\n",
      "Iteration 155, loss = 0.02285342\n",
      "Iteration 156, loss = 0.02278314\n",
      "Iteration 157, loss = 0.02250771\n",
      "Iteration 158, loss = 0.02215678\n",
      "Iteration 159, loss = 0.02205268\n",
      "Iteration 160, loss = 0.02200205\n",
      "Iteration 161, loss = 0.02218044\n",
      "Iteration 162, loss = 0.02181904\n",
      "Iteration 163, loss = 0.02177783\n",
      "Iteration 164, loss = 0.02178318\n",
      "Iteration 165, loss = 0.02176006\n",
      "Iteration 166, loss = 0.02146899\n",
      "Iteration 167, loss = 0.02147219\n",
      "Iteration 168, loss = 0.02158709\n",
      "Iteration 169, loss = 0.02157194\n",
      "Iteration 170, loss = 0.02149890\n",
      "Iteration 171, loss = 0.02115899\n",
      "Iteration 172, loss = 0.02138462\n",
      "Iteration 173, loss = 0.02123717\n",
      "Iteration 174, loss = 0.02118516\n",
      "Iteration 175, loss = 0.02129866\n",
      "Iteration 176, loss = 0.02096725\n",
      "Iteration 177, loss = 0.02064937\n",
      "Iteration 178, loss = 0.02078441\n",
      "Iteration 179, loss = 0.02059541\n",
      "Iteration 180, loss = 0.02069192\n",
      "Iteration 181, loss = 0.02063104\n",
      "Iteration 182, loss = 0.02085672\n",
      "Iteration 183, loss = 0.02044783\n",
      "Iteration 184, loss = 0.02064762\n",
      "Iteration 185, loss = 0.02088690\n",
      "Iteration 186, loss = 0.02058187\n",
      "Iteration 187, loss = 0.01994718\n",
      "Iteration 188, loss = 0.02002164\n",
      "Iteration 189, loss = 0.02006943\n",
      "Iteration 190, loss = 0.02088066\n",
      "Iteration 191, loss = 0.02004663\n",
      "Iteration 192, loss = 0.01965983\n",
      "Iteration 193, loss = 0.01973468\n",
      "Iteration 194, loss = 0.02032756\n",
      "Iteration 195, loss = 0.01955647\n",
      "Iteration 196, loss = 0.01964274\n",
      "Iteration 197, loss = 0.01963867\n",
      "Iteration 198, loss = 0.01963145\n",
      "Iteration 199, loss = 0.01903591\n",
      "Iteration 200, loss = 0.02002774\n",
      "Iteration 201, loss = 0.01923233\n",
      "Iteration 202, loss = 0.01911190\n",
      "Iteration 203, loss = 0.01913248\n",
      "Iteration 204, loss = 0.01926483\n",
      "Iteration 205, loss = 0.01903278\n",
      "Iteration 206, loss = 0.01878960\n",
      "Iteration 207, loss = 0.01903537\n",
      "Iteration 208, loss = 0.01888035\n",
      "Iteration 209, loss = 0.01871782\n",
      "Iteration 210, loss = 0.01863067\n",
      "Iteration 211, loss = 0.01857894\n",
      "Iteration 212, loss = 0.01840631\n",
      "Iteration 213, loss = 0.01822702\n",
      "Iteration 214, loss = 0.01805714\n",
      "Iteration 215, loss = 0.01846926\n",
      "Iteration 216, loss = 0.01832477\n",
      "Iteration 217, loss = 0.01791229\n",
      "Iteration 218, loss = 0.01860365\n",
      "Iteration 219, loss = 0.01882309\n",
      "Iteration 220, loss = 0.01780025\n",
      "Iteration 221, loss = 0.01790411\n",
      "Iteration 222, loss = 0.01763362\n",
      "Iteration 223, loss = 0.01777375\n",
      "Iteration 224, loss = 0.01818948\n",
      "Iteration 225, loss = 0.01771156\n",
      "Iteration 226, loss = 0.01732314\n",
      "Iteration 227, loss = 0.01765701\n",
      "Iteration 228, loss = 0.01737694\n",
      "Iteration 229, loss = 0.01720342\n",
      "Iteration 230, loss = 0.01708613\n",
      "Iteration 231, loss = 0.01750637\n",
      "Iteration 232, loss = 0.01774031\n",
      "Iteration 233, loss = 0.01713676\n",
      "Iteration 234, loss = 0.01728921\n",
      "Iteration 235, loss = 0.01693668\n",
      "Iteration 236, loss = 0.01673024\n",
      "Iteration 237, loss = 0.01677041\n",
      "Iteration 238, loss = 0.01680972\n",
      "Iteration 239, loss = 0.01681879\n",
      "Iteration 240, loss = 0.01689977\n",
      "Iteration 241, loss = 0.01643914\n",
      "Iteration 242, loss = 0.01650121\n",
      "Iteration 243, loss = 0.01674933\n",
      "Iteration 244, loss = 0.01718166\n",
      "Iteration 245, loss = 0.01668106\n",
      "Iteration 246, loss = 0.01655131\n",
      "Iteration 247, loss = 0.01629144\n",
      "Iteration 248, loss = 0.01605915\n",
      "Iteration 249, loss = 0.01589813\n",
      "Iteration 250, loss = 0.01622404\n",
      "Iteration 251, loss = 0.01600558\n",
      "Iteration 252, loss = 0.01656670\n",
      "Iteration 253, loss = 0.01610070\n",
      "Iteration 254, loss = 0.01696891\n",
      "Iteration 255, loss = 0.01706781\n",
      "Iteration 256, loss = 0.01679889\n",
      "Iteration 257, loss = 0.01612192\n",
      "Iteration 258, loss = 0.01558891\n",
      "Iteration 259, loss = 0.01559046\n",
      "Iteration 260, loss = 0.01589706\n",
      "Iteration 261, loss = 0.01557633\n",
      "Iteration 262, loss = 0.01598533\n",
      "Iteration 263, loss = 0.01551695\n",
      "Iteration 264, loss = 0.01575474\n",
      "Iteration 265, loss = 0.01545857\n",
      "Iteration 266, loss = 0.01595387\n",
      "Iteration 267, loss = 0.01542132\n",
      "Iteration 268, loss = 0.01483257\n",
      "Iteration 269, loss = 0.01489452\n",
      "Iteration 270, loss = 0.01485574\n",
      "Iteration 271, loss = 0.01503897\n",
      "Iteration 272, loss = 0.01472755\n",
      "Iteration 273, loss = 0.01510840\n",
      "Iteration 274, loss = 0.01464940\n",
      "Iteration 275, loss = 0.01470506\n",
      "Iteration 276, loss = 0.01508457\n",
      "Iteration 277, loss = 0.01502750\n",
      "Iteration 278, loss = 0.01460663\n",
      "Iteration 279, loss = 0.01489612\n",
      "Iteration 280, loss = 0.01467702\n",
      "Iteration 281, loss = 0.01435132\n",
      "Iteration 282, loss = 0.01428885\n",
      "Iteration 283, loss = 0.01460371\n",
      "Iteration 284, loss = 0.01416914\n",
      "Iteration 285, loss = 0.01462795\n",
      "Iteration 286, loss = 0.01416715\n",
      "Iteration 287, loss = 0.01443737\n",
      "Iteration 288, loss = 0.01406267\n",
      "Iteration 289, loss = 0.01422189\n",
      "Iteration 290, loss = 0.01427034\n",
      "Iteration 291, loss = 0.01404129\n",
      "Iteration 292, loss = 0.01401400\n",
      "Iteration 293, loss = 0.01370554\n",
      "Iteration 294, loss = 0.01402912\n",
      "Iteration 295, loss = 0.01369461\n",
      "Iteration 296, loss = 0.01397793\n",
      "Iteration 297, loss = 0.01382216\n",
      "Iteration 298, loss = 0.01352086\n",
      "Iteration 299, loss = 0.01396923\n",
      "Iteration 300, loss = 0.01449774\n",
      "Iteration 301, loss = 0.01351396\n",
      "Iteration 302, loss = 0.01355027\n",
      "Iteration 303, loss = 0.01387413\n",
      "Iteration 304, loss = 0.01396514\n",
      "Iteration 305, loss = 0.01355313\n",
      "Iteration 306, loss = 0.01339656\n",
      "Iteration 307, loss = 0.01335450\n",
      "Iteration 308, loss = 0.01322749\n",
      "Iteration 309, loss = 0.01344989\n",
      "Iteration 310, loss = 0.01341064\n",
      "Iteration 311, loss = 0.01337475\n",
      "Iteration 312, loss = 0.01354834\n",
      "Iteration 313, loss = 0.01543951\n",
      "Iteration 314, loss = 0.01395672\n",
      "Iteration 315, loss = 0.01372592\n",
      "Iteration 316, loss = 0.01311491\n",
      "Iteration 317, loss = 0.01294592\n",
      "Iteration 318, loss = 0.01314594\n",
      "Iteration 319, loss = 0.01348682\n",
      "Iteration 320, loss = 0.01399670\n",
      "Iteration 321, loss = 0.01394881\n",
      "Iteration 322, loss = 0.01400902\n",
      "Iteration 323, loss = 0.01331691\n",
      "Iteration 324, loss = 0.01285228\n",
      "Iteration 325, loss = 0.01259922\n",
      "Iteration 326, loss = 0.01274480\n",
      "Iteration 327, loss = 0.01267388\n",
      "Iteration 328, loss = 0.01274281\n",
      "Iteration 329, loss = 0.01264374\n",
      "Iteration 330, loss = 0.01407237\n",
      "Iteration 331, loss = 0.01320979\n",
      "Iteration 332, loss = 0.01248606\n",
      "Iteration 333, loss = 0.01262828\n",
      "Iteration 334, loss = 0.01329808\n",
      "Iteration 335, loss = 0.01288749\n",
      "Iteration 336, loss = 0.01367424\n",
      "Iteration 337, loss = 0.01254087\n",
      "Iteration 338, loss = 0.01230786\n",
      "Iteration 339, loss = 0.01292572\n",
      "Iteration 340, loss = 0.01232770\n",
      "Iteration 341, loss = 0.01274234\n",
      "Iteration 342, loss = 0.01246779\n",
      "Iteration 343, loss = 0.01241659\n",
      "Iteration 344, loss = 0.01248911\n",
      "Iteration 345, loss = 0.01251285\n",
      "Iteration 346, loss = 0.01236641\n",
      "Iteration 347, loss = 0.01259959\n",
      "Iteration 348, loss = 0.01254850\n",
      "Iteration 349, loss = 0.01229700\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from setup import get_datasets, get_results\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(8,8,8), activation='relu', solver='adam', max_iter=500, verbose=True)\n",
    "\n",
    "X_trains, X_tests, y_trains, y_tests = get_datasets(None)\n",
    "\n",
    "for i in range(len(X_trains)):  \n",
    "    mlp.fit(X_trains[i], y_trains[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Test #   Accuracy  Sensitivity  Specificity\n",
      "count  18.000000  18.000000    18.000000    18.000000\n",
      "mean    8.500000   0.931757     0.997352     0.864209\n",
      "std     5.338539   0.119296     0.004550     0.246929\n",
      "min     0.000000   0.570000     0.983221     0.162338\n",
      "25%     4.250000   0.961250     0.996652     0.933166\n",
      "50%     8.500000   0.981667     1.000000     0.966686\n",
      "75%    12.750000   0.991663     1.000000     0.991369\n",
      "max    17.000000   0.998333     1.000000     0.996700\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(len(X_tests)):  \n",
    "    mlp_predictions = mlp.predict(X_tests[i])\n",
    "    predictions_nominal = [ 0 if x < 0.5 else 1 for x in mlp_predictions]\n",
    "    cm, acc, sen, spe = get_results(predictions_nominal, y_tests[i])\n",
    "    \n",
    "    results.append([i, acc, sen, spe])\n",
    "    \n",
    "res = pd.DataFrame(data=results, columns=[\"Test #\", \"Accuracy\", \"Sensitivity\", \"Specificity\"])\n",
    "\n",
    "print(res.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('mlp_scores.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
